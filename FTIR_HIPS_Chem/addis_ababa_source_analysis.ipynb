{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5cc0ad8f",
   "metadata": {},
   "source": "# Addis Ababa: BC/EC Method Comparison by Source Apportionment\n\nThis notebook compares HIPS Fabs, FTIR EC, and Aethalometer BC measurements,\nstratified by dominant aerosol source type from PMF/source apportionment analysis.\n\n## Method Pairs (all analyses applied to each):\n1. **FTIR EC vs HIPS Fabs/MAC**\n2. **FTIR EC vs Aethalometer IR BCc**\n3. **HIPS Fabs/MAC vs Aethalometer IR BCc**\n\n## Analysis for Each Pair:\n1. **Baseline Regression** — All data scatter with regression statistics\n2. **Source-Separated Regressions** — Filter by dominant source type\n3. **Threshold-Filtered Analysis** — Exclude \"mixed days\" using source contribution thresholds\n\n## Additional Visualizations:\n4. **Source Contribution Visualization** — Daily source fraction bar charts\n\n## Source Categories:\n- Charcoal burning\n- Wood burning\n- Fossil fuel\n- Polluted marine\n- Sea salt\n\n---"
  },
  {
   "cell_type": "markdown",
   "id": "a522bb8b",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f623c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from matplotlib.dates import MonthLocator, DateFormatter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add scripts folder to path\n",
    "notebook_dir = os.path.dirname(os.path.abspath('__file__'))\n",
    "scripts_path = os.path.join(notebook_dir, 'scripts')\n",
    "if scripts_path not in sys.path:\n",
    "    sys.path.insert(0, scripts_path)\n",
    "\n",
    "from config import SITES, MAC_VALUE\n",
    "from data_matching import (\n",
    "    load_aethalometer_data,\n",
    "    load_filter_data,\n",
    "    add_base_filter_id,\n",
    "    match_all_parameters,\n",
    "    load_etad_factors_with_filter_ids,\n",
    ")\n",
    "print(\"Loaded config and data_matching modules\")\n",
    "\n",
    "# Configure matplotlib\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 11\n",
    "plt.rcParams['axes.labelsize'] = 12\n",
    "plt.rcParams['axes.titlesize'] = 13\n",
    "\n",
    "# Create output directories\n",
    "def setup_directories():\n",
    "    dirs = {\n",
    "        'plots': 'output/plots/addis_ababa/source_regression',\n",
    "        'data': 'output/data/addis_ababa'\n",
    "    }\n",
    "    for dir_path in dirs.values():\n",
    "        os.makedirs(dir_path, exist_ok=True)\n",
    "    return dirs\n",
    "\n",
    "dirs = setup_directories()\n",
    "print(\"Setup complete!\")\n",
    "print(f\"MAC value: {MAC_VALUE} m²/g\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12fcdff3",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777af457",
   "metadata": {},
   "outputs": [],
   "source": "# Site configuration\nADDIS_CONFIG = {\n    'name': 'Addis_Ababa',\n    'code': 'ETAD',\n    'timezone': 'Africa/Addis_Ababa',\n}\n\n# Ethiopian seasons\nSEASONS = {\n    'Dry Season': [10, 11, 12, 1, 2],\n    'Belg Rainy Season': [3, 4, 5],\n    'Kiremt Rainy Season': [6, 7, 8, 9]\n}\nSEASONS_ORDER = ['Dry Season', 'Belg Rainy Season', 'Kiremt Rainy Season']\nSEASON_COLORS = {'Dry Season': '#E67E22', 'Belg Rainy Season': '#27AE60', 'Kiremt Rainy Season': '#3498DB'}\n\n# Source apportionment categories and their colors\nSOURCE_CATEGORIES = {\n    'charcoal': {'label': 'Charcoal Burning', 'color': '#2C3E50', 'marker': 'o'},\n    'wood': {'label': 'Wood Burning', 'color': '#8B4513', 'marker': 's'},\n    'fossil_fuel': {'label': 'Fossil Fuel', 'color': '#7D3C98', 'marker': '^'},\n    'polluted_marine': {'label': 'Polluted Marine', 'color': '#2980B9', 'marker': 'D'},\n    'sea_salt': {'label': 'Sea Salt', 'color': '#1ABC9C', 'marker': 'v'},\n}\nSOURCE_ORDER = ['charcoal', 'wood', 'fossil_fuel', 'polluted_marine', 'sea_salt']\n\n# Fraction column names (for stacking order in bar charts)\nFRAC_COLS = ['charcoal_frac', 'wood_frac', 'fossil_fuel_frac', 'polluted_marine_frac', 'sea_salt_frac']\n\n# Stack order for bar charts: (column_name, label, color)\nSTACK_ORDER = [\n    ('charcoal_frac', 'Charcoal Burning', '#2C3E50'),\n    ('wood_frac', 'Wood Burning', '#8B4513'),\n    ('fossil_fuel_frac', 'Fossil Fuel', '#7D3C98'),\n    ('polluted_marine_frac', 'Polluted Marine', '#2980B9'),\n    ('sea_salt_frac', 'Sea Salt', '#1ABC9C'),\n]\n\n# BC/EC measurement methods\nMETHODS = {\n    'hips_fabs': {'label': 'HIPS Fabs/MAC', 'color': '#2ca02c', 'unit': 'µg/m³'},\n    'ftir_ec': {'label': 'FTIR EC', 'color': '#d62728', 'unit': 'µg/m³'},\n    'ir_bcc': {'label': 'Aeth IR BCc', 'color': '#1f77b4', 'unit': 'µg/m³'},\n    'uv_bcc': {'label': 'Aeth UV BCc', 'color': '#ff7f0e', 'unit': 'µg/m³'},\n}\n\n# All method pairs to analyze: (x_col, y_col, x_label, y_label, file_prefix)\nMETHOD_PAIRS = [\n    ('ftir_ec', 'hips_fabs', 'FTIR EC (µg/m³)', 'HIPS Fabs/MAC (µg/m³)', 'hips_vs_ec'),\n    ('ftir_ec', 'ir_bcc', 'FTIR EC (µg/m³)', 'Aeth IR BCc (µg/m³)', 'aeth_ir_vs_ec'),\n    ('hips_fabs', 'ir_bcc', 'HIPS Fabs/MAC (µg/m³)', 'Aeth IR BCc (µg/m³)', 'aeth_ir_vs_hips'),\n]\n\n# Thresholds to test for dominant source filtering\nDOMINANCE_THRESHOLDS = [0.30, 0.40, 0.50, 0.60]  # 30%, 40%, 50%, 60%\n\nprint(f\"Site: {ADDIS_CONFIG['name']}\")\nprint(f\"Source categories: {', '.join(SOURCE_CATEGORIES.keys())}\")\nprint(f\"Method pairs to analyze: {len(METHOD_PAIRS)}\")\nfor x_col, y_col, x_lab, y_lab, prefix in METHOD_PAIRS:\n    print(f\"  {y_lab} vs {x_lab}\")\nprint(f\"Dominance thresholds to test: {DOMINANCE_THRESHOLDS}\")"
  },
  {
   "cell_type": "markdown",
   "id": "f743782b",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "\n",
    "Load factor contributions (joined to Filter IDs via `oldDate`), then merge with\n",
    "FTIR EC, HIPS Fabs, and Aethalometer BC measurements via `base_filter_id`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf2492a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Load factor contributions with Filter IDs (joined via oldDate)\n",
    "# =============================================================================\n",
    "factors_df = load_etad_factors_with_filter_ids()\n",
    "\n",
    "# Map GF columns to the _frac names used by the rest of the notebook\n",
    "FACTOR_TO_FRAC = {\n",
    "    'GF3 (Charcoal)':              'charcoal_frac',\n",
    "    'GF2 (Wood Burning)':          'wood_frac',\n",
    "    'GF5 (Fossil Fuel Combustion)':'fossil_fuel_frac',\n",
    "    'GF4 (Polluted Marine)':       'polluted_marine_frac',\n",
    "    'GF1 (Sea Salt Mixed)':        'sea_salt_frac',\n",
    "}\n",
    "factors_df = factors_df.rename(columns=FACTOR_TO_FRAC)\n",
    "frac_cols = list(FACTOR_TO_FRAC.values())\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"RAW GF FRACTIONS (before normalization)\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nThese are the raw PM2.5 mass fractions from PMF analysis.\")\n",
    "print(\"They do NOT sum to 1.0 — they represent the fraction of total PM2.5 from each source.\\n\")\n",
    "\n",
    "# Show raw fraction statistics\n",
    "raw_sums = factors_df[frac_cols].sum(axis=1)\n",
    "print(f\"Raw GF row sums: min={raw_sums.min():.3f}, max={raw_sums.max():.3f}, mean={raw_sums.mean():.3f}\")\n",
    "print(\"\\nRaw GF statistics by source:\")\n",
    "for col in frac_cols:\n",
    "    vals = factors_df[col].dropna()\n",
    "    print(f\"  {col}: mean={vals.mean():.4f}, min={vals.min():.4f}, max={vals.max():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83aa33c1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 1: Normalization of GF Fractions\n",
    "\n",
    "### Why Normalize?\n",
    "\n",
    "The raw GF fractions don't sum to 1.0 — they represent the fraction of **total PM₂.₅ mass**\n",
    "attributable to each source. The sum varies by day (typically 0.03–0.46) depending on how\n",
    "much of the aerosol mass is \"explained\" by the PMF factors.\n",
    "\n",
    "For visualization and source comparison, we normalize so that:\n",
    "- Each source's contribution is relative to the **explained OM** (not total PM₂.₅)\n",
    "- The normalized fractions sum to exactly 1.0 (100%)\n",
    "\n",
    "### Normalization Formula\n",
    "\n",
    "For each day:\n",
    "```\n",
    "normalized_fraction[source] = raw_GF[source] / sum(all raw_GF values)\n",
    "```\n",
    "\n",
    "### Example\n",
    "\n",
    "| Source | Raw GF | Normalized |\n",
    "|--------|--------|------------|\n",
    "| Fossil Fuel | 0.05 | 0.05/0.25 = 0.20 |\n",
    "| Polluted Marine | 0.03 | 0.03/0.25 = 0.12 |\n",
    "| Sea Salt | 0.02 | 0.02/0.25 = 0.08 |\n",
    "| Wood Burning | 0.07 | 0.07/0.25 = 0.28 |\n",
    "| Charcoal | 0.08 | 0.08/0.25 = 0.32 |\n",
    "| **Total** | **0.25** | **1.00** |\n",
    "\n",
    "### Key Point\n",
    "\n",
    "The normalization changes the interpretation:\n",
    "- **Raw GF**: Fraction of total PM₂.₅ mass from each source\n",
    "- **Normalized**: Fraction of **explained OM** from each source (always sums to 100%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f4d557",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# NORMALIZATION: Divide each fraction by the row total so they sum to 1.0\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"STEP 1: NORMALIZATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Calculate row totals (sum of all GF fractions for each day)\n",
    "row_totals = factors_df[frac_cols].sum(axis=1)\n",
    "\n",
    "print(f\"\\nRow totals before normalization:\")\n",
    "print(f\"  Min: {row_totals.min():.4f}\")\n",
    "print(f\"  Max: {row_totals.max():.4f}\")\n",
    "print(f\"  Mean: {row_totals.mean():.4f}\")\n",
    "\n",
    "# Normalize: divide each fraction by the row total\n",
    "for col in frac_cols:\n",
    "    factors_df[col] = factors_df[col] / row_totals\n",
    "\n",
    "# Verify normalization worked\n",
    "normalized_sums = factors_df[frac_cols].sum(axis=1)\n",
    "print(f\"\\nRow totals after normalization:\")\n",
    "print(f\"  Min: {normalized_sums.min():.4f}\")\n",
    "print(f\"  Max: {normalized_sums.max():.4f}\")\n",
    "print(f\"  Mean: {normalized_sums.mean():.4f}\")\n",
    "print(f\"  (Should all be ~1.0)\")\n",
    "\n",
    "print(\"\\nNormalized fraction statistics by source:\")\n",
    "for col in frac_cols:\n",
    "    vals = factors_df[col].dropna()\n",
    "    print(f\"  {col}: mean={vals.mean():.3f}, min={vals.min():.3f}, max={vals.max():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda5c7ac",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Load and Merge BC/EC Measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdff3905",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Load aethalometer + filter measurements and match by date\n",
    "# =============================================================================\n",
    "aethalometer_data = load_aethalometer_data()\n",
    "filter_data = load_filter_data()\n",
    "filter_data = add_base_filter_id(filter_data)\n",
    "\n",
    "df_aeth = aethalometer_data.get('Addis_Ababa')\n",
    "bc_df = match_all_parameters('Addis_Ababa', 'ETAD', df_aeth, filter_data)\n",
    "\n",
    "# =============================================================================\n",
    "# Merge BC/EC measurements with factor contributions via base_filter_id\n",
    "# =============================================================================\n",
    "# Get the base_filter_id for each bc_df date by looking up in the unified dataset\n",
    "etad_filters = filter_data[filter_data['Site'] == 'ETAD'][['SampleDate', 'FilterId']].drop_duplicates()\n",
    "etad_filters = etad_filters.rename(columns={'SampleDate': 'date', 'FilterId': 'base_filter_id'})\n",
    "bc_df['date'] = pd.to_datetime(bc_df['date'])\n",
    "etad_filters['date'] = pd.to_datetime(etad_filters['date'])\n",
    "\n",
    "bc_with_id = pd.merge(bc_df, etad_filters, on='date', how='left')\n",
    "\n",
    "# Now merge with factor contributions on base_filter_id\n",
    "factor_merge_cols = ['base_filter_id'] + frac_cols\n",
    "df = pd.merge(bc_with_id, factors_df[factor_merge_cols].drop_duplicates(),\n",
    "              on='base_filter_id', how='inner')\n",
    "\n",
    "# =============================================================================\n",
    "# Add temporal features\n",
    "# =============================================================================\n",
    "df['Month'] = df['date'].dt.month\n",
    "df['Ethiopian_Season'] = df['Month'].map(lambda m:\n",
    "    'Dry Season' if m in SEASONS['Dry Season'] else\n",
    "    'Belg Rainy Season' if m in SEASONS['Belg Rainy Season'] else\n",
    "    'Kiremt Rainy Season'\n",
    ")\n",
    "\n",
    "# Determine dominant source for each sample (using normalized fractions)\n",
    "df['dominant_source'] = df[frac_cols].idxmax(axis=1).str.replace('_frac', '')\n",
    "df['dominant_fraction'] = df[frac_cols].max(axis=1)\n",
    "\n",
    "print(f\"\\nFinal dataset: {len(df)} samples\")\n",
    "print(f\"Date range: {df['date'].min().date()} to {df['date'].max().date()}\")\n",
    "print(f\"\\nBC/EC availability:\")\n",
    "for col in ['ftir_ec', 'hips_fabs', 'ir_bcc']:\n",
    "    if col in df.columns:\n",
    "        n = df[col].notna().sum()\n",
    "        print(f\"  {col}: {n} samples\")\n",
    "\n",
    "print(f\"\\nDominant source distribution:\")\n",
    "print(df['dominant_source'].value_counts().to_string())\n",
    "print(f\"\\nDominant fraction stats: mean={df['dominant_fraction'].mean():.1%}, \"\n",
    "      f\"min={df['dominant_fraction'].min():.1%}, max={df['dominant_fraction'].max():.1%}\")\n",
    "print(f\"Samples with ≥50% dominant: {(df['dominant_fraction'] >= 0.50).sum()}\")\n",
    "print(f\"Samples with ≥30% dominant: {(df['dominant_fraction'] >= 0.30).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a54148e",
   "metadata": {},
   "source": "---\n\n# Task 1: Baseline Regression Plots (All Method Pairs)\n\n**Goal**: Create baseline scatter plots with regression statistics for all three method comparisons."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30515327",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": "def plot_regression(df, x_col, y_col, x_label, y_label, title, color_by=None,\n                    color_dict=None, ax=None, show_stats=True, force_through_origin=False):\n    \"\"\"\n    Create a regression scatter plot with statistics.\n    \"\"\"\n    if ax is None:\n        fig, ax = plt.subplots(figsize=(8, 7))\n    else:\n        fig = ax.figure\n    \n    valid = df[[x_col, y_col]].dropna()\n    if color_by and color_by in df.columns:\n        valid = pd.merge(valid, df[[color_by]], left_index=True, right_index=True)\n    \n    if len(valid) < 3:\n        ax.text(0.5, 0.5, f'Insufficient data\\n(n={len(valid)})', \n                transform=ax.transAxes, ha='center', va='center', fontsize=14)\n        ax.set_title(title)\n        return fig, None\n    \n    x = valid[x_col].values\n    y = valid[y_col].values\n    \n    # Plot points\n    if color_by and color_by in valid.columns and color_dict:\n        for category in valid[color_by].unique():\n            mask = valid[color_by] == category\n            cat_info = color_dict.get(category, {'color': 'gray', 'label': category, 'marker': 'o'})\n            ax.scatter(valid.loc[mask, x_col], valid.loc[mask, y_col],\n                      s=60, alpha=0.7, color=cat_info.get('color', 'gray'),\n                      marker=cat_info.get('marker', 'o'),\n                      edgecolors='black', linewidth=0.3,\n                      label=f\"{cat_info.get('label', category)} (n={mask.sum()})\")\n    else:\n        ax.scatter(x, y, s=60, alpha=0.6, color='#3498DB', edgecolors='black', linewidth=0.3)\n    \n    # Regression\n    if force_through_origin:\n        slope = np.sum(x * y) / np.sum(x * x)\n        intercept = 0\n        y_pred = slope * x\n        ss_res = np.sum((y - y_pred) ** 2)\n        ss_tot = np.sum((y - np.mean(y)) ** 2)\n        r_squared = 1 - (ss_res / ss_tot)\n        r = np.sqrt(r_squared) * np.sign(slope)\n        p = stats.pearsonr(x, y)[1]\n        se = np.sqrt(ss_res / (len(x) - 1))\n    else:\n        slope, intercept, r, p, se = stats.linregress(x, y)\n        r_squared = r ** 2\n    \n    # Plot regression line and 1:1\n    ax_max = max(x.max(), y.max()) * 1.1\n    x_fit = np.linspace(0, ax_max, 100)\n    ax.plot(x_fit, slope * x_fit + intercept, 'k-', linewidth=2, alpha=0.7, label='Regression')\n    ax.plot([0, ax_max], [0, ax_max], 'k--', linewidth=1.5, alpha=0.4, label='1:1 line')\n    \n    # Statistics annotation\n    if show_stats:\n        sig = '***' if p < 0.001 else '**' if p < 0.01 else '*' if p < 0.05 else 'ns'\n        if force_through_origin:\n            stats_text = f'y = {slope:.3f}x\\nR² = {r_squared:.3f} ({sig})\\nn = {len(valid)}'\n        else:\n            stats_text = f'y = {slope:.3f}x + {intercept:.3f}\\nR² = {r_squared:.3f} ({sig})\\nn = {len(valid)}'\n        ax.text(0.03, 0.97, stats_text, transform=ax.transAxes, fontsize=10, va='top',\n                bbox=dict(boxstyle='round', facecolor='white', alpha=0.9))\n    \n    # Formatting\n    ax.set_xlim(0, ax_max)\n    ax.set_ylim(0, ax_max)\n    ax.set_xlabel(x_label, fontsize=12)\n    ax.set_ylabel(y_label, fontsize=12)\n    ax.set_title(title, fontsize=13, fontweight='bold')\n    ax.set_aspect('equal')\n    ax.grid(True, alpha=0.3)\n    \n    if color_by:\n        ax.legend(fontsize=8, loc='lower right')\n    \n    results = {\n        'slope': slope, 'intercept': intercept, 'r': r, 'r_squared': r_squared,\n        'p_value': p, 'se': se, 'n': len(valid)\n    }\n    \n    return fig, results\n\n\nprint(\"=\"*80)\nprint(\"TASK 1: BASELINE REGRESSIONS — ALL METHOD PAIRS\")\nprint(\"=\"*80)\n\nbaseline_results = {}\n\nfor x_col, y_col, x_label, y_label, prefix in METHOD_PAIRS:\n    pair_label = f'{y_label} vs {x_label}'\n    print(f\"\\n--- {pair_label} ---\")\n    \n    fig, results = plot_regression(\n        df, x_col, y_col, x_label, y_label,\n        f'{y_label.split(\" (\")[0]} vs {x_label.split(\" (\")[0]} — All Data'\n    )\n    plt.tight_layout()\n    plt.savefig(os.path.join(dirs['plots'], f'{prefix}_baseline.png'), dpi=150, bbox_inches='tight')\n    plt.show()\n    \n    if results:\n        baseline_results[prefix] = results\n        print(f\"  Slope: {results['slope']:.4f}\")\n        print(f\"  Intercept: {results['intercept']:.4f}\")\n        print(f\"  R²: {results['r_squared']:.4f}\")\n        print(f\"  p-value: {results['p_value']:.2e}\")\n        print(f\"  n: {results['n']}\")"
  },
  {
   "cell_type": "markdown",
   "id": "a1766020",
   "metadata": {},
   "source": "---\n\n# Task 2: Source-Separated Regression Plots (All Method Pairs)\n\n**Goal**: Create regression plots filtered by dominant source type for all three method comparisons."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0971188",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": "def plot_source_separated_regressions(df, x_col, y_col, x_label, y_label, \n                                       sources=SOURCE_ORDER, source_info=SOURCE_CATEGORIES):\n    \"\"\"\n    Create panel of regression plots, one per dominant source.\n    \"\"\"\n    n_sources = len(sources)\n    n_cols = 3\n    n_rows = int(np.ceil(n_sources / n_cols))\n    \n    fig, axes = plt.subplots(n_rows, n_cols, figsize=(6*n_cols, 5.5*n_rows))\n    axes = axes.flatten() if n_sources > 1 else [axes]\n    \n    results_all = {}\n    \n    for idx, source in enumerate(sources):\n        ax = axes[idx]\n        \n        # Filter to dominant source\n        source_mask = df['dominant_source'] == source\n        source_data = df[source_mask].copy()\n        \n        if len(source_data) < 3:\n            ax.text(0.5, 0.5, f'{source_info[source][\"label\"]}\\n(n={len(source_data)})\\nInsufficient data',\n                   transform=ax.transAxes, ha='center', va='center', fontsize=11)\n            ax.set_title(source_info[source]['label'], fontsize=11, fontweight='bold',\n                        color=source_info[source]['color'])\n            ax.grid(True, alpha=0.3)\n            continue\n        \n        valid = source_data[[x_col, y_col]].dropna()\n        \n        if len(valid) < 3:\n            ax.text(0.5, 0.5, f'{source_info[source][\"label\"]}\\n(n={len(valid)})\\nInsufficient data',\n                   transform=ax.transAxes, ha='center', va='center', fontsize=11)\n            ax.set_title(source_info[source]['label'], fontsize=11, fontweight='bold',\n                        color=source_info[source]['color'])\n            ax.grid(True, alpha=0.3)\n            continue\n        \n        x = valid[x_col].values\n        y = valid[y_col].values\n        \n        # Scatter\n        ax.scatter(x, y, s=50, alpha=0.6, color=source_info[source]['color'],\n                  marker=source_info[source]['marker'], edgecolors='black', linewidth=0.3)\n        \n        # Regression\n        slope, intercept, r, p, se = stats.linregress(x, y)\n        \n        ax_max = max(x.max(), y.max()) * 1.1 if len(x) > 0 else 10\n        x_fit = np.linspace(0, ax_max, 100)\n        ax.plot(x_fit, slope * x_fit + intercept, 'k-', linewidth=1.5, alpha=0.7)\n        ax.plot([0, ax_max], [0, ax_max], 'k--', linewidth=1, alpha=0.3)\n        \n        # Stats\n        sig = '***' if p < 0.001 else '**' if p < 0.01 else '*' if p < 0.05 else 'ns'\n        ax.text(0.03, 0.97, f'y = {slope:.3f}x + {intercept:.2f}\\nR² = {r**2:.3f} ({sig})\\nn = {len(valid)}',\n                transform=ax.transAxes, fontsize=9, va='top',\n                bbox=dict(boxstyle='round', facecolor='white', alpha=0.9))\n        \n        ax.set_xlim(0, ax_max)\n        ax.set_ylim(0, ax_max)\n        ax.set_xlabel(x_label, fontsize=10)\n        ax.set_ylabel(y_label, fontsize=10)\n        ax.set_title(source_info[source]['label'], fontsize=11, fontweight='bold',\n                    color=source_info[source]['color'])\n        ax.set_aspect('equal')\n        ax.grid(True, alpha=0.3)\n        \n        results_all[source] = {\n            'slope': slope, 'intercept': intercept, 'r': r, 'r_squared': r**2,\n            'p_value': p, 'n': len(valid)\n        }\n    \n    # Hide unused axes\n    for idx in range(n_sources, len(axes)):\n        axes[idx].set_visible(False)\n    \n    plt.suptitle(f'{y_label} vs {x_label} — By Dominant Source',\n                fontsize=14, fontweight='bold', y=1.02)\n    plt.tight_layout()\n    \n    return fig, results_all\n\n\nprint(\"=\"*80)\nprint(\"TASK 2: SOURCE-SEPARATED REGRESSIONS — ALL METHOD PAIRS\")\nprint(\"=\"*80)\n\nall_source_results = {}\n\nfor x_col, y_col, x_label, y_label, prefix in METHOD_PAIRS:\n    pair_label = f'{y_label.split(\" (\")[0]} vs {x_label.split(\" (\")[0]}'\n    print(f\"\\n{'='*60}\")\n    print(f\"  {pair_label}\")\n    print(f\"{'='*60}\")\n    \n    fig, source_results = plot_source_separated_regressions(\n        df, x_col, y_col, x_label, y_label\n    )\n    plt.savefig(os.path.join(dirs['plots'], f'{prefix}_by_source.png'), dpi=150, bbox_inches='tight')\n    plt.show()\n    \n    all_source_results[prefix] = source_results\n    \n    # Summary table\n    print(f\"\\n{'Source':<20s} {'n':>5s} {'Slope':>8s} {'Intercept':>10s} {'R²':>8s} {'p-value':>12s}\")\n    print(\"-\" * 70)\n    for source in SOURCE_ORDER:\n        if source in source_results:\n            r = source_results[source]\n            sig = '*' if r['p_value'] < 0.05 else ''\n            print(f\"{SOURCE_CATEGORIES[source]['label']:<20s} {r['n']:>5d} {r['slope']:>8.3f} \"\n                  f\"{r['intercept']:>10.3f} {r['r_squared']:>8.3f} {r['p_value']:>11.2e}{sig}\")"
  },
  {
   "cell_type": "markdown",
   "id": "c40781d6",
   "metadata": {},
   "source": "---\n\n# Task 3: Threshold-Filtered Regressions (All Method Pairs)\n\n**Goal**: Filter out \"mixed days\" for all three method comparisons — only keep days where dominant source exceeds threshold."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8290e25",
   "metadata": {},
   "outputs": [],
   "source": "def analyze_threshold_filtering(df, thresholds=DOMINANCE_THRESHOLDS):\n    \"\"\"\n    Analyze how threshold filtering affects sample counts.\n    \"\"\"\n    print(\"\\nThreshold Filtering Analysis:\")\n    print(\"=\" * 70)\n    print(f\"{'Threshold':<12s}\", end='')\n    for source in SOURCE_ORDER:\n        print(f\" {SOURCE_CATEGORIES[source]['label'][:10]:>10s}\", end='')\n    print(f\" {'Total':>10s}\")\n    print(\"-\" * 70)\n    \n    for thresh in thresholds:\n        filtered = df[df['dominant_fraction'] >= thresh]\n        print(f\"{thresh*100:.0f}%{'':<9s}\", end='')\n        for source in SOURCE_ORDER:\n            n = (filtered['dominant_source'] == source).sum()\n            print(f\" {n:>10d}\", end='')\n        print(f\" {len(filtered):>10d}\")\n\n\ndef plot_threshold_comparison(df, x_col, y_col, x_label, y_label, \n                               thresholds=DOMINANCE_THRESHOLDS):\n    \"\"\"\n    Create comparison plots at different thresholds.\n    \"\"\"\n    n_thresh = len(thresholds)\n    fig, axes = plt.subplots(1, n_thresh + 1, figsize=(5*(n_thresh+1), 5))\n    \n    results = {}\n    \n    # Plot 0: All data\n    ax = axes[0]\n    valid = df[[x_col, y_col]].dropna()\n    if len(valid) >= 3:\n        x, y = valid[x_col].values, valid[y_col].values\n        ax.scatter(x, y, s=40, alpha=0.5, color='gray', edgecolors='black', linewidth=0.2)\n        slope, intercept, r, p, se = stats.linregress(x, y)\n        ax_max = max(x.max(), y.max()) * 1.1\n        x_fit = np.linspace(0, ax_max, 100)\n        ax.plot(x_fit, slope * x_fit + intercept, 'k-', linewidth=1.5)\n        ax.plot([0, ax_max], [0, ax_max], 'k--', linewidth=1, alpha=0.3)\n        ax.text(0.03, 0.97, f'R² = {r**2:.3f}\\nn = {len(valid)}',\n                transform=ax.transAxes, fontsize=9, va='top',\n                bbox=dict(boxstyle='round', facecolor='white', alpha=0.9))\n        ax.set_xlim(0, ax_max)\n        ax.set_ylim(0, ax_max)\n        results['all'] = {'r_squared': r**2, 'n': len(valid), 'slope': slope}\n    ax.set_xlabel(x_label, fontsize=10)\n    ax.set_ylabel(y_label, fontsize=10)\n    ax.set_title('All Data', fontsize=11, fontweight='bold')\n    ax.set_aspect('equal')\n    ax.grid(True, alpha=0.3)\n    \n    # Threshold-filtered plots\n    for idx, thresh in enumerate(thresholds):\n        ax = axes[idx + 1]\n        filtered = df[df['dominant_fraction'] >= thresh]\n        valid = filtered[[x_col, y_col, 'dominant_source']].dropna()\n        \n        if len(valid) >= 3:\n            # Color by source\n            for source in SOURCE_ORDER:\n                mask = valid['dominant_source'] == source\n                if mask.sum() > 0:\n                    ax.scatter(valid.loc[mask, x_col], valid.loc[mask, y_col],\n                              s=40, alpha=0.6, color=SOURCE_CATEGORIES[source]['color'],\n                              marker=SOURCE_CATEGORIES[source]['marker'],\n                              edgecolors='black', linewidth=0.2,\n                              label=SOURCE_CATEGORIES[source]['label'][:8])\n            \n            x, y = valid[x_col].values, valid[y_col].values\n            slope, intercept, r, p, se = stats.linregress(x, y)\n            ax_max = max(x.max(), y.max()) * 1.1\n            x_fit = np.linspace(0, ax_max, 100)\n            ax.plot(x_fit, slope * x_fit + intercept, 'k-', linewidth=1.5)\n            ax.plot([0, ax_max], [0, ax_max], 'k--', linewidth=1, alpha=0.3)\n            ax.text(0.03, 0.97, f'R² = {r**2:.3f}\\nn = {len(valid)}',\n                    transform=ax.transAxes, fontsize=9, va='top',\n                    bbox=dict(boxstyle='round', facecolor='white', alpha=0.9))\n            ax.set_xlim(0, ax_max)\n            ax.set_ylim(0, ax_max)\n            results[f'{thresh*100:.0f}%'] = {'r_squared': r**2, 'n': len(valid), 'slope': slope}\n        \n        ax.set_xlabel(x_label, fontsize=10)\n        ax.set_title(f'≥{thresh*100:.0f}% Dominant', fontsize=11, fontweight='bold')\n        ax.set_aspect('equal')\n        ax.grid(True, alpha=0.3)\n        if idx == len(thresholds) - 1:\n            ax.legend(fontsize=6, loc='lower right')\n    \n    plt.suptitle(f'{y_label} vs {x_label} — Threshold Comparison',\n                fontsize=13, fontweight='bold', y=1.02)\n    plt.tight_layout()\n    \n    return fig, results\n\n\ndef plot_source_regressions_threshold_filtered(df, x_col, y_col, x_label, y_label, threshold=0.50):\n    \"\"\"\n    Plot source-separated regressions with threshold filter applied.\n    \"\"\"\n    filtered = df[df['dominant_fraction'] >= threshold].copy()\n    \n    print(f\"\\nFiltered to {threshold*100:.0f}%+ dominant: {len(filtered)} samples\")\n    print(f\"Source breakdown: {filtered['dominant_source'].value_counts().to_dict()}\")\n    \n    fig, results = plot_source_separated_regressions(\n        filtered, x_col, y_col, x_label, y_label\n    )\n    \n    plt.suptitle(f'{y_label} vs {x_label} — By Source (≥{threshold*100:.0f}% Dominant)',\n                fontsize=14, fontweight='bold', y=1.02)\n    \n    return fig, results\n\n\nprint(\"=\"*80)\nprint(\"TASK 3: THRESHOLD-FILTERED REGRESSIONS — ALL METHOD PAIRS\")\nprint(\"=\"*80)\n\n# Show threshold sample counts (same for all pairs)\nanalyze_threshold_filtering(df)\n\nall_thresh_results = {}\n\nfor x_col, y_col, x_label, y_label, prefix in METHOD_PAIRS:\n    pair_label = f'{y_label.split(\" (\")[0]} vs {x_label.split(\" (\")[0]}'\n    print(f\"\\n{'='*60}\")\n    print(f\"  {pair_label} — Threshold Comparison\")\n    print(f\"{'='*60}\")\n    \n    # Threshold comparison strip plot\n    fig, thresh_results = plot_threshold_comparison(\n        df, x_col, y_col, x_label, y_label\n    )\n    plt.savefig(os.path.join(dirs['plots'], f'{prefix}_threshold_comparison.png'), dpi=150, bbox_inches='tight')\n    plt.show()\n    \n    all_thresh_results[prefix] = thresh_results\n    \n    print(f\"\\nR² by Threshold:\")\n    for key, val in thresh_results.items():\n        print(f\"  {key}: R² = {val['r_squared']:.3f}, n = {val['n']}\")\n    \n    # Source-separated at 50% threshold\n    print(f\"\\n--- {pair_label} — By Source (≥50% Dominant) ---\")\n    fig, thresh_source_results = plot_source_regressions_threshold_filtered(\n        df, x_col, y_col, x_label, y_label, threshold=0.50\n    )\n    plt.savefig(os.path.join(dirs['plots'], f'{prefix}_by_source_50pct.png'), dpi=150, bbox_inches='tight')\n    plt.show()"
  },
  {
   "cell_type": "markdown",
   "id": "04662d73",
   "metadata": {},
   "source": "---\n\n# Task 4: Source Contribution Bar Charts\n\n**Goal**: Visualize daily source contributions as stacked bar charts.\n\n## Stacking Method\n\nThe bars are stacked using a running `bottom` accumulator:\n\n```python\nbottom = np.zeros(len(valid_norm))  # Start at 0\n\nfor col, label, color in STACK_ORDER:\n    values = valid_norm[col].values\n    ax.bar(x_dates, values, width=bar_width, bottom=bottom, ...)\n    bottom += values  # Next source starts where this one ended\n```\n\nThis produces bars that all reach exactly 1.0 (100%), with each color segment \nshowing that source's relative contribution to the explained OM."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8719068f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_source_contributions_stacked_bars(df, date_col='date', max_samples=60):\n",
    "    \"\"\"\n",
    "    Create stacked bar chart of daily source contributions (normalized fractions).\n",
    "    \n",
    "    The fractions have already been normalized to sum to 1.0 (100%).\n",
    "    Uses a `bottom` accumulator to stack bars so they reach exactly 100%.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : DataFrame with normalized fraction columns\n",
    "    date_col : name of the date column\n",
    "    max_samples : maximum number of samples to show (for readability)\n",
    "    \"\"\"\n",
    "    # Filter to samples with source data\n",
    "    valid_norm = df.dropna(subset=FRAC_COLS).copy()\n",
    "    valid_norm = valid_norm.sort_values(date_col)\n",
    "    \n",
    "    if len(valid_norm) == 0:\n",
    "        print(\"No valid source apportionment data\")\n",
    "        return None\n",
    "    \n",
    "    # Limit to manageable number for visualization\n",
    "    if len(valid_norm) > max_samples:\n",
    "        valid_norm = valid_norm.tail(max_samples)\n",
    "        print(f\"Showing last {max_samples} samples\")\n",
    "    \n",
    "    # Verify normalization\n",
    "    row_sums = valid_norm[FRAC_COLS].sum(axis=1)\n",
    "    print(f\"\\nVerifying normalization for bar chart:\")\n",
    "    print(f\"  Row sums: min={row_sums.min():.4f}, max={row_sums.max():.4f}, mean={row_sums.mean():.4f}\")\n",
    "    print(f\"  (Should all be ~1.0)\")\n",
    "    \n",
    "    # Create figure\n",
    "    fig, ax = plt.subplots(figsize=(16, 6))\n",
    "    \n",
    "    # X-axis: sample indices\n",
    "    x = np.arange(len(valid_norm))\n",
    "    bar_width = 0.8\n",
    "    \n",
    "    # ==========================================================================\n",
    "    # STACKING: Use a running `bottom` accumulator\n",
    "    # ==========================================================================\n",
    "    bottom = np.zeros(len(valid_norm))  # Start at 0\n",
    "    \n",
    "    for col, label, color in STACK_ORDER:\n",
    "        # Get the normalized values for this source\n",
    "        values = valid_norm[col].values\n",
    "        \n",
    "        # Create bar starting at current `bottom`\n",
    "        ax.bar(x, values, bar_width, bottom=bottom, \n",
    "               color=color, label=label,\n",
    "               edgecolor='white', linewidth=0.5)\n",
    "        \n",
    "        # Update bottom for next source: this source's bars now form the base\n",
    "        bottom += values\n",
    "    \n",
    "    # ==========================================================================\n",
    "    # Formatting\n",
    "    # ==========================================================================\n",
    "    \n",
    "    # X-axis labels (show every 3rd date for readability)\n",
    "    date_labels = valid_norm[date_col].dt.strftime('%m/%d')\n",
    "    ax.set_xticks(x[::3])\n",
    "    ax.set_xticklabels(date_labels.iloc[::3], rotation=45, ha='right', fontsize=8)\n",
    "    \n",
    "    ax.set_xlabel('Date', fontsize=12)\n",
    "    ax.set_ylabel('Source Fraction (Normalized)', fontsize=12)\n",
    "    ax.set_title('Daily Source Contributions — Normalized to 100%', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # Legend outside plot\n",
    "    ax.legend(loc='upper left', bbox_to_anchor=(1.01, 1), fontsize=9)\n",
    "    \n",
    "    # Y-axis should go from 0 to 1.0 (or slightly above for visibility)\n",
    "    ax.set_ylim(0, 1.05)\n",
    "    ax.set_yticks([0, 0.25, 0.5, 0.75, 1.0])\n",
    "    ax.set_yticklabels(['0%', '25%', '50%', '75%', '100%'])\n",
    "    \n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "\n",
    "def plot_source_contributions_summary(df):\n",
    "    \"\"\"\n",
    "    Summary visualization: overall average and seasonal breakdown.\n",
    "    \"\"\"\n",
    "    valid = df.dropna(subset=FRAC_COLS + ['Ethiopian_Season'])\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Panel 1: Overall average (pie chart)\n",
    "    ax = axes[0]\n",
    "    avg_fracs = [valid[col].mean() for col in FRAC_COLS]\n",
    "    colors = [SOURCE_CATEGORIES[c.replace('_frac', '')]['color'] for c in FRAC_COLS]\n",
    "    labels = [SOURCE_CATEGORIES[c.replace('_frac', '')]['label'] for c in FRAC_COLS]\n",
    "    \n",
    "    wedges, texts, autotexts = ax.pie(avg_fracs, labels=None, autopct='%1.1f%%',\n",
    "                                       colors=colors, startangle=90,\n",
    "                                       wedgeprops={'edgecolor': 'white', 'linewidth': 1})\n",
    "    ax.legend(wedges, labels, loc='center left', bbox_to_anchor=(0.85, 0.5), fontsize=9)\n",
    "    ax.set_title(f'Overall Source Contributions\\n(n={len(valid)} samples)', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    # Panel 2: By season (grouped bars)\n",
    "    ax = axes[1]\n",
    "    \n",
    "    x = np.arange(len(SEASONS_ORDER))\n",
    "    n_sources = len(FRAC_COLS)\n",
    "    width = 0.15\n",
    "    \n",
    "    for i, col in enumerate(FRAC_COLS):\n",
    "        source_key = col.replace('_frac', '')\n",
    "        means = []\n",
    "        stds = []\n",
    "        for season in SEASONS_ORDER:\n",
    "            season_data = valid[valid['Ethiopian_Season'] == season][col]\n",
    "            means.append(season_data.mean() if len(season_data) > 0 else 0)\n",
    "            stds.append(season_data.std() if len(season_data) > 1 else 0)\n",
    "        \n",
    "        offset = (i - n_sources/2 + 0.5) * width\n",
    "        ax.bar(x + offset, means, width, yerr=stds, capsize=2,\n",
    "               color=SOURCE_CATEGORIES[source_key]['color'],\n",
    "               label=SOURCE_CATEGORIES[source_key]['label'],\n",
    "               edgecolor='black', linewidth=0.5)\n",
    "    \n",
    "    ax.set_xlabel('Ethiopian Season', fontsize=11)\n",
    "    ax.set_ylabel('Mean Fraction', fontsize=11)\n",
    "    ax.set_title('Source Contributions by Season', fontsize=12, fontweight='bold')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels([s.replace(' Season', '') for s in SEASONS_ORDER], rotation=15)\n",
    "    ax.legend(fontsize=8, loc='upper right')\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "\n",
    "def plot_dominant_source_timeseries(df):\n",
    "    \"\"\"\n",
    "    Time series showing which source is dominant each day and its fraction.\n",
    "    \"\"\"\n",
    "    valid = df.dropna(subset=['dominant_source', 'dominant_fraction']).copy()\n",
    "    valid = valid.sort_values('date')\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 1, figsize=(16, 8), sharex=True,\n",
    "                              gridspec_kw={'height_ratios': [2, 1]})\n",
    "    \n",
    "    # Panel 1: Dominant fraction colored by source\n",
    "    ax = axes[0]\n",
    "    for source in SOURCE_ORDER:\n",
    "        mask = valid['dominant_source'] == source\n",
    "        if mask.sum() > 0:\n",
    "            ax.scatter(valid.loc[mask, 'date'], valid.loc[mask, 'dominant_fraction'],\n",
    "                      s=50, alpha=0.7, color=SOURCE_CATEGORIES[source]['color'],\n",
    "                      marker=SOURCE_CATEGORIES[source]['marker'],\n",
    "                      label=SOURCE_CATEGORIES[source]['label'],\n",
    "                      edgecolors='black', linewidth=0.3)\n",
    "    \n",
    "    ax.axhline(0.5, color='red', linestyle='--', linewidth=1, alpha=0.5, label='50% threshold')\n",
    "    ax.set_ylabel('Dominant Source Fraction', fontsize=11)\n",
    "    ax.set_title('Dominant Source Time Series', fontsize=13, fontweight='bold')\n",
    "    ax.legend(fontsize=8, loc='upper right', ncol=2)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_ylim(0, 1)\n",
    "    \n",
    "    # Panel 2: Source category (categorical)\n",
    "    ax = axes[1]\n",
    "    source_map = {s: i for i, s in enumerate(SOURCE_ORDER)}\n",
    "    valid['source_idx'] = valid['dominant_source'].map(source_map)\n",
    "    \n",
    "    for source in SOURCE_ORDER:\n",
    "        mask = valid['dominant_source'] == source\n",
    "        if mask.sum() > 0:\n",
    "            ax.scatter(valid.loc[mask, 'date'], valid.loc[mask, 'source_idx'],\n",
    "                      s=40, alpha=0.8, color=SOURCE_CATEGORIES[source]['color'],\n",
    "                      marker='|')\n",
    "    \n",
    "    ax.set_yticks(range(len(SOURCE_ORDER)))\n",
    "    ax.set_yticklabels([SOURCE_CATEGORIES[s]['label'] for s in SOURCE_ORDER], fontsize=9)\n",
    "    ax.set_xlabel('Date', fontsize=11)\n",
    "    ax.set_ylabel('Dominant Source', fontsize=11)\n",
    "    ax.grid(True, alpha=0.3, axis='x')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"TASK 5: SOURCE CONTRIBUTION VISUALIZATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Daily stacked bar chart (main visualization)\n",
    "print(\"\\n--- Stacked Bar Chart (Normalized to 100%) ---\")\n",
    "fig = plot_source_contributions_stacked_bars(df)\n",
    "if fig:\n",
    "    plt.savefig(os.path.join(dirs['plots'], 'source_contributions_stacked_bars.png'), dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# Summary plots\n",
    "print(\"\\n--- Summary Visualizations ---\")\n",
    "fig = plot_source_contributions_summary(df)\n",
    "plt.savefig(os.path.join(dirs['plots'], 'source_contributions_summary.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Dominant source time series\n",
    "print(\"\\n--- Dominant Source Time Series ---\")\n",
    "fig = plot_dominant_source_timeseries(df)\n",
    "plt.savefig(os.path.join(dirs['plots'], 'dominant_source_timeseries.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Print verification stats\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SOURCE CONTRIBUTION VERIFICATION\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nNormalized fraction statistics (should sum to ~1.0 for each row):\")\n",
    "for col in FRAC_COLS:\n",
    "    if col in df.columns:\n",
    "        vals = df[col].dropna()\n",
    "        print(f\"  {col}: mean={vals.mean():.3f}, median={vals.median():.3f}, \"\n",
    "              f\"min={vals.min():.3f}, max={vals.max():.3f}\")\n",
    "\n",
    "row_sums = df[FRAC_COLS].sum(axis=1).dropna()\n",
    "print(f\"\\nRow sums: mean={row_sums.mean():.4f}, min={row_sums.min():.4f}, max={row_sums.max():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e5acca",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Summary Table: All Regression Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0fd2bf",
   "metadata": {},
   "outputs": [],
   "source": "print(\"=\"*80)\nprint(\"SUMMARY: ALL REGRESSION RESULTS\")\nprint(\"=\"*80)\n\nprint(\"\\nBaseline Method Comparison Summary:\")\nprint(\"-\" * 70)\n\nfor x_col, y_col, x_label, y_label, prefix in METHOD_PAIRS:\n    valid = df[[x_col, y_col]].dropna()\n    if len(valid) >= 3:\n        slope, intercept, r, p, se = stats.linregress(valid[x_col], valid[y_col])\n        pair_label = f'{y_label.split(\" (\")[0]} vs {x_label.split(\" (\")[0]}'\n        print(f\"\\n{pair_label}:\")\n        print(f\"  n = {len(valid)}\")\n        print(f\"  Slope = {slope:.3f}\")\n        print(f\"  Intercept = {intercept:.3f}\")\n        print(f\"  R² = {r**2:.3f}\")\n        print(f\"  p = {p:.2e}\")\n\nprint(\"\\n\\nThreshold R² Comparison Across Method Pairs:\")\nprint(\"-\" * 70)\nprint(f\"{'Method Pair':<30s}\", end='')\nprint(f\" {'All':>8s}\", end='')\nfor t in DOMINANCE_THRESHOLDS:\n    print(f\" {'≥'+str(int(t*100))+'%':>8s}\", end='')\nprint()\nprint(\"-\" * 70)\nfor x_col, y_col, x_label, y_label, prefix in METHOD_PAIRS:\n    pair_label = f'{y_label.split(\" (\")[0]} vs {x_label.split(\" (\")[0]}'\n    if prefix in all_thresh_results:\n        print(f\"{pair_label:<30s}\", end='')\n        tr = all_thresh_results[prefix]\n        if 'all' in tr:\n            print(f\" {tr['all']['r_squared']:>8.3f}\", end='')\n        else:\n            print(f\" {'N/A':>8s}\", end='')\n        for t in DOMINANCE_THRESHOLDS:\n            key = f'{t*100:.0f}%'\n            if key in tr:\n                print(f\" {tr[key]['r_squared']:>8.3f}\", end='')\n            else:\n                print(f\" {'N/A':>8s}\", end='')\n        print()"
  },
  {
   "cell_type": "markdown",
   "id": "98989e7a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Export Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3805ed3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save merged dataset\n",
    "output_path = os.path.join(dirs['data'], 'bc_ec_source_merged.csv')\n",
    "df.to_csv(output_path, index=False)\n",
    "print(f\"\\nSaved merged dataset to: {output_path}\")\n",
    "\n",
    "# Save summary statistics\n",
    "summary_stats = []\n",
    "for col in ['ftir_ec', 'hips_fabs', 'ir_bcc', 'uv_bcc']:\n",
    "    if col in df.columns:\n",
    "        vals = df[col].dropna()\n",
    "        if len(vals) > 0:\n",
    "            summary_stats.append({\n",
    "                'Variable': col,\n",
    "                'n': len(vals),\n",
    "                'Mean': vals.mean(),\n",
    "                'Std': vals.std(),\n",
    "                'Median': vals.median(),\n",
    "                'Min': vals.min(),\n",
    "                'Max': vals.max()\n",
    "            })\n",
    "\n",
    "stats_df = pd.DataFrame(summary_stats)\n",
    "stats_path = os.path.join(dirs['data'], 'bc_ec_summary_stats.csv')\n",
    "stats_df.to_csv(stats_path, index=False)\n",
    "print(f\"Saved summary statistics to: {stats_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ANALYSIS COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nPlots saved to: {dirs['plots']}\")\n",
    "print(f\"Data saved to: {dirs['data']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477ce930",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Appendix: Data Flow & Normalization Explained\n",
    "\n",
    "## How data is loaded and merged:\n",
    "\n",
    "1. **`load_etad_factors_with_filter_ids()`** loads both CSVs and joins on `oldDate`:\n",
    "   - `ETAD Factor Contributions .csv` (PMF fractions GF1-GF5 + concentrations K_F1-K_F5)\n",
    "   - `ETAD Filter ID.csv` (maps dates to FilterId like `ETAD-0035-3`)\n",
    "   - Produces `base_filter_id` (e.g., `ETAD-0035`) for joining to unified dataset\n",
    "\n",
    "2. **`match_all_parameters()`** loads FTIR EC, HIPS Fabs, and Aethalometer BC from the\n",
    "   unified filter dataset, matched by date\n",
    "\n",
    "3. **Final merge** joins BC/EC measurements to factor contributions on `base_filter_id`\n",
    "\n",
    "## Normalization Process:\n",
    "\n",
    "### Step 1: Calculate Row Totals\n",
    "The raw GF fractions don't sum to 1.0 (they sum to ~0.03–0.46). We normalize them:\n",
    "\n",
    "```python\n",
    "# Calculate row totals\n",
    "row_totals = factors_df[frac_cols].sum(axis=1)\n",
    "\n",
    "# Divide each fraction by the total so they sum to 1.0\n",
    "for col in frac_cols:\n",
    "    factors_df[col] = factors_df[col] / row_totals\n",
    "```\n",
    "\n",
    "### Example for one day:\n",
    "\n",
    "| Source | Raw GF | Normalized |\n",
    "|--------|--------|------------|\n",
    "| Fossil Fuel | 0.05 | 0.05/0.25 = 0.20 |\n",
    "| Polluted Marine | 0.03 | 0.03/0.25 = 0.12 |\n",
    "| Sea Salt | 0.02 | 0.02/0.25 = 0.08 |\n",
    "| Wood Burning | 0.07 | 0.07/0.25 = 0.28 |\n",
    "| Charcoal | 0.08 | 0.08/0.25 = 0.32 |\n",
    "| **Total** | **0.25** | **1.00** |\n",
    "\n",
    "### Step 2: Stacking for Bar Charts\n",
    "The bars are stacked using a running `bottom` accumulator:\n",
    "\n",
    "```python\n",
    "bottom = np.zeros(len(valid_norm))  # Start at 0\n",
    "\n",
    "for col, label, color in STACK_ORDER:\n",
    "    values = valid_norm[col].values\n",
    "    ax.bar(x_dates, values, width=bar_width, bottom=bottom, ...)\n",
    "    bottom += values  # Next source starts where this one ended\n",
    "```\n",
    "\n",
    "This produces bars that all reach exactly 1.0 (100%), with each color segment showing \n",
    "that source's relative contribution to the explained OM.\n",
    "\n",
    "### Key Point\n",
    "The normalization changes the interpretation:\n",
    "- **Raw GF**: Fraction of total PM₂.₅ mass from each source\n",
    "- **Normalized**: Fraction of **explained OM** from each source (always sums to 100%)\n",
    "\n",
    "## Key parameters:\n",
    "- `DOMINANCE_THRESHOLDS`: List of thresholds to test (default: 30%, 40%, 50%, 60%)\n",
    "- `MAC_VALUE`: Mass absorption coefficient for HIPS conversion (default: 10 m²/g)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}