{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89f17b31",
   "metadata": {},
   "source": [
    "# PKL Data Cleaning Pipeline Demo\n",
    "\n",
    "This notebook demonstrates how to use the PKL data cleaning pipeline with configurable data directory paths.\n",
    "\n",
    "The PKL cleaning pipeline provides comprehensive data cleaning for aethalometer data in PKL format, including:\n",
    "- Status-based cleaning using external calibration\n",
    "- Optical saturation removal\n",
    "- Flow validation and range checking\n",
    "- Temperature change detection\n",
    "- Roughness-based quality control\n",
    "- DEMA smoothing\n",
    "\n",
    "## Key Features\n",
    "- **Configurable data directory**: Set your own path instead of hardcoded paths\n",
    "- **Modular design**: Use individual cleaning methods or the complete pipeline\n",
    "- **External calibration**: Preserves external calibration script as-is\n",
    "- **Comprehensive reporting**: Track data removal at each step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afecbe29",
   "metadata": {},
   "source": [
    "## Setup and Configuration\n",
    "\n",
    "First, let's configure the data directory path. **Change this to match your actual data location!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4531d9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Data directory configured: /Users/ahzs645/Library/CloudStorage/GoogleDrive-ahzs645@gmail.com/My Drive/University/Research/Grad/UC Davis Ann/NASA MAIA/Data/Aethelometry Data/Kyan Data/Mergedcleaned and uncleaned MA350 data20250707030704/\n",
      "üìç Directory exists: True\n",
      "‚úÖ Found directory with 2 items\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIGURE YOUR DATA DIRECTORY HERE\n",
    "# =============================================================================\n",
    "\n",
    "# Option 1: Use relative path (default)\n",
    "data_directory = \"/Users/ahzs645/Library/CloudStorage/GoogleDrive-ahzs645@gmail.com/My Drive/University/Research/Grad/UC Davis Ann/NASA MAIA/Data/Aethelometry Data/Kyan Data/Mergedcleaned and uncleaned MA350 data20250707030704/\"\n",
    "\n",
    "# Option 2: Use absolute path (recommended for production)\n",
    "# data_directory = \"/Users/your-username/path/to/your/pkl/data/\"\n",
    "\n",
    "# Option 3: Use environment variable\n",
    "# data_directory = os.getenv('PKL_DATA_PATH', '../JPL_aeth/')\n",
    "\n",
    "# Option 4: Interactive input\n",
    "# data_directory = input(\"Enter path to PKL data directory: \")\n",
    "\n",
    "print(f\"üìÅ Data directory configured: {data_directory}\")\n",
    "print(f\"üìç Directory exists: {os.path.exists(data_directory)}\")\n",
    "\n",
    "if not os.path.exists(data_directory):\n",
    "    print(\"‚ö†Ô∏è  Warning: Directory does not exist. Please update the path above.\")\n",
    "else:\n",
    "    print(f\"‚úÖ Found directory with {len(os.listdir(data_directory))} items\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488645cc",
   "metadata": {},
   "source": [
    "## Import PKL Cleaning Modules\n",
    "\n",
    "Import the PKL cleaning functionality from the aethmodular package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b244b78",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'src'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Import PKL cleaning functionality\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mqc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PKLDataCleaner, load_and_clean_pkl_data\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m‚úÖ PKL cleaning modules imported successfully\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'src'"
     ]
    }
   ],
   "source": [
    "# Import PKL cleaning functionality\n",
    "from src.data.qc import PKLDataCleaner, load_and_clean_pkl_data\n",
    "\n",
    "print(\"‚úÖ PKL cleaning modules imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197da6cf",
   "metadata": {},
   "source": [
    "## Method 1: Using the PKLDataCleaner Class (Recommended)\n",
    "\n",
    "Create a PKLDataCleaner instance with your configured data directory. This is the recommended approach as it encapsulates the configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8c3201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create PKL data cleaner with custom data directory\n",
    "cleaner = PKLDataCleaner(\n",
    "    data_directory=data_directory,\n",
    "    wavelengths_to_filter=['IR', 'Blue']  # Optional: customize wavelengths\n",
    ")\n",
    "\n",
    "print(f\"üîß PKL cleaner initialized with data directory: {cleaner.data_directory}\")\n",
    "print(f\"üìä Wavelengths to filter: {cleaner.wls_to_filter}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a02bcb4",
   "metadata": {},
   "source": [
    "### Load and Clean Data Using the Instance Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609f86c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and clean data using the instance method\n",
    "# This will use the data_directory specified when creating the cleaner\n",
    "try:\n",
    "    df_cleaned = cleaner.load_and_clean_data(\n",
    "        # Optional parameters for data loading\n",
    "        verbose=True,\n",
    "        summary=True,\n",
    "        file_number_printout=True\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nüìã Cleaned data summary:\")\n",
    "    print(f\"   Shape: {df_cleaned.shape}\")\n",
    "    print(f\"   Date range: {df_cleaned['datetime_local'].min()} to {df_cleaned['datetime_local'].max()}\")\n",
    "    print(f\"   Columns: {list(df_cleaned.columns[:10])}{'...' if len(df_cleaned.columns) > 10 else ''}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading data: {e}\")\n",
    "    print(\"üí° Make sure your data directory path is correct and contains PKL files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37afe49b",
   "metadata": {},
   "source": [
    "## Method 2: Using the Standalone Function\n",
    "\n",
    "Alternatively, you can use the standalone function with a custom directory path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32bc3a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and clean data using standalone function\n",
    "try:\n",
    "    df_cleaned_v2 = load_and_clean_pkl_data(\n",
    "        directory_path=data_directory,\n",
    "        verbose=False,\n",
    "        summary=False\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ Data loaded using standalone function\")\n",
    "    print(f\"üìã Shape: {df_cleaned_v2.shape}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error with standalone function: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689e2799",
   "metadata": {},
   "source": [
    "## Individual Cleaning Steps\n",
    "\n",
    "You can also apply individual cleaning steps if you want more control over the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212bd19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Apply individual cleaning steps\n",
    "if 'df_cleaned' in locals():\n",
    "    # Start with a subset for demonstration\n",
    "    df_sample = df_cleaned.head(1000).copy()\n",
    "    \n",
    "    print(\"üîß Applying individual cleaning steps:\")\n",
    "    print(f\"   Original sample size: {len(df_sample)}\")\n",
    "    \n",
    "    # Apply status cleaning\n",
    "    df_step1 = cleaner.clean_by_status(df_sample)\n",
    "    \n",
    "    # Apply optical saturation cleaning\n",
    "    df_step2 = cleaner.clean_optical_saturation(df_step1)\n",
    "    \n",
    "    # Apply flow range cleaning\n",
    "    df_step3 = cleaner.clean_flow_range(df_step2)\n",
    "    \n",
    "    print(f\"   Final sample size: {len(df_step3)}\")\n",
    "    print(f\"   Total removed: {len(df_sample) - len(df_step3)} rows\")\n",
    "else:\n",
    "    print(\"‚è≠Ô∏è  Skipping individual steps demo (no data loaded)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d9cf72",
   "metadata": {},
   "source": [
    "## Data Quality Assessment\n",
    "\n",
    "Analyze the quality of the cleaned data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06fc95a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'df_cleaned' in locals():\n",
    "    print(\"üìä Data Quality Assessment\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Basic statistics\n",
    "    print(f\"Total data points: {len(df_cleaned):,}\")\n",
    "    print(f\"Date range: {(df_cleaned['datetime_local'].max() - df_cleaned['datetime_local'].min()).days} days\")\n",
    "    \n",
    "    # Check for missing values\n",
    "    missing_cols = df_cleaned.isnull().sum()\n",
    "    missing_cols = missing_cols[missing_cols > 0]\n",
    "    \n",
    "    if len(missing_cols) > 0:\n",
    "        print(\"\\n‚ùó Columns with missing values:\")\n",
    "        for col, count in missing_cols.head(5).items():\n",
    "            print(f\"   {col}: {count:,} ({count/len(df_cleaned)*100:.2f}%)\")\n",
    "    else:\n",
    "        print(\"\\n‚úÖ No missing values found\")\n",
    "    \n",
    "    # Check unique instruments\n",
    "    if 'Serial number' in df_cleaned.columns:\n",
    "        instruments = df_cleaned['Serial number'].unique()\n",
    "        print(f\"\\nüì± Instruments: {len(instruments)} unique\")\n",
    "        for inst in instruments[:5]:\n",
    "            count = (df_cleaned['Serial number'] == inst).sum()\n",
    "            print(f\"   {inst}: {count:,} data points\")\n",
    "    \n",
    "    # Display sample of cleaned data\n",
    "    print(\"\\nüìã Sample of cleaned data:\")\n",
    "    display_cols = ['datetime_local', 'Serial number'] if 'Serial number' in df_cleaned.columns else ['datetime_local']\n",
    "    display_cols.extend([col for col in df_cleaned.columns if 'BC' in col][:3])\n",
    "    print(df_cleaned[display_cols].head())\n",
    "    \n",
    "else:\n",
    "    print(\"‚è≠Ô∏è  No data available for quality assessment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7388fe",
   "metadata": {},
   "source": [
    "## Configuration for Different Environments\n",
    "\n",
    "Here are examples of how to configure the data directory for different environments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7b072f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîß Configuration Examples\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "print(\"\\n1Ô∏è‚É£  Development Environment:\")\n",
    "print('   cleaner = PKLDataCleaner(data_directory=\"../data/pkl/\")')\n",
    "\n",
    "print(\"\\n2Ô∏è‚É£  Production Environment:\")\n",
    "print('   cleaner = PKLDataCleaner(data_directory=\"/opt/data/aethalometer/pkl/\")')\n",
    "\n",
    "print(\"\\n3Ô∏è‚É£  Using Environment Variable:\")\n",
    "print('   import os')\n",
    "print('   data_dir = os.getenv(\"AETH_DATA_PATH\", \"../JPL_aeth/\")')\n",
    "print('   cleaner = PKLDataCleaner(data_directory=data_dir)')\n",
    "\n",
    "print(\"\\n4Ô∏è‚É£  User Configuration File:\")\n",
    "print('   import json')\n",
    "print('   with open(\"config.json\") as f:')\n",
    "print('       config = json.load(f)')\n",
    "print('   cleaner = PKLDataCleaner(data_directory=config[\"pkl_data_path\"])')\n",
    "\n",
    "print(\"\\n5Ô∏è‚É£  Command Line Argument:\")\n",
    "print('   import sys')\n",
    "print('   data_dir = sys.argv[1] if len(sys.argv) > 1 else \"../JPL_aeth/\"')\n",
    "print('   cleaner = PKLDataCleaner(data_directory=data_dir)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d374ab7",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "After cleaning your PKL data, you can:\n",
    "\n",
    "1. **Quality Control Analysis**: Use other QC modules for comprehensive quality assessment\n",
    "2. **Visualization**: Create plots and visualizations of the cleaned data\n",
    "3. **Export Results**: Save cleaned data to various formats\n",
    "4. **Integration**: Integrate with other analysis pipelines\n",
    "\n",
    "Example integration with other QC modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a81b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Integrate with other QC modules\n",
    "try:\n",
    "    from src.data.qc import quick_quality_check\n",
    "    \n",
    "    if 'df_cleaned' in locals():\n",
    "        print(\"üîç Running quick quality check on cleaned PKL data:\")\n",
    "        quick_quality_check(df_cleaned.set_index('datetime_local'), freq='min')\n",
    "    else:\n",
    "        print(\"‚è≠Ô∏è  No cleaned data available for quality check\")\n",
    "        \n",
    "except ImportError as e:\n",
    "    print(f\"‚ö†Ô∏è  QC modules not available: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error running quality check: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613131fe",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "‚úÖ **Configurable Data Paths**: How to set custom data directory paths instead of hardcoded ones\n",
    "\n",
    "‚úÖ **Multiple Usage Patterns**: Class-based and function-based approaches\n",
    "\n",
    "‚úÖ **Individual Cleaning Steps**: Fine-grained control over the cleaning process\n",
    "\n",
    "‚úÖ **Quality Assessment**: Basic data quality checks after cleaning\n",
    "\n",
    "‚úÖ **Environment Configuration**: Examples for different deployment scenarios\n",
    "\n",
    "The PKL cleaning pipeline is now properly integrated into the aethmodular package structure while preserving the external calibration script for easy updates."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
