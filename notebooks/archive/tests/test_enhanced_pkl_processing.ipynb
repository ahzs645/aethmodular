{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enhanced PKL Processing Test Notebook\n",
    "\n",
    "This notebook tests the integrated enhanced PKL processing functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Advanced plotting style configured\n",
      "üöÄ Aethalometer-FTIR/HIPS Pipeline with Simplified Setup\n",
      "============================================================\n",
      "üìä Configuration Summary:\n",
      "   Site: ETAD\n",
      "   Wavelength: Red\n",
      "   Output format: jpl\n",
      "   Quality threshold: 10 minutes\n",
      "   Output directory: outputs\n",
      "\n",
      "üìÅ File paths:\n",
      "   pkl_data: ‚úÖ df_uncleaned_Jacros_API_and_OG.pkl\n",
      "   csv_data: ‚úÖ Jacros_MA350_1-min_2022-2024_Cleaned.csv\n",
      "   FTIR DB: ‚úÖ spartan_ftir_hips.db\n",
      "üßπ Enhanced setup with PKL cleaning capabilities loaded\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "import sys\n",
    "import os\n",
    "sys.path.insert(0, os.path.join(os.path.dirname(os.getcwd()), 'src'))\n",
    "\n",
    "# Import the new enhanced PKL processing module\n",
    "from data.qc.enhanced_pkl_processing import process_pkl_data_enhanced, EnhancedPKLProcessor\n",
    "from config.notebook_config import NotebookConfig\n",
    "from notebook_utils.pkl_cleaning_integration import create_enhanced_setup\n",
    "\n",
    "# Your existing configuration\n",
    "config = NotebookConfig(\n",
    "    site_code='ETAD',\n",
    "    wavelength='Red',\n",
    "    quality_threshold=10,\n",
    "    output_format='jpl',\n",
    "    min_samples_for_analysis=30,\n",
    "    confidence_level=0.95,\n",
    "    outlier_threshold=3.0,\n",
    "    figure_size=(12, 8),\n",
    "    font_size=10,\n",
    "    dpi=300\n",
    ")\n",
    "\n",
    "# Set your data paths (same as before)\n",
    "base_data_path = \"/Users/ahzs645/Library/CloudStorage/GoogleDrive-ahzs645@gmail.com/My Drive/University/Research/Grad/UC Davis Ann/NASA MAIA/Data\"\n",
    "\n",
    "config.aethalometer_files = {\n",
    "    'pkl_data': os.path.join(\n",
    "        base_data_path,\n",
    "        \"Aethelometry Data/Kyan Data/Mergedcleaned and uncleaned MA350 data20250707030704\",\n",
    "        \"df_uncleaned_Jacros_API_and_OG.pkl\"\n",
    "    ),\n",
    "    'csv_data': os.path.join(\n",
    "        base_data_path,\n",
    "        \"Aethelometry Data/Raw\",\n",
    "        \"Jacros_MA350_1-min_2022-2024_Cleaned.csv\"\n",
    "    )\n",
    "}\n",
    "\n",
    "config.ftir_db_path = os.path.join(\n",
    "    base_data_path,\n",
    "    \"EC-HIPS-Aeth Comparison/Data/Original Data/Combined Database\",\n",
    "    \"spartan_ftir_hips.db\"\n",
    ")\n",
    "\n",
    "# Create enhanced setup\n",
    "setup = create_enhanced_setup(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Loading datasets...\n",
      "üì¶ Setting up modular system...\n",
      "‚úÖ Aethalometer loaders imported\n",
      "‚úÖ Database loader imported\n",
      "‚úÖ Plotting utilities imported\n",
      "‚úÖ Plotting style configured\n",
      "‚úÖ Successfully imported 5 modular components\n",
      "\n",
      "============================================================\n",
      "üìÅ LOADING DATASETS\n",
      "============================================================\n",
      "üìÅ Loading all datasets...\n",
      "\n",
      "==================================================\n",
      "üìä Loading pkl_data\n",
      "==================================================\n",
      "üìÅ Loading pkl_data: df_uncleaned_Jacros_API_and_OG.pkl\n",
      "Detected format: standard\n",
      "Set 'datetime_local' as DatetimeIndex for time series operations\n",
      "Converted 17 columns to JPL format\n",
      "Warning: Missing recommended columns: ['datetime_local', 'Biomass.BCc', 'Fossil.fuel.BCc']\n",
      "‚úÖ Modular load: 1,665,156 rows √ó 238 columns\n",
      "üìä Method: modular\n",
      "üìä Format: jpl\n",
      "üìä Memory: 7443.05 MB\n",
      "üßÆ BC columns: 30\n",
      "üìà ATN columns: 25\n",
      "üìÖ Time range: 2021-01-09 16:38:00 to 2025-06-26 23:18:00\n",
      "‚úÖ pkl_data loaded successfully\n",
      "\n",
      "==================================================\n",
      "üìä Loading csv_data\n",
      "==================================================\n",
      "üìÅ Loading csv_data: Jacros_MA350_1-min_2022-2024_Cleaned.csv\n",
      "Set 'Time (Local)' as DatetimeIndex for time series operations\n",
      "Converted 5 columns to JPL format\n",
      "‚úÖ Modular load: 1,095,086 rows √ó 77 columns\n",
      "üìä Method: modular\n",
      "üìä Format: jpl\n",
      "üìä Memory: 884.83 MB\n",
      "üßÆ BC columns: 15\n",
      "üìà ATN columns: 10\n",
      "üìÖ Time range: 2022-04-12 12:46:01+03:00 to 2024-08-20 12:01:00+03:00\n",
      "‚úÖ csv_data loaded successfully\n",
      "\n",
      "==================================================\n",
      "üóÉÔ∏è Loading FTIR/HIPS data\n",
      "==================================================\n",
      "üóÉÔ∏è Loading FTIR/HIPS data for site ETAD...\n",
      "üìä Available sites: ['ILNZ', 'ILHA', 'ZAJB', 'CAHA', 'CASH', 'AEAZ', 'AUMN', 'KRUL', 'MXMC', 'ZAPR', 'CHTS', 'ETAD', 'INDH', 'TWTA', 'USPA', 'TWKA', 'KRSE', 'PRFJ', 'BDDU', 'BIBU', 'USNO', 'IDBD', None]\n",
      "‚úÖ Modular FTIR load: 168 samples\n",
      "üìÖ Date range: 2022-12-07 00:00:00 to 2024-05-12 00:00:00\n",
      "‚úÖ FTIR/HIPS data loaded successfully\n",
      "\n",
      "üìä Loading summary: 3 datasets loaded\n",
      "\n",
      "üìä LOADING SUMMARY\n",
      "============================================================\n",
      "‚úÖ Successfully loaded 3 datasets\n",
      "   - pkl_data: 1,665,156 rows √ó 238 columns\n",
      "   - csv_data: 1,095,086 rows √ó 77 columns\n",
      "   - ftir_hips: 168 rows √ó 12 columns\n",
      "============================================================\n",
      "‚úÖ Converting datetime_local from index to column...\n",
      "üìä PKL data ready: (1665156, 239)\n",
      "üìÖ Date range: 2021-01-09 16:38:00 to 2025-06-26 23:18:00\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "print(\"üìÅ Loading datasets...\")\n",
    "datasets = setup.load_all_data()\n",
    "\n",
    "# Get PKL data\n",
    "pkl_data_original = setup.get_dataset('pkl_data')\n",
    "\n",
    "# Quick fix for datetime_local issue (same as before)\n",
    "if 'datetime_local' not in pkl_data_original.columns:\n",
    "    if pkl_data_original.index.name == 'datetime_local':\n",
    "        print(\"‚úÖ Converting datetime_local from index to column...\")\n",
    "        pkl_data_original = pkl_data_original.reset_index()\n",
    "    elif hasattr(pkl_data_original.index, 'tz'):\n",
    "        print(\"‚úÖ Creating datetime_local column from datetime index...\")\n",
    "        pkl_data_original['datetime_local'] = pkl_data_original.index\n",
    "        pkl_data_original = pkl_data_original.reset_index(drop=True)\n",
    "\n",
    "print(f\"üìä PKL data ready: {pkl_data_original.shape}\")\n",
    "print(f\"üìÖ Date range: {pkl_data_original['datetime_local'].min()} to {pkl_data_original['datetime_local'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Enhanced PKL Processing WITH Ethiopia Fix\n",
      "============================================================\n",
      "üöÄ Enhanced PKL Data Processing Pipeline\n",
      "============================================================\n",
      "üîß Comprehensive Preprocessing Pipeline\n",
      "============================================================\n",
      "Step 1: Processing datetime...\n",
      "\n",
      "Step 2: Fixing column names...\n",
      "‚úÖ Renamed 16 columns\n",
      "\n",
      "Step 3: Converting data types...\n",
      "Converted IR ATN1 to float.\n",
      "Converted UV ATN1 to float.\n",
      "Converted Blue ATN1 to float.\n",
      "Converted Green ATN1 to float.\n",
      "Converted Red ATN1 to float.\n",
      "‚úÖ Applied calibration.convert_to_float()\n",
      "\n",
      "Step 4: Adding Session ID...\n",
      "\n",
      "Step 5: Adding delta calculations...\n",
      "‚úÖ Applied calibration.add_deltas()\n",
      "\n",
      "Step 6: Final adjustments...\n",
      "‚úÖ Filtered to 2022+: 1,665,156 -> 1,627,058 rows\n",
      "üîÑ Applying DEMA Smoothing...\n",
      "========================================\n",
      "\n",
      "Processing IR wavelength...\n",
      "  Available BC columns: ['IR BC1', 'IR BC2', 'IR BCc']\n",
      "  ‚úÖ Created IR BC1 smoothed\n",
      "  ‚úÖ Created IR BC2 smoothed\n",
      "  ‚úÖ Created IR BCc smoothed\n",
      "\n",
      "Processing Blue wavelength...\n",
      "  Available BC columns: ['Blue BC1', 'Blue BC2', 'Blue BCc']\n",
      "  ‚úÖ Created Blue BC1 smoothed\n",
      "  ‚úÖ Created Blue BC2 smoothed\n",
      "  ‚úÖ Created Blue BCc smoothed\n",
      "\n",
      "üîß Applying Site-Specific Corrections...\n",
      "==================================================\n",
      "üîß Applying Ethiopia (ETAD) site corrections...\n",
      "  üìä Applying IR loading compensation fix...\n",
      "    üìà IR median K: 0.012900\n",
      "    üéØ IR optimal K: 0.011910\n",
      "  üìä Applying Blue loading compensation fix...\n",
      "    üìà Blue median K: 0.009536\n",
      "    üéØ Blue optimal K: 0.009205\n",
      "  üìä Applying Red loading compensation fix...\n",
      "    üìà Red median K: 0.010900\n",
      "    üéØ Red optimal K: 0.010451\n",
      "  üìä Applying Green loading compensation fix...\n",
      "    üìà Green median K: 0.009947\n",
      "    üéØ Green optimal K: 0.005172\n",
      "  üìä Applying UV loading compensation fix...\n",
      "    üìà UV median K: 0.008200\n",
      "    üéØ UV optimal K: 0.013024\n",
      "‚úÖ Site corrections completed successfully!\n",
      "\n",
      "üßπ Final Cleaning Pipeline\n",
      "============================================================\n",
      "Starting PKL data cleaning pipeline...\n",
      "==================================================\n",
      "üîç Data Structure Diagnosis:\n",
      "------------------------------\n",
      "DataFrame shape: (1627058, 309)\n",
      "Date range: 2022-04-12 09:12:00 to 2025-06-26 23:18:00\n",
      "BC columns: 30 (e.g., ['Blue BC1', 'Blue BC2', 'Blue BCc'])\n",
      "BC smoothed columns: 6 (e.g., ['IR BC1 smoothed', 'IR BC2 smoothed', 'IR BCc smoothed'])\n",
      "ATN columns: 40 (e.g., ['Blue ATN1', 'Blue ATN2', 'Green ATN1'])\n",
      "Flow columns: 4 (e.g., ['Flow setpoint (mL/min)', 'Flow total (mL/min)', 'Flow1 (mL/min)'])\n",
      "\n",
      "Targeted wavelengths: ['IR', 'Blue']\n",
      "  IR: ‚úÖ BC | ‚úÖ BC smoothed | ‚úÖ ATN\n",
      "  Blue: ‚úÖ BC | ‚úÖ BC smoothed | ‚úÖ ATN\n",
      "------------------------------\n",
      "\n",
      "üßπ Starting cleaning steps...\n",
      "1919 datapoints removed due to Start up or Tape advance status\n",
      "Statuses of concern, count by device and status:\n",
      "\n",
      "MA350-0238 Flow unstable 750\n",
      "MA350-0238 Optical saturation 0\n",
      "MA350-0238 Sample timing error 0\n",
      "Number of datapoints with invalid optics values\n",
      "AFTER dropping data with 'Optical saturation' status values: 802\n",
      "Removed 56128 datapoints for optics\n",
      "Status cleaning: Removed 58797 rows (3.61%)\n",
      "Extreme BCc cleaning: Removed 13900 rows (0.89%)\n",
      "Flow range cleaning: Removed 0 rows (0.00%)\n",
      "Abnormal flow ratio: Removed 29362 rows (1.89%)\n",
      "Leak ratio cleaning: Removed 507 rows (0.03%)\n",
      "BCc denominator cleaning: Removed 25537 rows (1.68%)\n",
      "Sharp change 605\n",
      "noise 1425\n",
      "Temperature change cleaning: Removed 2030 rows (0.14%)\n",
      "IR ATN1_roughness: threshold=0.1147, high periods flagged: 17945 rows so far\n",
      "IR ATN2_roughness: threshold=0.1008, high periods flagged: 18439 rows so far\n",
      "Blue ATN1_roughness: threshold=0.1859, high periods flagged: 19114 rows so far\n",
      "Blue ATN2_roughness: threshold=0.1664, high periods flagged: 19142 rows so far\n",
      "==================================================\n",
      "Cleaning complete! Final data shape: (1477783, 318)\n",
      "\n",
      "üìä Processing Results Summary:\n",
      "============================================================\n",
      "Original data points: 1,665,156\n",
      "After preprocessing: 1,627,058\n",
      "After smoothing: 1,627,058\n",
      "After site corrections: 1,627,058\n",
      "Final cleaned: 1,477,783\n",
      "Total removed: 187,373 (11.25%)\n",
      "\n",
      "‚úÖ PKL data processing completed successfully!\n",
      "\n",
      "üìä Final data verification:\n",
      "Shape: (1477783, 318)\n",
      "Date range: 2022-04-12 09:54:00 to 2025-06-26 23:18:00\n",
      "  ‚úÖ IR ATN1\n",
      "  ‚úÖ IR BCc\n",
      "  ‚úÖ Blue ATN1\n",
      "  ‚úÖ Blue BCc\n",
      "  ‚úÖ Flow total (mL/min)\n",
      "  ‚úÖ Smoothed columns: 6\n",
      "\n",
      "üíæ Cleaned data exported:\n",
      "  üìÑ CSV: pkl_data_cleaned_ethiopia.csv\n",
      "  üì¶ Pickle: pkl_data_cleaned_ethiopia.pkl\n",
      "\n",
      "‚úÖ Processing complete: (1477783, 318)\n",
      "\n",
      "üîß Ethiopia correction columns added (27):\n",
      "  ‚Ä¢ Blue BCc denominator\n",
      "  ‚Ä¢ Blue BCc_corrected\n",
      "  ‚Ä¢ Blue BCc_manual\n",
      "  ‚Ä¢ Blue BCc_optimized\n",
      "  ‚Ä¢ Blue denominator_manual\n",
      "  ‚Ä¢ Blue denominator_optimized\n",
      "  ‚Ä¢ Green BCc_corrected\n",
      "  ‚Ä¢ Green BCc_manual\n",
      "  ‚Ä¢ Green BCc_optimized\n",
      "  ‚Ä¢ Green denominator_manual\n",
      "  ... and 17 more\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# ENHANCED PKL PROCESSING with optional Ethiopia fix\n",
    "\n",
    "# üéõÔ∏è Configuration: Toggle Ethiopia fix here\n",
    "APPLY_ETHIOPIA_FIX = True  # Set to True to enable Ethiopia pneumatic pump fix\n",
    "\n",
    "print(f\"üöÄ Enhanced PKL Processing {'WITH' if APPLY_ETHIOPIA_FIX else 'WITHOUT'} Ethiopia Fix\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "pkl_data_cleaned = process_pkl_data_enhanced(\n",
    "    pkl_data_original,\n",
    "    wavelengths_to_filter=['IR', 'Blue'],\n",
    "    export_path=f'pkl_data_cleaned_{\"ethiopia\" if APPLY_ETHIOPIA_FIX else \"standard\"}',\n",
    "    apply_ethiopia_fix=APPLY_ETHIOPIA_FIX,  # üîß Ethiopia fix toggle\n",
    "    site_code='ETAD' if APPLY_ETHIOPIA_FIX else None,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Processing complete: {pkl_data_cleaned.shape}\")\n",
    "\n",
    "# Show what Ethiopia corrections were added (if any)\n",
    "if APPLY_ETHIOPIA_FIX:\n",
    "    ethiopia_cols = [col for col in pkl_data_cleaned.columns if any(x in col for x in ['corrected', 'manual', 'optimized', 'denominator'])]\n",
    "    if ethiopia_cols:\n",
    "        print(f\"\\nüîß Ethiopia correction columns added ({len(ethiopia_cols)}):\")\n",
    "        for col in sorted(ethiopia_cols)[:10]:  # Show first 10\n",
    "            print(f\"  ‚Ä¢ {col}\")\n",
    "        if len(ethiopia_cols) > 10:\n",
    "            print(f\"  ... and {len(ethiopia_cols)-10} more\")\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è No Ethiopia correction columns found\")\n",
    "else:\n",
    "    print(\"\\nüìä Standard processing - no Ethiopia corrections applied\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Ethiopia Fix Validation:\n",
      "==================================================\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'src'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m50\u001b[39m)\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Use the validation functions from the site corrections module\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mprocessors\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msite_corrections\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SiteCorrections, apply_ethiopia_fix\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Create a small sample for comparison (to demonstrate the fix)\u001b[39;00m\n\u001b[32m     12\u001b[39m sample_size = \u001b[32m10000\u001b[39m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'src'"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# VALIDATION: Ethiopia fix validation (only runs if fix was applied)\n",
    "\n",
    "if APPLY_ETHIOPIA_FIX:\n",
    "    print(\"üìä Ethiopia Fix Validation:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Use the validation functions from the site corrections module\n",
    "    from src.data.processors.site_corrections import SiteCorrections, apply_ethiopia_fix\n",
    "    \n",
    "    # Create a small sample for comparison (to demonstrate the fix)\n",
    "    sample_size = 10000\n",
    "    sample_original = pkl_data_original.head(sample_size)\n",
    "    \n",
    "    print(f\"\\nüî¨ Running validation on sample data ({sample_size:,} rows)...\")\n",
    "    \n",
    "    # Apply just the Ethiopia fix (without full processing) for comparison\n",
    "    sample_with_fix = apply_ethiopia_fix(sample_original, verbose=True)\n",
    "    \n",
    "    # Create corrector for validation\n",
    "    corrector = SiteCorrections(site_code='ETAD', verbose=False)\n",
    "    \n",
    "    # Validate the fix\n",
    "    validation_results = corrector.validate_corrections(\n",
    "        sample_original,  # Original\n",
    "        sample_with_fix,  # With Ethiopia fix only\n",
    "        wavelength='IR'\n",
    "    )\n",
    "    \n",
    "    print(\"\\nüìà Validation Results:\")\n",
    "    for key, value in validation_results.items():\n",
    "        if isinstance(value, dict):\n",
    "            print(f\"  {key}:\")\n",
    "            for subkey, subval in value.items():\n",
    "                if isinstance(subval, float):\n",
    "                    print(f\"    {subkey}: {subval:.6f}\")\n",
    "                else:\n",
    "                    print(f\"    {subkey}: {subval}\")\n",
    "        else:\n",
    "            if isinstance(value, float):\n",
    "                print(f\"  {key}: {value:.6f}\")\n",
    "            else:\n",
    "                print(f\"  {key}: {value}\")\n",
    "    \n",
    "    # Check correlation improvements\n",
    "    if 'original_atn_correlation' in validation_results and 'corrected_atn_correlation' in validation_results:\n",
    "        orig_corr = validation_results['original_atn_correlation'] \n",
    "        corr_corr = validation_results['corrected_atn_correlation']\n",
    "        improvement = abs(orig_corr) - abs(corr_corr)\n",
    "        \n",
    "        print(f\"\\nüéØ Key Improvement Metric:\")\n",
    "        print(f\"  Original BCc-ATN1 correlation: {orig_corr:.6f}\")\n",
    "        print(f\"  Corrected BCc-ATN1 correlation: {corr_corr:.6f}\")\n",
    "        print(f\"  Improvement: {improvement:.6f} ({'‚úÖ Better!' if improvement > 0 else '‚ö†Ô∏è Check data'})\")\n",
    "        \n",
    "        if improvement > 0:\n",
    "            print(f\"  üéâ Ethiopia fix successfully reduced correlation by {improvement:.6f}\")\n",
    "else:\n",
    "    print(\"üìä Ethiopia Fix Validation: SKIPPED\")\n",
    "    print(\"=\" * 50)\n",
    "    print(\"Set APPLY_ETHIOPIA_FIX = True to run validation\")\n",
    "\n",
    "print(f\"\\nüìä Final Data Summary:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Shape: {pkl_data_cleaned.shape}\")\n",
    "print(f\"Date range: {pkl_data_cleaned['datetime_local'].min()} to {pkl_data_cleaned['datetime_local'].max()}\")\n",
    "\n",
    "# Check key columns\n",
    "key_cols = ['datetime_local', 'IR ATN1', 'IR BCc', 'Blue ATN1', 'Blue BCc', 'Flow total (mL/min)']\n",
    "for col in key_cols:\n",
    "    status = \"‚úÖ\" if col in pkl_data_cleaned.columns else \"‚ùå\"\n",
    "    print(f\"  {status} {col}\")\n",
    "\n",
    "# Show Ethiopia-specific columns if present\n",
    "ethiopia_specific = [col for col in pkl_data_cleaned.columns if any(x in col for x in ['corrected', 'manual', 'optimized', 'denominator'])]\n",
    "if ethiopia_specific:\n",
    "    print(f\"  üîß Ethiopia corrections: {len(ethiopia_specific)} columns\")\n",
    "\n",
    "memory_mb = pkl_data_cleaned.memory_usage(deep=True).sum() / 1024 / 1024\n",
    "print(f\"  üíæ Memory usage: {memory_mb:.1f} MB\")\n",
    "\n",
    "print(f\"\\n‚úÖ {'Ethiopia-enhanced' if APPLY_ETHIOPIA_FIX else 'Standard'} processing complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Ethiopia Fix Validation:\n",
      "==================================================\n",
      "üìÅ Loading datasets for comparison...\n",
      "  üîç Original uncleaned: df_uncleaned_Jacros_API_and_OG.pkl\n",
      "  üîß Ethiopia processed: pkl_data_cleaned_ethiopia.pkl\n",
      "  ‚úÖ Loaded Original: (1665156, 239)\n",
      "  ‚úÖ Loaded Ethiopia: (1477783, 318)\n",
      "\n",
      "üî¨ Comparing Original Uncleaned vs Ethiopia-Enhanced Processed:\n",
      "======================================================================\n",
      "üìä Dataset transformation:\n",
      "  Original uncleaned: (1665156, 239) (239 columns)\n",
      "  Ethiopia processed: (1477783, 318) (318 columns)\n",
      "  üîß Ethiopia corrections added: 27 columns\n",
      "  üìà Corrections by wavelength: IR(6), Blue(6), Red(5), Green(5), UV(5)\n",
      "\n",
      "üéØ Ethiopia Fix Impact Analysis:\n",
      "--------------------------------------------------\n",
      "\n",
      "IR Wavelength Analysis:\n",
      "  üìã Column availability:\n",
      "    Original IR BCc: ‚úÖ\n",
      "    Ethiopia IR BCc_corrected: ‚úÖ\n",
      "    IR ATN1: ‚úÖ\n",
      "  üìä Statistical comparison:\n",
      "    Original IR BCc mean: 2790.828\n",
      "    Corrected BCc mean: 6093.255\n",
      "    Mean difference: 3302.426\n",
      "  üéØ Correlation with IR ATN1:\n",
      "    Original correlation: 0.417843\n",
      "    Ethiopia corrected: -0.005505\n",
      "    Improvement: 0.412338 (‚úÖ Better!)\n",
      "    üéâ 98.7% correlation reduction!\n",
      "    ‚ú® Excellent: Near-zero correlation achieved!\n",
      "\n",
      "Blue Wavelength Analysis:\n",
      "  üìã Column availability:\n",
      "    Original Blue BCc: ‚úÖ\n",
      "    Ethiopia Blue BCc_corrected: ‚úÖ\n",
      "    Blue ATN1: ‚úÖ\n",
      "  üìä Statistical comparison:\n",
      "    Original Blue BCc mean: 3074.248\n",
      "    Corrected BCc mean: 6549.301\n",
      "    Mean difference: 3475.054\n",
      "  üéØ Correlation with Blue ATN1:\n",
      "    Original correlation: 0.424024\n",
      "    Ethiopia corrected: 0.008182\n",
      "    Improvement: 0.415842 (‚úÖ Better!)\n",
      "    üéâ 98.1% correlation reduction!\n",
      "    ‚ú® Excellent: Near-zero correlation achieved!\n",
      "\n",
      "‚úÖ Ethiopia Fix Summary:\n",
      "==============================\n",
      "üìà Data processing: 1,665,156 ‚Üí 1,477,783 rows\n",
      "üîß Columns added: 79 correction columns\n",
      "üéØ Primary benefit: Reduced BCc-ATN1 correlation (pneumatic pump fix)\n",
      "üìä Quality control: Applied comprehensive cleaning pipeline\n",
      "üßπ DEMA smoothing: Added for noise reduction\n",
      "\n",
      "üìä Current Session Data:\n",
      "==================================================\n",
      "No current data available - run the processing cell first\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# VALIDATION: Compare original uncleaned data vs Ethiopia-enhanced processed data\n",
    "\n",
    "print(\"üìä Ethiopia Fix Validation:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Load both original uncleaned data and Ethiopia-enhanced processed data for comparison\n",
    "try:\n",
    "    import pandas as pd\n",
    "    import os\n",
    "    \n",
    "    # Use the same paths as your config\n",
    "    base_data_path = \"/Users/ahzs645/Library/CloudStorage/GoogleDrive-ahzs645@gmail.com/My Drive/University/Research/Grad/UC Davis Ann/NASA MAIA/Data\"\n",
    "    \n",
    "    original_file = os.path.join(\n",
    "        base_data_path,\n",
    "        \"Aethelometry Data/Kyan Data/Mergedcleaned and uncleaned MA350 data20250707030704\",\n",
    "        \"df_uncleaned_Jacros_API_and_OG.pkl\"\n",
    "    )\n",
    "    ethiopia_file = 'pkl_data_cleaned_ethiopia.pkl'\n",
    "    \n",
    "    print(\"üìÅ Loading datasets for comparison...\")\n",
    "    print(f\"  üîç Original uncleaned: {os.path.basename(original_file)}\")\n",
    "    print(f\"  üîß Ethiopia processed: {os.path.basename(ethiopia_file)}\")\n",
    "    \n",
    "    # Load datasets\n",
    "    datasets = {}\n",
    "    \n",
    "    # Load original uncleaned data\n",
    "    try:\n",
    "        datasets[\"Original\"] = pd.read_pickle(original_file)\n",
    "        print(f\"  ‚úÖ Loaded Original: {datasets['Original'].shape}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"  ‚ùå Original file not found: {original_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ö†Ô∏è Error loading original: {e}\")\n",
    "    \n",
    "    # Load Ethiopia-enhanced processed data\n",
    "    try:\n",
    "        datasets[\"Ethiopia\"] = pd.read_pickle(ethiopia_file)\n",
    "        print(f\"  ‚úÖ Loaded Ethiopia: {datasets['Ethiopia'].shape}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"  ‚ùå Ethiopia file not found: {ethiopia_file}\")\n",
    "        print(\"  üí° Make sure you've run processing with APPLY_ETHIOPIA_FIX = True\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ö†Ô∏è Error loading Ethiopia data: {e}\")\n",
    "    \n",
    "    # Validation if we have both datasets\n",
    "    if \"Original\" in datasets and \"Ethiopia\" in datasets:\n",
    "        print(f\"\\nüî¨ Comparing Original Uncleaned vs Ethiopia-Enhanced Processed:\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        original_data = datasets[\"Original\"]\n",
    "        ethiopia_data = datasets[\"Ethiopia\"]\n",
    "        \n",
    "        # Basic comparison\n",
    "        print(f\"üìä Dataset transformation:\")\n",
    "        print(f\"  Original uncleaned: {original_data.shape} ({original_data.shape[1]} columns)\")\n",
    "        print(f\"  Ethiopia processed: {ethiopia_data.shape} ({ethiopia_data.shape[1]} columns)\")\n",
    "        \n",
    "        # Check Ethiopia-specific columns in processed data\n",
    "        ethiopia_cols = [col for col in ethiopia_data.columns if any(x in col for x in ['corrected', 'manual', 'optimized', 'denominator'])]\n",
    "        if ethiopia_cols:\n",
    "            print(f\"  üîß Ethiopia corrections added: {len(ethiopia_cols)} columns\")\n",
    "            \n",
    "            # Group by wavelength\n",
    "            wavelengths = ['IR', 'Blue', 'Red', 'Green', 'UV']\n",
    "            correction_summary = []\n",
    "            for wl in wavelengths:\n",
    "                wl_cols = [col for col in ethiopia_cols if wl in col]\n",
    "                if wl_cols:\n",
    "                    correction_summary.append(f\"{wl}({len(wl_cols)})\")\n",
    "            \n",
    "            print(f\"  üìà Corrections by wavelength: {', '.join(correction_summary)}\")\n",
    "        \n",
    "        # Detailed validation for key wavelengths\n",
    "        print(f\"\\nüéØ Ethiopia Fix Impact Analysis:\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        for wl in ['IR', 'Blue']:\n",
    "            print(f\"\\n{wl} Wavelength Analysis:\")\n",
    "            \n",
    "            # Column names\n",
    "            original_bcc = f'{wl} BCc'  # or might be BC1 in original\n",
    "            if original_bcc not in original_data.columns and f'{wl} BC1' in original_data.columns:\n",
    "                original_bcc = f'{wl} BC1'\n",
    "            \n",
    "            corrected_bcc = f'{wl} BCc_corrected'\n",
    "            atn_col = f'{wl} ATN1'\n",
    "            \n",
    "            # Check if required columns exist\n",
    "            has_original_bcc = original_bcc in original_data.columns\n",
    "            has_corrected_bcc = corrected_bcc in ethiopia_data.columns\n",
    "            has_atn_original = atn_col in original_data.columns\n",
    "            has_atn_ethiopia = atn_col in ethiopia_data.columns\n",
    "            \n",
    "            print(f\"  üìã Column availability:\")\n",
    "            print(f\"    Original {original_bcc}: {'‚úÖ' if has_original_bcc else '‚ùå'}\")\n",
    "            print(f\"    Ethiopia {corrected_bcc}: {'‚úÖ' if has_corrected_bcc else '‚ùå'}\")\n",
    "            print(f\"    {atn_col}: {'‚úÖ' if has_atn_original and has_atn_ethiopia else '‚ùå'}\")\n",
    "            \n",
    "            if has_original_bcc and has_corrected_bcc and has_atn_original and has_atn_ethiopia:\n",
    "                try:\n",
    "                    # Get sample of data for analysis (use smaller sample for speed)\n",
    "                    sample_size = min(50000, len(original_data), len(ethiopia_data))\n",
    "                    \n",
    "                    # Original data analysis\n",
    "                    orig_sample = original_data.head(sample_size)\n",
    "                    orig_bcc_data = orig_sample[original_bcc].dropna()\n",
    "                    orig_atn_data = orig_sample[atn_col].dropna()\n",
    "                    common_orig = orig_bcc_data.index.intersection(orig_atn_data.index)\n",
    "                    \n",
    "                    # Ethiopia data analysis\n",
    "                    eth_sample = ethiopia_data.head(sample_size)\n",
    "                    eth_bcc_data = eth_sample[corrected_bcc].dropna()\n",
    "                    eth_atn_data = eth_sample[atn_col].dropna()\n",
    "                    common_eth = eth_bcc_data.index.intersection(eth_atn_data.index)\n",
    "                    \n",
    "                    if len(common_orig) > 100 and len(common_eth) > 100:\n",
    "                        # Calculate correlations\n",
    "                        orig_corr = orig_sample.loc[common_orig, original_bcc].corr(orig_sample.loc[common_orig, atn_col])\n",
    "                        corr_corr = eth_sample.loc[common_eth, corrected_bcc].corr(eth_sample.loc[common_eth, atn_col])\n",
    "                        \n",
    "                        # Calculate basic statistics\n",
    "                        orig_mean = orig_sample[original_bcc].mean()\n",
    "                        corr_mean = eth_sample[corrected_bcc].mean()\n",
    "                        \n",
    "                        print(f\"  üìä Statistical comparison:\")\n",
    "                        print(f\"    Original {original_bcc} mean: {orig_mean:.3f}\")\n",
    "                        print(f\"    Corrected BCc mean: {corr_mean:.3f}\")\n",
    "                        print(f\"    Mean difference: {abs(corr_mean - orig_mean):.3f}\")\n",
    "                        \n",
    "                        print(f\"  üéØ Correlation with {atn_col}:\")\n",
    "                        print(f\"    Original correlation: {orig_corr:.6f}\")\n",
    "                        print(f\"    Ethiopia corrected: {corr_corr:.6f}\")\n",
    "                        \n",
    "                        if not (pd.isna(orig_corr) or pd.isna(corr_corr)):\n",
    "                            improvement = abs(orig_corr) - abs(corr_corr)\n",
    "                            print(f\"    Improvement: {improvement:.6f} ({'‚úÖ Better!' if improvement > 0 else '‚ö†Ô∏è Check data'})\")\n",
    "                            \n",
    "                            if improvement > 0:\n",
    "                                improvement_pct = (improvement / abs(orig_corr)) * 100\n",
    "                                print(f\"    üéâ {improvement_pct:.1f}% correlation reduction!\")\n",
    "                                \n",
    "                                # Ethiopia fix effectiveness\n",
    "                                if abs(corr_corr) < 0.1:  # Near zero correlation\n",
    "                                    print(f\"    ‚ú® Excellent: Near-zero correlation achieved!\")\n",
    "                                elif improvement > 0.1:  # Significant improvement\n",
    "                                    print(f\"    üëç Good: Significant correlation reduction\")\n",
    "                                else:\n",
    "                                    print(f\"    üìà Moderate improvement\")\n",
    "                        else:\n",
    "                            print(f\"    ‚ö†Ô∏è Could not calculate correlation improvement\")\n",
    "                    else:\n",
    "                        print(f\"    ‚ö†Ô∏è Insufficient data for analysis (orig: {len(common_orig)}, eth: {len(common_eth)})\")\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"    ‚ö†Ô∏è Error in analysis: {e}\")\n",
    "            else:\n",
    "                missing_cols = []\n",
    "                if not has_original_bcc:\n",
    "                    missing_cols.append(f\"Original: {original_bcc}\")\n",
    "                if not has_corrected_bcc:\n",
    "                    missing_cols.append(f\"Ethiopia: {corrected_bcc}\")\n",
    "                if not (has_atn_original and has_atn_ethiopia):\n",
    "                    missing_cols.append(f\"ATN: {atn_col}\")\n",
    "                print(f\"    ‚ùå Missing columns: {', '.join(missing_cols)}\")\n",
    "        \n",
    "        # Summary of Ethiopia fix benefits\n",
    "        print(f\"\\n‚úÖ Ethiopia Fix Summary:\")\n",
    "        print(\"=\" * 30)\n",
    "        print(f\"üìà Data processing: {original_data.shape[0]:,} ‚Üí {ethiopia_data.shape[0]:,} rows\")\n",
    "        print(f\"üîß Columns added: {ethiopia_data.shape[1] - original_data.shape[1]} correction columns\")\n",
    "        print(f\"üéØ Primary benefit: Reduced BCc-ATN1 correlation (pneumatic pump fix)\")\n",
    "        print(f\"üìä Quality control: Applied comprehensive cleaning pipeline\")\n",
    "        print(f\"üßπ DEMA smoothing: Added for noise reduction\")\n",
    "        \n",
    "    elif \"Ethiopia\" in datasets:\n",
    "        # Only Ethiopia data available - validate it has corrections\n",
    "        ethiopia_data = datasets[\"Ethiopia\"]\n",
    "        ethiopia_cols = [col for col in ethiopia_data.columns if any(x in col for x in ['corrected', 'manual', 'optimized', 'denominator'])]\n",
    "        \n",
    "        print(f\"\\n‚úÖ Ethiopia-enhanced data loaded successfully!\")\n",
    "        print(f\"üîß Found {len(ethiopia_cols)} correction columns\")\n",
    "        print(\"‚ö†Ô∏è Original uncleaned data not available for comparison\")\n",
    "        \n",
    "        if ethiopia_cols:\n",
    "            wavelengths = ['IR', 'Blue', 'Red', 'Green', 'UV']\n",
    "            for wl in wavelengths:\n",
    "                wl_cols = [col for col in ethiopia_cols if wl in col]\n",
    "                if wl_cols:\n",
    "                    print(f\"  {wl}: {len(wl_cols)} corrections\")\n",
    "    \n",
    "    else:\n",
    "        print(f\"\\n‚ö†Ô∏è Could not load required files for comparison\")\n",
    "        if \"Original\" not in datasets:\n",
    "            print(\"‚ùå Original uncleaned data not found\")\n",
    "        if \"Ethiopia\" not in datasets:\n",
    "            print(\"‚ùå Ethiopia processed data not found - run processing with APPLY_ETHIOPIA_FIX = True\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error during validation: {e}\")\n",
    "\n",
    "print(f\"\\nüìä Current Session Data:\")\n",
    "print(\"=\" * 50)\n",
    "if 'pkl_data_cleaned' in locals():\n",
    "    print(f\"Shape: {pkl_data_cleaned.shape}\")\n",
    "    print(f\"Date range: {pkl_data_cleaned['datetime_local'].min()} to {pkl_data_cleaned['datetime_local'].max()}\")\n",
    "    \n",
    "    # Show Ethiopia-specific columns if present\n",
    "    ethiopia_specific = [col for col in pkl_data_cleaned.columns if any(x in col for x in ['corrected', 'manual', 'optimized', 'denominator'])]\n",
    "    if ethiopia_specific:\n",
    "        print(f\"üîß Ethiopia corrections in current data: {len(ethiopia_specific)} columns\")\n",
    "        has_ethiopia_fix = True\n",
    "    else:\n",
    "        has_ethiopia_fix = False\n",
    "    \n",
    "    print(f\"‚úÖ Current data: {'Ethiopia-enhanced' if has_ethiopia_fix else 'Standard'} processing\")\n",
    "else:\n",
    "    print(\"No current data available - run the processing cell first\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
