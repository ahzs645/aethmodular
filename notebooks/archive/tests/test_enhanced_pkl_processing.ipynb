{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enhanced PKL Processing Test Notebook\n",
    "\n",
    "This notebook tests the integrated enhanced PKL processing functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Advanced plotting style configured\n",
      "🚀 Aethalometer-FTIR/HIPS Pipeline with Simplified Setup\n",
      "============================================================\n",
      "📊 Configuration Summary:\n",
      "   Site: ETAD\n",
      "   Wavelength: Red\n",
      "   Output format: jpl\n",
      "   Quality threshold: 10 minutes\n",
      "   Output directory: outputs\n",
      "\n",
      "📁 File paths:\n",
      "   pkl_data: ✅ df_uncleaned_Jacros_API_and_OG.pkl\n",
      "   csv_data: ✅ Jacros_MA350_1-min_2022-2024_Cleaned.csv\n",
      "   FTIR DB: ✅ spartan_ftir_hips.db\n",
      "🧹 Enhanced setup with PKL cleaning capabilities loaded\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "import sys\n",
    "import os\n",
    "sys.path.insert(0, os.path.join(os.path.dirname(os.getcwd()), 'src'))\n",
    "\n",
    "# Import the new enhanced PKL processing module\n",
    "from data.qc.enhanced_pkl_processing import process_pkl_data_enhanced, EnhancedPKLProcessor\n",
    "from config.notebook_config import NotebookConfig\n",
    "from notebook_utils.pkl_cleaning_integration import create_enhanced_setup\n",
    "\n",
    "# Your existing configuration\n",
    "config = NotebookConfig(\n",
    "    site_code='ETAD',\n",
    "    wavelength='Red',\n",
    "    quality_threshold=10,\n",
    "    output_format='jpl',\n",
    "    min_samples_for_analysis=30,\n",
    "    confidence_level=0.95,\n",
    "    outlier_threshold=3.0,\n",
    "    figure_size=(12, 8),\n",
    "    font_size=10,\n",
    "    dpi=300\n",
    ")\n",
    "\n",
    "# Set your data paths (same as before)\n",
    "base_data_path = \"/Users/ahzs645/Library/CloudStorage/GoogleDrive-ahzs645@gmail.com/My Drive/University/Research/Grad/UC Davis Ann/NASA MAIA/Data\"\n",
    "\n",
    "config.aethalometer_files = {\n",
    "    'pkl_data': os.path.join(\n",
    "        base_data_path,\n",
    "        \"Aethelometry Data/Kyan Data/Mergedcleaned and uncleaned MA350 data20250707030704\",\n",
    "        \"df_uncleaned_Jacros_API_and_OG.pkl\"\n",
    "    ),\n",
    "    'csv_data': os.path.join(\n",
    "        base_data_path,\n",
    "        \"Aethelometry Data/Raw\",\n",
    "        \"Jacros_MA350_1-min_2022-2024_Cleaned.csv\"\n",
    "    )\n",
    "}\n",
    "\n",
    "config.ftir_db_path = os.path.join(\n",
    "    base_data_path,\n",
    "    \"EC-HIPS-Aeth Comparison/Data/Original Data/Combined Database\",\n",
    "    \"spartan_ftir_hips.db\"\n",
    ")\n",
    "\n",
    "# Create enhanced setup\n",
    "setup = create_enhanced_setup(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📁 Loading datasets...\n",
      "📦 Setting up modular system...\n",
      "✅ Aethalometer loaders imported\n",
      "✅ Database loader imported\n",
      "✅ Plotting utilities imported\n",
      "✅ Plotting style configured\n",
      "✅ Successfully imported 5 modular components\n",
      "\n",
      "============================================================\n",
      "📁 LOADING DATASETS\n",
      "============================================================\n",
      "📁 Loading all datasets...\n",
      "\n",
      "==================================================\n",
      "📊 Loading pkl_data\n",
      "==================================================\n",
      "📁 Loading pkl_data: df_uncleaned_Jacros_API_and_OG.pkl\n",
      "Detected format: standard\n",
      "Set 'datetime_local' as DatetimeIndex for time series operations\n",
      "Converted 17 columns to JPL format\n",
      "Warning: Missing recommended columns: ['datetime_local', 'Biomass.BCc', 'Fossil.fuel.BCc']\n",
      "✅ Modular load: 1,665,156 rows × 238 columns\n",
      "📊 Method: modular\n",
      "📊 Format: jpl\n",
      "📊 Memory: 7443.05 MB\n",
      "🧮 BC columns: 30\n",
      "📈 ATN columns: 25\n",
      "📅 Time range: 2021-01-09 16:38:00 to 2025-06-26 23:18:00\n",
      "✅ pkl_data loaded successfully\n",
      "\n",
      "==================================================\n",
      "📊 Loading csv_data\n",
      "==================================================\n",
      "📁 Loading csv_data: Jacros_MA350_1-min_2022-2024_Cleaned.csv\n",
      "Set 'Time (Local)' as DatetimeIndex for time series operations\n",
      "Converted 5 columns to JPL format\n",
      "✅ Modular load: 1,095,086 rows × 77 columns\n",
      "📊 Method: modular\n",
      "📊 Format: jpl\n",
      "📊 Memory: 884.83 MB\n",
      "🧮 BC columns: 15\n",
      "📈 ATN columns: 10\n",
      "📅 Time range: 2022-04-12 12:46:01+03:00 to 2024-08-20 12:01:00+03:00\n",
      "✅ csv_data loaded successfully\n",
      "\n",
      "==================================================\n",
      "🗃️ Loading FTIR/HIPS data\n",
      "==================================================\n",
      "🗃️ Loading FTIR/HIPS data for site ETAD...\n",
      "📊 Available sites: ['ILNZ', 'ILHA', 'ZAJB', 'CAHA', 'CASH', 'AEAZ', 'AUMN', 'KRUL', 'MXMC', 'ZAPR', 'CHTS', 'ETAD', 'INDH', 'TWTA', 'USPA', 'TWKA', 'KRSE', 'PRFJ', 'BDDU', 'BIBU', 'USNO', 'IDBD', None]\n",
      "✅ Modular FTIR load: 168 samples\n",
      "📅 Date range: 2022-12-07 00:00:00 to 2024-05-12 00:00:00\n",
      "✅ FTIR/HIPS data loaded successfully\n",
      "\n",
      "📊 Loading summary: 3 datasets loaded\n",
      "\n",
      "📊 LOADING SUMMARY\n",
      "============================================================\n",
      "✅ Successfully loaded 3 datasets\n",
      "   - pkl_data: 1,665,156 rows × 238 columns\n",
      "   - csv_data: 1,095,086 rows × 77 columns\n",
      "   - ftir_hips: 168 rows × 12 columns\n",
      "============================================================\n",
      "✅ Converting datetime_local from index to column...\n",
      "📊 PKL data ready: (1665156, 239)\n",
      "📅 Date range: 2021-01-09 16:38:00 to 2025-06-26 23:18:00\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "print(\"📁 Loading datasets...\")\n",
    "datasets = setup.load_all_data()\n",
    "\n",
    "# Get PKL data\n",
    "pkl_data_original = setup.get_dataset('pkl_data')\n",
    "\n",
    "# Quick fix for datetime_local issue (same as before)\n",
    "if 'datetime_local' not in pkl_data_original.columns:\n",
    "    if pkl_data_original.index.name == 'datetime_local':\n",
    "        print(\"✅ Converting datetime_local from index to column...\")\n",
    "        pkl_data_original = pkl_data_original.reset_index()\n",
    "    elif hasattr(pkl_data_original.index, 'tz'):\n",
    "        print(\"✅ Creating datetime_local column from datetime index...\")\n",
    "        pkl_data_original['datetime_local'] = pkl_data_original.index\n",
    "        pkl_data_original = pkl_data_original.reset_index(drop=True)\n",
    "\n",
    "print(f\"📊 PKL data ready: {pkl_data_original.shape}\")\n",
    "print(f\"📅 Date range: {pkl_data_original['datetime_local'].min()} to {pkl_data_original['datetime_local'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Enhanced PKL Processing WITH Ethiopia Fix\n",
      "============================================================\n",
      "🚀 Enhanced PKL Data Processing Pipeline\n",
      "============================================================\n",
      "🔧 Comprehensive Preprocessing Pipeline\n",
      "============================================================\n",
      "Step 1: Processing datetime...\n",
      "\n",
      "Step 2: Fixing column names...\n",
      "✅ Renamed 16 columns\n",
      "\n",
      "Step 3: Converting data types...\n",
      "Converted IR ATN1 to float.\n",
      "Converted UV ATN1 to float.\n",
      "Converted Blue ATN1 to float.\n",
      "Converted Green ATN1 to float.\n",
      "Converted Red ATN1 to float.\n",
      "✅ Applied calibration.convert_to_float()\n",
      "\n",
      "Step 4: Adding Session ID...\n",
      "\n",
      "Step 5: Adding delta calculations...\n",
      "✅ Applied calibration.add_deltas()\n",
      "\n",
      "Step 6: Final adjustments...\n",
      "✅ Filtered to 2022+: 1,665,156 -> 1,627,058 rows\n",
      "🔄 Applying DEMA Smoothing...\n",
      "========================================\n",
      "\n",
      "Processing IR wavelength...\n",
      "  Available BC columns: ['IR BC1', 'IR BC2', 'IR BCc']\n",
      "  ✅ Created IR BC1 smoothed\n",
      "  ✅ Created IR BC2 smoothed\n",
      "  ✅ Created IR BCc smoothed\n",
      "\n",
      "Processing Blue wavelength...\n",
      "  Available BC columns: ['Blue BC1', 'Blue BC2', 'Blue BCc']\n",
      "  ✅ Created Blue BC1 smoothed\n",
      "  ✅ Created Blue BC2 smoothed\n",
      "  ✅ Created Blue BCc smoothed\n",
      "\n",
      "🔧 Applying Site-Specific Corrections...\n",
      "==================================================\n",
      "🔧 Applying Ethiopia (ETAD) site corrections...\n",
      "  📊 Applying IR loading compensation fix...\n",
      "    📈 IR median K: 0.012900\n",
      "    🎯 IR optimal K: 0.011910\n",
      "  📊 Applying Blue loading compensation fix...\n",
      "    📈 Blue median K: 0.009536\n",
      "    🎯 Blue optimal K: 0.009205\n",
      "  📊 Applying Red loading compensation fix...\n",
      "    📈 Red median K: 0.010900\n",
      "    🎯 Red optimal K: 0.010451\n",
      "  📊 Applying Green loading compensation fix...\n",
      "    📈 Green median K: 0.009947\n",
      "    🎯 Green optimal K: 0.005172\n",
      "  📊 Applying UV loading compensation fix...\n",
      "    📈 UV median K: 0.008200\n",
      "    🎯 UV optimal K: 0.013024\n",
      "✅ Site corrections completed successfully!\n",
      "\n",
      "🧹 Final Cleaning Pipeline\n",
      "============================================================\n",
      "Starting PKL data cleaning pipeline...\n",
      "==================================================\n",
      "🔍 Data Structure Diagnosis:\n",
      "------------------------------\n",
      "DataFrame shape: (1627058, 309)\n",
      "Date range: 2022-04-12 09:12:00 to 2025-06-26 23:18:00\n",
      "BC columns: 30 (e.g., ['Blue BC1', 'Blue BC2', 'Blue BCc'])\n",
      "BC smoothed columns: 6 (e.g., ['IR BC1 smoothed', 'IR BC2 smoothed', 'IR BCc smoothed'])\n",
      "ATN columns: 40 (e.g., ['Blue ATN1', 'Blue ATN2', 'Green ATN1'])\n",
      "Flow columns: 4 (e.g., ['Flow setpoint (mL/min)', 'Flow total (mL/min)', 'Flow1 (mL/min)'])\n",
      "\n",
      "Targeted wavelengths: ['IR', 'Blue']\n",
      "  IR: ✅ BC | ✅ BC smoothed | ✅ ATN\n",
      "  Blue: ✅ BC | ✅ BC smoothed | ✅ ATN\n",
      "------------------------------\n",
      "\n",
      "🧹 Starting cleaning steps...\n",
      "1919 datapoints removed due to Start up or Tape advance status\n",
      "Statuses of concern, count by device and status:\n",
      "\n",
      "MA350-0238 Flow unstable 750\n",
      "MA350-0238 Optical saturation 0\n",
      "MA350-0238 Sample timing error 0\n",
      "Number of datapoints with invalid optics values\n",
      "AFTER dropping data with 'Optical saturation' status values: 802\n",
      "Removed 56128 datapoints for optics\n",
      "Status cleaning: Removed 58797 rows (3.61%)\n",
      "Extreme BCc cleaning: Removed 13900 rows (0.89%)\n",
      "Flow range cleaning: Removed 0 rows (0.00%)\n",
      "Abnormal flow ratio: Removed 29362 rows (1.89%)\n",
      "Leak ratio cleaning: Removed 507 rows (0.03%)\n",
      "BCc denominator cleaning: Removed 25537 rows (1.68%)\n",
      "Sharp change 605\n",
      "noise 1425\n",
      "Temperature change cleaning: Removed 2030 rows (0.14%)\n",
      "IR ATN1_roughness: threshold=0.1147, high periods flagged: 17945 rows so far\n",
      "IR ATN2_roughness: threshold=0.1008, high periods flagged: 18439 rows so far\n",
      "Blue ATN1_roughness: threshold=0.1859, high periods flagged: 19114 rows so far\n",
      "Blue ATN2_roughness: threshold=0.1664, high periods flagged: 19142 rows so far\n",
      "==================================================\n",
      "Cleaning complete! Final data shape: (1477783, 318)\n",
      "\n",
      "📊 Processing Results Summary:\n",
      "============================================================\n",
      "Original data points: 1,665,156\n",
      "After preprocessing: 1,627,058\n",
      "After smoothing: 1,627,058\n",
      "After site corrections: 1,627,058\n",
      "Final cleaned: 1,477,783\n",
      "Total removed: 187,373 (11.25%)\n",
      "\n",
      "✅ PKL data processing completed successfully!\n",
      "\n",
      "📊 Final data verification:\n",
      "Shape: (1477783, 318)\n",
      "Date range: 2022-04-12 09:54:00 to 2025-06-26 23:18:00\n",
      "  ✅ IR ATN1\n",
      "  ✅ IR BCc\n",
      "  ✅ Blue ATN1\n",
      "  ✅ Blue BCc\n",
      "  ✅ Flow total (mL/min)\n",
      "  ✅ Smoothed columns: 6\n",
      "\n",
      "💾 Cleaned data exported:\n",
      "  📄 CSV: pkl_data_cleaned_ethiopia.csv\n",
      "  📦 Pickle: pkl_data_cleaned_ethiopia.pkl\n",
      "\n",
      "✅ Processing complete: (1477783, 318)\n",
      "\n",
      "🔧 Ethiopia correction columns added (27):\n",
      "  • Blue BCc denominator\n",
      "  • Blue BCc_corrected\n",
      "  • Blue BCc_manual\n",
      "  • Blue BCc_optimized\n",
      "  • Blue denominator_manual\n",
      "  • Blue denominator_optimized\n",
      "  • Green BCc_corrected\n",
      "  • Green BCc_manual\n",
      "  • Green BCc_optimized\n",
      "  • Green denominator_manual\n",
      "  ... and 17 more\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# ENHANCED PKL PROCESSING with optional Ethiopia fix\n",
    "\n",
    "# 🎛️ Configuration: Toggle Ethiopia fix here\n",
    "APPLY_ETHIOPIA_FIX = True  # Set to True to enable Ethiopia pneumatic pump fix\n",
    "\n",
    "print(f\"🚀 Enhanced PKL Processing {'WITH' if APPLY_ETHIOPIA_FIX else 'WITHOUT'} Ethiopia Fix\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "pkl_data_cleaned = process_pkl_data_enhanced(\n",
    "    pkl_data_original,\n",
    "    wavelengths_to_filter=['IR', 'Blue'],\n",
    "    export_path=f'pkl_data_cleaned_{\"ethiopia\" if APPLY_ETHIOPIA_FIX else \"standard\"}',\n",
    "    apply_ethiopia_fix=APPLY_ETHIOPIA_FIX,  # 🔧 Ethiopia fix toggle\n",
    "    site_code='ETAD' if APPLY_ETHIOPIA_FIX else None,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(f\"\\n✅ Processing complete: {pkl_data_cleaned.shape}\")\n",
    "\n",
    "# Show what Ethiopia corrections were added (if any)\n",
    "if APPLY_ETHIOPIA_FIX:\n",
    "    ethiopia_cols = [col for col in pkl_data_cleaned.columns if any(x in col for x in ['corrected', 'manual', 'optimized', 'denominator'])]\n",
    "    if ethiopia_cols:\n",
    "        print(f\"\\n🔧 Ethiopia correction columns added ({len(ethiopia_cols)}):\")\n",
    "        for col in sorted(ethiopia_cols)[:10]:  # Show first 10\n",
    "            print(f\"  • {col}\")\n",
    "        if len(ethiopia_cols) > 10:\n",
    "            print(f\"  ... and {len(ethiopia_cols)-10} more\")\n",
    "    else:\n",
    "        print(\"\\n⚠️ No Ethiopia correction columns found\")\n",
    "else:\n",
    "    print(\"\\n📊 Standard processing - no Ethiopia corrections applied\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Ethiopia Fix Validation:\n",
      "==================================================\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'src'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m50\u001b[39m)\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Use the validation functions from the site corrections module\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mprocessors\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msite_corrections\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SiteCorrections, apply_ethiopia_fix\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Create a small sample for comparison (to demonstrate the fix)\u001b[39;00m\n\u001b[32m     12\u001b[39m sample_size = \u001b[32m10000\u001b[39m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'src'"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# VALIDATION: Ethiopia fix validation (only runs if fix was applied)\n",
    "\n",
    "if APPLY_ETHIOPIA_FIX:\n",
    "    print(\"📊 Ethiopia Fix Validation:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Use the validation functions from the site corrections module\n",
    "    from src.data.processors.site_corrections import SiteCorrections, apply_ethiopia_fix\n",
    "    \n",
    "    # Create a small sample for comparison (to demonstrate the fix)\n",
    "    sample_size = 10000\n",
    "    sample_original = pkl_data_original.head(sample_size)\n",
    "    \n",
    "    print(f\"\\n🔬 Running validation on sample data ({sample_size:,} rows)...\")\n",
    "    \n",
    "    # Apply just the Ethiopia fix (without full processing) for comparison\n",
    "    sample_with_fix = apply_ethiopia_fix(sample_original, verbose=True)\n",
    "    \n",
    "    # Create corrector for validation\n",
    "    corrector = SiteCorrections(site_code='ETAD', verbose=False)\n",
    "    \n",
    "    # Validate the fix\n",
    "    validation_results = corrector.validate_corrections(\n",
    "        sample_original,  # Original\n",
    "        sample_with_fix,  # With Ethiopia fix only\n",
    "        wavelength='IR'\n",
    "    )\n",
    "    \n",
    "    print(\"\\n📈 Validation Results:\")\n",
    "    for key, value in validation_results.items():\n",
    "        if isinstance(value, dict):\n",
    "            print(f\"  {key}:\")\n",
    "            for subkey, subval in value.items():\n",
    "                if isinstance(subval, float):\n",
    "                    print(f\"    {subkey}: {subval:.6f}\")\n",
    "                else:\n",
    "                    print(f\"    {subkey}: {subval}\")\n",
    "        else:\n",
    "            if isinstance(value, float):\n",
    "                print(f\"  {key}: {value:.6f}\")\n",
    "            else:\n",
    "                print(f\"  {key}: {value}\")\n",
    "    \n",
    "    # Check correlation improvements\n",
    "    if 'original_atn_correlation' in validation_results and 'corrected_atn_correlation' in validation_results:\n",
    "        orig_corr = validation_results['original_atn_correlation'] \n",
    "        corr_corr = validation_results['corrected_atn_correlation']\n",
    "        improvement = abs(orig_corr) - abs(corr_corr)\n",
    "        \n",
    "        print(f\"\\n🎯 Key Improvement Metric:\")\n",
    "        print(f\"  Original BCc-ATN1 correlation: {orig_corr:.6f}\")\n",
    "        print(f\"  Corrected BCc-ATN1 correlation: {corr_corr:.6f}\")\n",
    "        print(f\"  Improvement: {improvement:.6f} ({'✅ Better!' if improvement > 0 else '⚠️ Check data'})\")\n",
    "        \n",
    "        if improvement > 0:\n",
    "            print(f\"  🎉 Ethiopia fix successfully reduced correlation by {improvement:.6f}\")\n",
    "else:\n",
    "    print(\"📊 Ethiopia Fix Validation: SKIPPED\")\n",
    "    print(\"=\" * 50)\n",
    "    print(\"Set APPLY_ETHIOPIA_FIX = True to run validation\")\n",
    "\n",
    "print(f\"\\n📊 Final Data Summary:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Shape: {pkl_data_cleaned.shape}\")\n",
    "print(f\"Date range: {pkl_data_cleaned['datetime_local'].min()} to {pkl_data_cleaned['datetime_local'].max()}\")\n",
    "\n",
    "# Check key columns\n",
    "key_cols = ['datetime_local', 'IR ATN1', 'IR BCc', 'Blue ATN1', 'Blue BCc', 'Flow total (mL/min)']\n",
    "for col in key_cols:\n",
    "    status = \"✅\" if col in pkl_data_cleaned.columns else \"❌\"\n",
    "    print(f\"  {status} {col}\")\n",
    "\n",
    "# Show Ethiopia-specific columns if present\n",
    "ethiopia_specific = [col for col in pkl_data_cleaned.columns if any(x in col for x in ['corrected', 'manual', 'optimized', 'denominator'])]\n",
    "if ethiopia_specific:\n",
    "    print(f\"  🔧 Ethiopia corrections: {len(ethiopia_specific)} columns\")\n",
    "\n",
    "memory_mb = pkl_data_cleaned.memory_usage(deep=True).sum() / 1024 / 1024\n",
    "print(f\"  💾 Memory usage: {memory_mb:.1f} MB\")\n",
    "\n",
    "print(f\"\\n✅ {'Ethiopia-enhanced' if APPLY_ETHIOPIA_FIX else 'Standard'} processing complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Ethiopia Fix Validation:\n",
      "==================================================\n",
      "📁 Loading datasets for comparison...\n",
      "  🔍 Original uncleaned: df_uncleaned_Jacros_API_and_OG.pkl\n",
      "  🔧 Ethiopia processed: pkl_data_cleaned_ethiopia.pkl\n",
      "  ✅ Loaded Original: (1665156, 239)\n",
      "  ✅ Loaded Ethiopia: (1477783, 318)\n",
      "\n",
      "🔬 Comparing Original Uncleaned vs Ethiopia-Enhanced Processed:\n",
      "======================================================================\n",
      "📊 Dataset transformation:\n",
      "  Original uncleaned: (1665156, 239) (239 columns)\n",
      "  Ethiopia processed: (1477783, 318) (318 columns)\n",
      "  🔧 Ethiopia corrections added: 27 columns\n",
      "  📈 Corrections by wavelength: IR(6), Blue(6), Red(5), Green(5), UV(5)\n",
      "\n",
      "🎯 Ethiopia Fix Impact Analysis:\n",
      "--------------------------------------------------\n",
      "\n",
      "IR Wavelength Analysis:\n",
      "  📋 Column availability:\n",
      "    Original IR BCc: ✅\n",
      "    Ethiopia IR BCc_corrected: ✅\n",
      "    IR ATN1: ✅\n",
      "  📊 Statistical comparison:\n",
      "    Original IR BCc mean: 2790.828\n",
      "    Corrected BCc mean: 6093.255\n",
      "    Mean difference: 3302.426\n",
      "  🎯 Correlation with IR ATN1:\n",
      "    Original correlation: 0.417843\n",
      "    Ethiopia corrected: -0.005505\n",
      "    Improvement: 0.412338 (✅ Better!)\n",
      "    🎉 98.7% correlation reduction!\n",
      "    ✨ Excellent: Near-zero correlation achieved!\n",
      "\n",
      "Blue Wavelength Analysis:\n",
      "  📋 Column availability:\n",
      "    Original Blue BCc: ✅\n",
      "    Ethiopia Blue BCc_corrected: ✅\n",
      "    Blue ATN1: ✅\n",
      "  📊 Statistical comparison:\n",
      "    Original Blue BCc mean: 3074.248\n",
      "    Corrected BCc mean: 6549.301\n",
      "    Mean difference: 3475.054\n",
      "  🎯 Correlation with Blue ATN1:\n",
      "    Original correlation: 0.424024\n",
      "    Ethiopia corrected: 0.008182\n",
      "    Improvement: 0.415842 (✅ Better!)\n",
      "    🎉 98.1% correlation reduction!\n",
      "    ✨ Excellent: Near-zero correlation achieved!\n",
      "\n",
      "✅ Ethiopia Fix Summary:\n",
      "==============================\n",
      "📈 Data processing: 1,665,156 → 1,477,783 rows\n",
      "🔧 Columns added: 79 correction columns\n",
      "🎯 Primary benefit: Reduced BCc-ATN1 correlation (pneumatic pump fix)\n",
      "📊 Quality control: Applied comprehensive cleaning pipeline\n",
      "🧹 DEMA smoothing: Added for noise reduction\n",
      "\n",
      "📊 Current Session Data:\n",
      "==================================================\n",
      "No current data available - run the processing cell first\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# VALIDATION: Compare original uncleaned data vs Ethiopia-enhanced processed data\n",
    "\n",
    "print(\"📊 Ethiopia Fix Validation:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Load both original uncleaned data and Ethiopia-enhanced processed data for comparison\n",
    "try:\n",
    "    import pandas as pd\n",
    "    import os\n",
    "    \n",
    "    # Use the same paths as your config\n",
    "    base_data_path = \"/Users/ahzs645/Library/CloudStorage/GoogleDrive-ahzs645@gmail.com/My Drive/University/Research/Grad/UC Davis Ann/NASA MAIA/Data\"\n",
    "    \n",
    "    original_file = os.path.join(\n",
    "        base_data_path,\n",
    "        \"Aethelometry Data/Kyan Data/Mergedcleaned and uncleaned MA350 data20250707030704\",\n",
    "        \"df_uncleaned_Jacros_API_and_OG.pkl\"\n",
    "    )\n",
    "    ethiopia_file = 'pkl_data_cleaned_ethiopia.pkl'\n",
    "    \n",
    "    print(\"📁 Loading datasets for comparison...\")\n",
    "    print(f\"  🔍 Original uncleaned: {os.path.basename(original_file)}\")\n",
    "    print(f\"  🔧 Ethiopia processed: {os.path.basename(ethiopia_file)}\")\n",
    "    \n",
    "    # Load datasets\n",
    "    datasets = {}\n",
    "    \n",
    "    # Load original uncleaned data\n",
    "    try:\n",
    "        datasets[\"Original\"] = pd.read_pickle(original_file)\n",
    "        print(f\"  ✅ Loaded Original: {datasets['Original'].shape}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"  ❌ Original file not found: {original_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ⚠️ Error loading original: {e}\")\n",
    "    \n",
    "    # Load Ethiopia-enhanced processed data\n",
    "    try:\n",
    "        datasets[\"Ethiopia\"] = pd.read_pickle(ethiopia_file)\n",
    "        print(f\"  ✅ Loaded Ethiopia: {datasets['Ethiopia'].shape}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"  ❌ Ethiopia file not found: {ethiopia_file}\")\n",
    "        print(\"  💡 Make sure you've run processing with APPLY_ETHIOPIA_FIX = True\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ⚠️ Error loading Ethiopia data: {e}\")\n",
    "    \n",
    "    # Validation if we have both datasets\n",
    "    if \"Original\" in datasets and \"Ethiopia\" in datasets:\n",
    "        print(f\"\\n🔬 Comparing Original Uncleaned vs Ethiopia-Enhanced Processed:\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        original_data = datasets[\"Original\"]\n",
    "        ethiopia_data = datasets[\"Ethiopia\"]\n",
    "        \n",
    "        # Basic comparison\n",
    "        print(f\"📊 Dataset transformation:\")\n",
    "        print(f\"  Original uncleaned: {original_data.shape} ({original_data.shape[1]} columns)\")\n",
    "        print(f\"  Ethiopia processed: {ethiopia_data.shape} ({ethiopia_data.shape[1]} columns)\")\n",
    "        \n",
    "        # Check Ethiopia-specific columns in processed data\n",
    "        ethiopia_cols = [col for col in ethiopia_data.columns if any(x in col for x in ['corrected', 'manual', 'optimized', 'denominator'])]\n",
    "        if ethiopia_cols:\n",
    "            print(f\"  🔧 Ethiopia corrections added: {len(ethiopia_cols)} columns\")\n",
    "            \n",
    "            # Group by wavelength\n",
    "            wavelengths = ['IR', 'Blue', 'Red', 'Green', 'UV']\n",
    "            correction_summary = []\n",
    "            for wl in wavelengths:\n",
    "                wl_cols = [col for col in ethiopia_cols if wl in col]\n",
    "                if wl_cols:\n",
    "                    correction_summary.append(f\"{wl}({len(wl_cols)})\")\n",
    "            \n",
    "            print(f\"  📈 Corrections by wavelength: {', '.join(correction_summary)}\")\n",
    "        \n",
    "        # Detailed validation for key wavelengths\n",
    "        print(f\"\\n🎯 Ethiopia Fix Impact Analysis:\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        for wl in ['IR', 'Blue']:\n",
    "            print(f\"\\n{wl} Wavelength Analysis:\")\n",
    "            \n",
    "            # Column names\n",
    "            original_bcc = f'{wl} BCc'  # or might be BC1 in original\n",
    "            if original_bcc not in original_data.columns and f'{wl} BC1' in original_data.columns:\n",
    "                original_bcc = f'{wl} BC1'\n",
    "            \n",
    "            corrected_bcc = f'{wl} BCc_corrected'\n",
    "            atn_col = f'{wl} ATN1'\n",
    "            \n",
    "            # Check if required columns exist\n",
    "            has_original_bcc = original_bcc in original_data.columns\n",
    "            has_corrected_bcc = corrected_bcc in ethiopia_data.columns\n",
    "            has_atn_original = atn_col in original_data.columns\n",
    "            has_atn_ethiopia = atn_col in ethiopia_data.columns\n",
    "            \n",
    "            print(f\"  📋 Column availability:\")\n",
    "            print(f\"    Original {original_bcc}: {'✅' if has_original_bcc else '❌'}\")\n",
    "            print(f\"    Ethiopia {corrected_bcc}: {'✅' if has_corrected_bcc else '❌'}\")\n",
    "            print(f\"    {atn_col}: {'✅' if has_atn_original and has_atn_ethiopia else '❌'}\")\n",
    "            \n",
    "            if has_original_bcc and has_corrected_bcc and has_atn_original and has_atn_ethiopia:\n",
    "                try:\n",
    "                    # Get sample of data for analysis (use smaller sample for speed)\n",
    "                    sample_size = min(50000, len(original_data), len(ethiopia_data))\n",
    "                    \n",
    "                    # Original data analysis\n",
    "                    orig_sample = original_data.head(sample_size)\n",
    "                    orig_bcc_data = orig_sample[original_bcc].dropna()\n",
    "                    orig_atn_data = orig_sample[atn_col].dropna()\n",
    "                    common_orig = orig_bcc_data.index.intersection(orig_atn_data.index)\n",
    "                    \n",
    "                    # Ethiopia data analysis\n",
    "                    eth_sample = ethiopia_data.head(sample_size)\n",
    "                    eth_bcc_data = eth_sample[corrected_bcc].dropna()\n",
    "                    eth_atn_data = eth_sample[atn_col].dropna()\n",
    "                    common_eth = eth_bcc_data.index.intersection(eth_atn_data.index)\n",
    "                    \n",
    "                    if len(common_orig) > 100 and len(common_eth) > 100:\n",
    "                        # Calculate correlations\n",
    "                        orig_corr = orig_sample.loc[common_orig, original_bcc].corr(orig_sample.loc[common_orig, atn_col])\n",
    "                        corr_corr = eth_sample.loc[common_eth, corrected_bcc].corr(eth_sample.loc[common_eth, atn_col])\n",
    "                        \n",
    "                        # Calculate basic statistics\n",
    "                        orig_mean = orig_sample[original_bcc].mean()\n",
    "                        corr_mean = eth_sample[corrected_bcc].mean()\n",
    "                        \n",
    "                        print(f\"  📊 Statistical comparison:\")\n",
    "                        print(f\"    Original {original_bcc} mean: {orig_mean:.3f}\")\n",
    "                        print(f\"    Corrected BCc mean: {corr_mean:.3f}\")\n",
    "                        print(f\"    Mean difference: {abs(corr_mean - orig_mean):.3f}\")\n",
    "                        \n",
    "                        print(f\"  🎯 Correlation with {atn_col}:\")\n",
    "                        print(f\"    Original correlation: {orig_corr:.6f}\")\n",
    "                        print(f\"    Ethiopia corrected: {corr_corr:.6f}\")\n",
    "                        \n",
    "                        if not (pd.isna(orig_corr) or pd.isna(corr_corr)):\n",
    "                            improvement = abs(orig_corr) - abs(corr_corr)\n",
    "                            print(f\"    Improvement: {improvement:.6f} ({'✅ Better!' if improvement > 0 else '⚠️ Check data'})\")\n",
    "                            \n",
    "                            if improvement > 0:\n",
    "                                improvement_pct = (improvement / abs(orig_corr)) * 100\n",
    "                                print(f\"    🎉 {improvement_pct:.1f}% correlation reduction!\")\n",
    "                                \n",
    "                                # Ethiopia fix effectiveness\n",
    "                                if abs(corr_corr) < 0.1:  # Near zero correlation\n",
    "                                    print(f\"    ✨ Excellent: Near-zero correlation achieved!\")\n",
    "                                elif improvement > 0.1:  # Significant improvement\n",
    "                                    print(f\"    👍 Good: Significant correlation reduction\")\n",
    "                                else:\n",
    "                                    print(f\"    📈 Moderate improvement\")\n",
    "                        else:\n",
    "                            print(f\"    ⚠️ Could not calculate correlation improvement\")\n",
    "                    else:\n",
    "                        print(f\"    ⚠️ Insufficient data for analysis (orig: {len(common_orig)}, eth: {len(common_eth)})\")\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"    ⚠️ Error in analysis: {e}\")\n",
    "            else:\n",
    "                missing_cols = []\n",
    "                if not has_original_bcc:\n",
    "                    missing_cols.append(f\"Original: {original_bcc}\")\n",
    "                if not has_corrected_bcc:\n",
    "                    missing_cols.append(f\"Ethiopia: {corrected_bcc}\")\n",
    "                if not (has_atn_original and has_atn_ethiopia):\n",
    "                    missing_cols.append(f\"ATN: {atn_col}\")\n",
    "                print(f\"    ❌ Missing columns: {', '.join(missing_cols)}\")\n",
    "        \n",
    "        # Summary of Ethiopia fix benefits\n",
    "        print(f\"\\n✅ Ethiopia Fix Summary:\")\n",
    "        print(\"=\" * 30)\n",
    "        print(f\"📈 Data processing: {original_data.shape[0]:,} → {ethiopia_data.shape[0]:,} rows\")\n",
    "        print(f\"🔧 Columns added: {ethiopia_data.shape[1] - original_data.shape[1]} correction columns\")\n",
    "        print(f\"🎯 Primary benefit: Reduced BCc-ATN1 correlation (pneumatic pump fix)\")\n",
    "        print(f\"📊 Quality control: Applied comprehensive cleaning pipeline\")\n",
    "        print(f\"🧹 DEMA smoothing: Added for noise reduction\")\n",
    "        \n",
    "    elif \"Ethiopia\" in datasets:\n",
    "        # Only Ethiopia data available - validate it has corrections\n",
    "        ethiopia_data = datasets[\"Ethiopia\"]\n",
    "        ethiopia_cols = [col for col in ethiopia_data.columns if any(x in col for x in ['corrected', 'manual', 'optimized', 'denominator'])]\n",
    "        \n",
    "        print(f\"\\n✅ Ethiopia-enhanced data loaded successfully!\")\n",
    "        print(f\"🔧 Found {len(ethiopia_cols)} correction columns\")\n",
    "        print(\"⚠️ Original uncleaned data not available for comparison\")\n",
    "        \n",
    "        if ethiopia_cols:\n",
    "            wavelengths = ['IR', 'Blue', 'Red', 'Green', 'UV']\n",
    "            for wl in wavelengths:\n",
    "                wl_cols = [col for col in ethiopia_cols if wl in col]\n",
    "                if wl_cols:\n",
    "                    print(f\"  {wl}: {len(wl_cols)} corrections\")\n",
    "    \n",
    "    else:\n",
    "        print(f\"\\n⚠️ Could not load required files for comparison\")\n",
    "        if \"Original\" not in datasets:\n",
    "            print(\"❌ Original uncleaned data not found\")\n",
    "        if \"Ethiopia\" not in datasets:\n",
    "            print(\"❌ Ethiopia processed data not found - run processing with APPLY_ETHIOPIA_FIX = True\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error during validation: {e}\")\n",
    "\n",
    "print(f\"\\n📊 Current Session Data:\")\n",
    "print(\"=\" * 50)\n",
    "if 'pkl_data_cleaned' in locals():\n",
    "    print(f\"Shape: {pkl_data_cleaned.shape}\")\n",
    "    print(f\"Date range: {pkl_data_cleaned['datetime_local'].min()} to {pkl_data_cleaned['datetime_local'].max()}\")\n",
    "    \n",
    "    # Show Ethiopia-specific columns if present\n",
    "    ethiopia_specific = [col for col in pkl_data_cleaned.columns if any(x in col for x in ['corrected', 'manual', 'optimized', 'denominator'])]\n",
    "    if ethiopia_specific:\n",
    "        print(f\"🔧 Ethiopia corrections in current data: {len(ethiopia_specific)} columns\")\n",
    "        has_ethiopia_fix = True\n",
    "    else:\n",
    "        has_ethiopia_fix = False\n",
    "    \n",
    "    print(f\"✅ Current data: {'Ethiopia-enhanced' if has_ethiopia_fix else 'Standard'} processing\")\n",
    "else:\n",
    "    print(\"No current data available - run the processing cell first\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
