{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enhanced PKL Processing Test Notebook\n",
    "\n",
    "This notebook tests the integrated enhanced PKL processing functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ahzs645/Github/aethmodular/src/data/qc/../../external/calibration.py:2855: SyntaxWarning: invalid escape sequence '\\['\n",
      "  status_vals_desc = sub(\"'|\\[|\\]\",'',str(status_vals_desc_list))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Advanced plotting style configured\n",
      "🚀 Aethalometer-FTIR/HIPS Pipeline with Simplified Setup\n",
      "============================================================\n",
      "📊 Configuration Summary:\n",
      "   Site: ETAD\n",
      "   Wavelength: Red\n",
      "   Output format: jpl\n",
      "   Quality threshold: 10 minutes\n",
      "   Output directory: outputs\n",
      "\n",
      "📁 File paths:\n",
      "   pkl_data: ✅ df_uncleaned_Jacros_API_and_OG.pkl\n",
      "   csv_data: ✅ Jacros_MA350_1-min_2022-2024_Cleaned.csv\n",
      "   FTIR DB: ✅ spartan_ftir_hips.db\n",
      "🧹 Enhanced setup with PKL cleaning capabilities loaded\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "import sys\n",
    "import os\n",
    "sys.path.insert(0, os.path.join(os.path.dirname(os.getcwd()), 'src'))\n",
    "\n",
    "# Import the new enhanced PKL processing module\n",
    "from data.qc.enhanced_pkl_processing import process_pkl_data_enhanced, EnhancedPKLProcessor\n",
    "from config.notebook_config import NotebookConfig\n",
    "from notebook_utils.pkl_cleaning_integration import create_enhanced_setup\n",
    "\n",
    "# Your existing configuration\n",
    "config = NotebookConfig(\n",
    "    site_code='ETAD',\n",
    "    wavelength='Red',\n",
    "    quality_threshold=10,\n",
    "    output_format='jpl',\n",
    "    min_samples_for_analysis=30,\n",
    "    confidence_level=0.95,\n",
    "    outlier_threshold=3.0,\n",
    "    figure_size=(12, 8),\n",
    "    font_size=10,\n",
    "    dpi=300\n",
    ")\n",
    "\n",
    "# Set your data paths (same as before)\n",
    "base_data_path = \"/Users/ahzs645/Library/CloudStorage/GoogleDrive-ahzs645@gmail.com/My Drive/University/Research/Grad/UC Davis Ann/NASA MAIA/Data\"\n",
    "\n",
    "config.aethalometer_files = {\n",
    "    'pkl_data': os.path.join(\n",
    "        base_data_path,\n",
    "        \"Aethelometry Data/Kyan Data/Mergedcleaned and uncleaned MA350 data20250707030704\",\n",
    "        \"df_uncleaned_Jacros_API_and_OG.pkl\"\n",
    "    ),\n",
    "    'csv_data': os.path.join(\n",
    "        base_data_path,\n",
    "        \"Aethelometry Data/Raw\",\n",
    "        \"Jacros_MA350_1-min_2022-2024_Cleaned.csv\"\n",
    "    )\n",
    "}\n",
    "\n",
    "config.ftir_db_path = os.path.join(\n",
    "    base_data_path,\n",
    "    \"EC-HIPS-Aeth Comparison/Data/Original Data/Combined Database\",\n",
    "    \"spartan_ftir_hips.db\"\n",
    ")\n",
    "\n",
    "# Create enhanced setup\n",
    "setup = create_enhanced_setup(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📁 Loading datasets...\n",
      "📦 Setting up modular system...\n",
      "✅ Aethalometer loaders imported\n",
      "✅ Database loader imported\n",
      "✅ Plotting utilities imported\n",
      "✅ Plotting style configured\n",
      "✅ Successfully imported 5 modular components\n",
      "\n",
      "============================================================\n",
      "📁 LOADING DATASETS\n",
      "============================================================\n",
      "📁 Loading all datasets...\n",
      "\n",
      "==================================================\n",
      "📊 Loading pkl_data\n",
      "==================================================\n",
      "📁 Loading pkl_data: df_uncleaned_Jacros_API_and_OG.pkl\n",
      "Detected format: standard\n",
      "Set 'datetime_local' as DatetimeIndex for time series operations\n",
      "Converted 17 columns to JPL format\n",
      "Warning: Missing recommended columns: ['datetime_local', 'Biomass.BCc', 'Fossil.fuel.BCc']\n",
      "✅ Modular load: 1,665,156 rows × 238 columns\n",
      "📊 Method: modular\n",
      "📊 Format: jpl\n",
      "📊 Memory: 7443.05 MB\n",
      "🧮 BC columns: 30\n",
      "📈 ATN columns: 25\n",
      "📅 Time range: 2021-01-09 16:38:00 to 2025-06-26 23:18:00\n",
      "✅ pkl_data loaded successfully\n",
      "\n",
      "==================================================\n",
      "📊 Loading csv_data\n",
      "==================================================\n",
      "📁 Loading csv_data: Jacros_MA350_1-min_2022-2024_Cleaned.csv\n",
      "Set 'Time (Local)' as DatetimeIndex for time series operations\n",
      "Converted 5 columns to JPL format\n",
      "✅ Modular load: 1,095,086 rows × 77 columns\n",
      "📊 Method: modular\n",
      "📊 Format: jpl\n",
      "📊 Memory: 884.83 MB\n",
      "🧮 BC columns: 15\n",
      "📈 ATN columns: 10\n",
      "📅 Time range: 2022-04-12 12:46:01+03:00 to 2024-08-20 12:01:00+03:00\n",
      "✅ csv_data loaded successfully\n",
      "\n",
      "==================================================\n",
      "🗃️ Loading FTIR/HIPS data\n",
      "==================================================\n",
      "🗃️ Loading FTIR/HIPS data for site ETAD...\n",
      "📊 Available sites: ['ILNZ', 'ILHA', 'ZAJB', 'CAHA', 'CASH', 'AEAZ', 'AUMN', 'KRUL', 'MXMC', 'ZAPR', 'CHTS', 'ETAD', 'INDH', 'TWTA', 'USPA', 'TWKA', 'KRSE', 'PRFJ', 'BDDU', 'BIBU', 'USNO', 'IDBD', None]\n",
      "✅ Modular FTIR load: 168 samples\n",
      "📅 Date range: 2022-12-07 00:00:00 to 2024-05-12 00:00:00\n",
      "✅ FTIR/HIPS data loaded successfully\n",
      "\n",
      "📊 Loading summary: 3 datasets loaded\n",
      "\n",
      "📊 LOADING SUMMARY\n",
      "============================================================\n",
      "✅ Successfully loaded 3 datasets\n",
      "   - pkl_data: 1,665,156 rows × 238 columns\n",
      "   - csv_data: 1,095,086 rows × 77 columns\n",
      "   - ftir_hips: 168 rows × 12 columns\n",
      "============================================================\n",
      "✅ Converting datetime_local from index to column...\n",
      "📊 PKL data ready: (1665156, 239)\n",
      "📅 Date range: 2021-01-09 16:38:00 to 2025-06-26 23:18:00\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "print(\"📁 Loading datasets...\")\n",
    "datasets = setup.load_all_data()\n",
    "\n",
    "# Get PKL data\n",
    "pkl_data_original = setup.get_dataset('pkl_data')\n",
    "\n",
    "# Quick fix for datetime_local issue (same as before)\n",
    "if 'datetime_local' not in pkl_data_original.columns:\n",
    "    if pkl_data_original.index.name == 'datetime_local':\n",
    "        print(\"✅ Converting datetime_local from index to column...\")\n",
    "        pkl_data_original = pkl_data_original.reset_index()\n",
    "    elif hasattr(pkl_data_original.index, 'tz'):\n",
    "        print(\"✅ Creating datetime_local column from datetime index...\")\n",
    "        pkl_data_original['datetime_local'] = pkl_data_original.index\n",
    "        pkl_data_original = pkl_data_original.reset_index(drop=True)\n",
    "\n",
    "print(f\"📊 PKL data ready: {pkl_data_original.shape}\")\n",
    "print(f\"📅 Date range: {pkl_data_original['datetime_local'].min()} to {pkl_data_original['datetime_local'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Enhanced PKL Data Processing Pipeline\n",
      "============================================================\n",
      "🔧 Comprehensive Preprocessing Pipeline\n",
      "============================================================\n",
      "Step 1: Processing datetime...\n",
      "\n",
      "Step 2: Fixing column names...\n",
      "✅ Renamed 16 columns\n",
      "\n",
      "Step 3: Converting data types...\n",
      "Converted IR ATN1 to float.\n",
      "Converted UV ATN1 to float.\n",
      "Converted Blue ATN1 to float.\n",
      "Converted Green ATN1 to float.\n",
      "Converted Red ATN1 to float.\n",
      "✅ Applied calibration.convert_to_float()\n",
      "\n",
      "Step 4: Adding Session ID...\n",
      "\n",
      "Step 5: Adding delta calculations...\n",
      "✅ Applied calibration.add_deltas()\n",
      "\n",
      "Step 6: Final adjustments...\n",
      "✅ Filtered to 2022+: 1,665,156 -> 1,627,058 rows\n",
      "🔄 Applying DEMA Smoothing...\n",
      "========================================\n",
      "\n",
      "Processing IR wavelength...\n",
      "  Available BC columns: ['IR BC1', 'IR BC2', 'IR BCc']\n",
      "  ✅ Created IR BC1 smoothed\n",
      "  ✅ Created IR BC2 smoothed\n",
      "  ✅ Created IR BCc smoothed\n",
      "\n",
      "Processing Blue wavelength...\n",
      "  Available BC columns: ['Blue BC1', 'Blue BC2', 'Blue BCc']\n",
      "  ✅ Created Blue BC1 smoothed\n",
      "  ✅ Created Blue BC2 smoothed\n",
      "  ✅ Created Blue BCc smoothed\n",
      "\n",
      "🧹 Final Cleaning Pipeline\n",
      "============================================================\n",
      "Starting PKL data cleaning pipeline...\n",
      "==================================================\n",
      "🔍 Data Structure Diagnosis:\n",
      "------------------------------\n",
      "DataFrame shape: (1627058, 284)\n",
      "Date range: 2022-04-12 09:12:00 to 2025-06-26 23:18:00\n",
      "BC columns: 15 (e.g., ['Blue BC1', 'Blue BC2', 'Blue BCc'])\n",
      "BC smoothed columns: 6 (e.g., ['IR BC1 smoothed', 'IR BC2 smoothed', 'IR BCc smoothed'])\n",
      "ATN columns: 40 (e.g., ['Blue ATN1', 'Blue ATN2', 'Green ATN1'])\n",
      "Flow columns: 4 (e.g., ['Flow setpoint (mL/min)', 'Flow total (mL/min)', 'Flow1 (mL/min)'])\n",
      "\n",
      "Targeted wavelengths: ['IR', 'Blue']\n",
      "  IR: ✅ BC | ✅ BC smoothed | ✅ ATN\n",
      "  Blue: ✅ BC | ✅ BC smoothed | ✅ ATN\n",
      "------------------------------\n",
      "\n",
      "🧹 Starting cleaning steps...\n",
      "1919 datapoints removed due to Start up or Tape advance status\n",
      "Statuses of concern, count by device and status:\n",
      "\n",
      "MA350-0238 Flow unstable 750\n",
      "MA350-0238 Optical saturation 0\n",
      "MA350-0238 Sample timing error 0\n",
      "Number of datapoints with invalid optics values\n",
      "AFTER dropping data with 'Optical saturation' status values: 802\n",
      "Removed 56128 datapoints for optics\n",
      "Status cleaning: Removed 58797 rows (3.61%)\n",
      "Extreme BCc cleaning: Removed 13900 rows (0.89%)\n",
      "Flow range cleaning: Removed 0 rows (0.00%)\n",
      "Abnormal flow ratio: Removed 29362 rows (1.89%)\n",
      "Leak ratio cleaning: Removed 507 rows (0.03%)\n",
      "BCc denominator cleaning: Removed 25537 rows (1.68%)\n",
      "Sharp change 605\n",
      "noise 1425\n",
      "Temperature change cleaning: Removed 2030 rows (0.14%)\n",
      "IR ATN1_roughness: threshold=0.1147, high periods flagged: 17945 rows so far\n",
      "IR ATN2_roughness: threshold=0.1008, high periods flagged: 18439 rows so far\n",
      "Blue ATN1_roughness: threshold=0.1859, high periods flagged: 19114 rows so far\n",
      "Blue ATN2_roughness: threshold=0.1664, high periods flagged: 19142 rows so far\n",
      "==================================================\n",
      "Cleaning complete! Final data shape: (1477783, 293)\n",
      "\n",
      "📊 Processing Results Summary:\n",
      "============================================================\n",
      "Original data points: 1,665,156\n",
      "After preprocessing: 1,627,058\n",
      "After smoothing: 1,627,058\n",
      "Final cleaned: 1,477,783\n",
      "Total removed: 187,373 (11.25%)\n",
      "\n",
      "✅ PKL data processing completed successfully!\n",
      "\n",
      "📊 Final data verification:\n",
      "Shape: (1477783, 293)\n",
      "Date range: 2022-04-12 09:54:00 to 2025-06-26 23:18:00\n",
      "  ✅ IR ATN1\n",
      "  ✅ IR BCc\n",
      "  ✅ Blue ATN1\n",
      "  ✅ Blue BCc\n",
      "  ✅ Flow total (mL/min)\n",
      "  ✅ Smoothed columns: 6\n",
      "\n",
      "💾 Cleaned data exported:\n",
      "  📄 CSV: pkl_data_cleaned_enhanced.csv\n",
      "  📦 Pickle: pkl_data_cleaned_enhanced.pkl\n",
      "\n",
      "🎉 Processing Complete!\n",
      "📊 Final shape: (1477783, 293)\n",
      "🚀 Ready for further analysis!\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# SIMPLIFIED PROCESSING: Replace all the complex pipeline with one function call!\n",
    "\n",
    "# Option 1: Simple one-liner (with export)\n",
    "pkl_data_cleaned = process_pkl_data_enhanced(\n",
    "    pkl_data_original,\n",
    "    wavelengths_to_filter=['IR', 'Blue'],  # Focus on IR and Blue\n",
    "    export_path='pkl_data_cleaned_enhanced',  # Will create .csv and .pkl files\n",
    "    verbose=True  # Show detailed progress\n",
    ")\n",
    "\n",
    "print(\"\\n🎉 Processing Complete!\")\n",
    "print(f\"📊 Final shape: {pkl_data_cleaned.shape}\")\n",
    "print(\"🚀 Ready for further analysis!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Option 2: More control with the class (if you need to customize further)\n",
    "\n",
    "# Create processor with custom settings\n",
    "processor = EnhancedPKLProcessor(\n",
    "    wavelengths_to_filter=['IR', 'Blue'],\n",
    "    verbose=True,\n",
    "    # You can pass additional PKLDataCleaner arguments here\n",
    "    quality_threshold=10  # Example custom parameter\n",
    ")\n",
    "\n",
    "# Run the full pipeline\n",
    "pkl_data_cleaned_v2 = processor.process_pkl_data(\n",
    "    pkl_data_original,\n",
    "    export_path='pkl_data_cleaned_v2'\n",
    ")\n",
    "\n",
    "# Or run individual steps if needed\n",
    "# df_preprocessed = processor.comprehensive_preprocessing(pkl_data_original)\n",
    "# df_smoothed = processor.apply_dema_smoothing(df_preprocessed)\n",
    "# df_cleaned = processor.cleaner.clean_pipeline(df_smoothed, skip_preprocessing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Final Verification:\n",
      "==================================================\n",
      "✅ datetime_local\n",
      "✅ IR ATN1\n",
      "✅ IR BCc\n",
      "✅ Blue ATN1\n",
      "✅ Blue BCc\n",
      "✅ Flow total (mL/min)\n",
      "\n",
      "📈 Smoothed columns (6):\n",
      "  • IR BC1 smoothed\n",
      "  • IR BC2 smoothed\n",
      "  • IR BCc smoothed\n",
      "  • Blue BC1 smoothed\n",
      "  • Blue BC2 smoothed\n",
      "  • Blue BCc smoothed\n",
      "\n",
      "📊 Summary Statistics:\n",
      "Original rows: 1,665,156\n",
      "Final rows: 1,477,783\n",
      "Columns: 293\n",
      "Date range: 2022-04-12 09:54:00 to 2025-06-26 23:18:00\n",
      "Memory usage: 7090.1 MB\n",
      "\n",
      "✅ Enhanced PKL processing complete and verified!\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# Verification and comparison\n",
    "print(\"\\n📊 Final Verification:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Check that we have all the key columns\n",
    "key_columns = ['datetime_local', 'IR ATN1', 'IR BCc', 'Blue ATN1', 'Blue BCc', 'Flow total (mL/min)']\n",
    "for col in key_columns:\n",
    "    if col in pkl_data_cleaned.columns:\n",
    "        print(f\"✅ {col}\")\n",
    "    else:\n",
    "        print(f\"❌ {col}\")\n",
    "\n",
    "# Check smoothed columns\n",
    "smoothed_cols = [col for col in pkl_data_cleaned.columns if 'smoothed' in col]\n",
    "print(f\"\\n📈 Smoothed columns ({len(smoothed_cols)}):\")\n",
    "for col in smoothed_cols[:10]:  # Show first 10\n",
    "    print(f\"  • {col}\")\n",
    "\n",
    "# Summary statistics\n",
    "print(f\"\\n📊 Summary Statistics:\")\n",
    "print(f\"Original rows: {len(pkl_data_original):,}\")\n",
    "print(f\"Final rows: {len(pkl_data_cleaned):,}\")\n",
    "print(f\"Columns: {pkl_data_cleaned.shape[1]}\")\n",
    "print(f\"Date range: {pkl_data_cleaned['datetime_local'].min()} to {pkl_data_cleaned['datetime_local'].max()}\")\n",
    "\n",
    "# Memory usage\n",
    "memory_mb = pkl_data_cleaned.memory_usage(deep=True).sum() / 1024 / 1024\n",
    "print(f\"Memory usage: {memory_mb:.1f} MB\")\n",
    "\n",
    "print(\"\\n✅ Enhanced PKL processing complete and verified!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Quick Quality Check\n",
      "Time range: 2022-04-12 09:54:00 to 2025-06-26 23:18:00 (1172 days)\n",
      "Expected points: 1,687,045\n",
      "Actual points: 1,477,783\n",
      "Missing: 209,262 (12.40%)\n",
      "Full missing days: 69\n",
      "Partial missing days: 1100\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# Optional: Quick quality check using the modular QC tools\n",
    "from data.qc import quick_quality_check\n",
    "\n",
    "# Set datetime as index for quality check\n",
    "pkl_for_qc = pkl_data_cleaned.set_index('datetime_local')\n",
    "quick_quality_check(pkl_for_qc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
