{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e9dffb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Calibration module loaded successfully\n",
      "✅ Advanced plotting style configured\n",
      "🚀 Aethalometer-FTIR/HIPS Pipeline with Simplified Setup\n",
      "============================================================\n",
      "📊 Configuration Summary:\n",
      "   Site: ETAD\n",
      "   Wavelength: Red\n",
      "   Output format: jpl\n",
      "   Quality threshold: 10 minutes\n",
      "   Output directory: outputs\n",
      "\n",
      "📁 File paths:\n",
      "   pkl_data: ✅ df_uncleaned_Jacros_API_and_OG.pkl\n",
      "   csv_data: ✅ Jacros_MA350_1-min_2022-2024_Cleaned.csv\n",
      "   FTIR DB: ✅ spartan_ftir_hips.db\n",
      "🧹 Enhanced setup with PKL cleaning capabilities loaded\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.insert(0, os.path.join(os.path.dirname(os.getcwd()), 'src'))\n",
    "\n",
    "# Import enhanced setup with PKL cleaning capabilities\n",
    "from notebook_utils.pkl_cleaning_integration import create_enhanced_setup\n",
    "from config.notebook_config import NotebookConfig\n",
    "from data.qc.pkl_cleaning import PKLDataCleaner\n",
    "\n",
    "# ADD THIS: Import calibration module\n",
    "import importlib.util\n",
    "try:\n",
    "    # Try to import calibration module (adjust path as needed)\n",
    "    module_path = os.path.join(os.path.dirname(os.getcwd()), \"src\", \"external\", \"calibration.py\")\n",
    "    if not os.path.exists(module_path):\n",
    "        # Alternative paths to try\n",
    "        alt_paths = [\n",
    "            os.path.join(os.getcwd(), \"calibration.py\"),\n",
    "            os.path.join(os.path.dirname(os.getcwd()), \"calibration.py\"),\n",
    "            \"calibration.py\"\n",
    "        ]\n",
    "        for alt_path in alt_paths:\n",
    "            if os.path.exists(alt_path):\n",
    "                module_path = alt_path\n",
    "                break\n",
    "    \n",
    "    if os.path.exists(module_path):\n",
    "        module_name = \"calibration\"\n",
    "        spec = importlib.util.spec_from_file_location(module_name, module_path)\n",
    "        module = importlib.util.module_from_spec(spec)\n",
    "        sys.modules[module_name] = module\n",
    "        spec.loader.exec_module(module)\n",
    "        import calibration\n",
    "        print(\"✅ Calibration module loaded successfully\")\n",
    "        HAS_CALIBRATION = True\n",
    "    else:\n",
    "        print(\"⚠️ Calibration module not found, will use alternative preprocessing\")\n",
    "        HAS_CALIBRATION = False\n",
    "except Exception as e:\n",
    "    print(f\"⚠️ Could not import calibration module: {e}\")\n",
    "    HAS_CALIBRATION = False\n",
    "\n",
    "# Your existing configuration\n",
    "config = NotebookConfig(\n",
    "    site_code='ETAD',\n",
    "    wavelength='Red',\n",
    "    quality_threshold=10,\n",
    "    output_format='jpl',\n",
    "    min_samples_for_analysis=30,\n",
    "    confidence_level=0.95,\n",
    "    outlier_threshold=3.0,\n",
    "    figure_size=(12, 8),\n",
    "    font_size=10,\n",
    "    dpi=300\n",
    ")\n",
    "\n",
    "# Set your data paths (same as before)\n",
    "base_data_path = \"/Users/ahzs645/Library/CloudStorage/GoogleDrive-ahzs645@gmail.com/My Drive/University/Research/Grad/UC Davis Ann/NASA MAIA/Data\"\n",
    "\n",
    "config.aethalometer_files = {\n",
    "    'pkl_data': os.path.join(\n",
    "        base_data_path,\n",
    "        \"Aethelometry Data/Kyan Data/Mergedcleaned and uncleaned MA350 data20250707030704\",\n",
    "        \"df_uncleaned_Jacros_API_and_OG.pkl\"\n",
    "    ),\n",
    "    'csv_data': os.path.join(\n",
    "        base_data_path,\n",
    "        \"Aethelometry Data/Raw\",\n",
    "        \"Jacros_MA350_1-min_2022-2024_Cleaned.csv\"\n",
    "    )\n",
    "}\n",
    "\n",
    "config.ftir_db_path = os.path.join(\n",
    "    base_data_path,\n",
    "    \"EC-HIPS-Aeth Comparison/Data/Original Data/Combined Database\",\n",
    "    \"spartan_ftir_hips.db\"\n",
    ")\n",
    "\n",
    "# Create enhanced setup with PKL cleaning capabilities\n",
    "setup = create_enhanced_setup(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f42c9f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📁 Loading datasets...\n",
      "📦 Setting up modular system...\n",
      "✅ Aethalometer loaders imported\n",
      "✅ Database loader imported\n",
      "✅ Plotting utilities imported\n",
      "✅ Plotting style configured\n",
      "✅ Successfully imported 5 modular components\n",
      "\n",
      "============================================================\n",
      "📁 LOADING DATASETS\n",
      "============================================================\n",
      "📁 Loading all datasets...\n",
      "\n",
      "==================================================\n",
      "📊 Loading pkl_data\n",
      "==================================================\n",
      "📁 Loading pkl_data: df_uncleaned_Jacros_API_and_OG.pkl\n",
      "Detected format: standard\n",
      "Set 'datetime_local' as DatetimeIndex for time series operations\n",
      "Converted 17 columns to JPL format\n",
      "Warning: Missing recommended columns: ['datetime_local', 'Biomass.BCc', 'Fossil.fuel.BCc']\n",
      "✅ Modular load: 1,665,156 rows × 238 columns\n",
      "📊 Method: modular\n",
      "📊 Format: jpl\n",
      "📊 Memory: 7443.05 MB\n",
      "🧮 BC columns: 30\n",
      "📈 ATN columns: 25\n",
      "📅 Time range: 2021-01-09 16:38:00 to 2025-06-26 23:18:00\n",
      "✅ pkl_data loaded successfully\n",
      "\n",
      "==================================================\n",
      "📊 Loading csv_data\n",
      "==================================================\n",
      "📁 Loading csv_data: Jacros_MA350_1-min_2022-2024_Cleaned.csv\n",
      "Set 'Time (Local)' as DatetimeIndex for time series operations\n",
      "Converted 5 columns to JPL format\n",
      "✅ Modular load: 1,095,086 rows × 77 columns\n",
      "📊 Method: modular\n",
      "📊 Format: jpl\n",
      "📊 Memory: 884.83 MB\n",
      "🧮 BC columns: 15\n",
      "📈 ATN columns: 10\n",
      "📅 Time range: 2022-04-12 12:46:01+03:00 to 2024-08-20 12:01:00+03:00\n",
      "✅ csv_data loaded successfully\n",
      "\n",
      "==================================================\n",
      "🗃️ Loading FTIR/HIPS data\n",
      "==================================================\n",
      "🗃️ Loading FTIR/HIPS data for site ETAD...\n",
      "📊 Available sites: ['ILNZ', 'ILHA', 'ZAJB', 'CAHA', 'CASH', 'AEAZ', 'AUMN', 'KRUL', 'MXMC', 'ZAPR', 'CHTS', 'ETAD', 'INDH', 'TWTA', 'USPA', 'TWKA', 'KRSE', 'PRFJ', 'BDDU', 'BIBU', 'USNO', 'IDBD', None]\n",
      "✅ Modular FTIR load: 168 samples\n",
      "📅 Date range: 2022-12-07 00:00:00 to 2024-05-12 00:00:00\n",
      "✅ FTIR/HIPS data loaded successfully\n",
      "\n",
      "📊 Loading summary: 3 datasets loaded\n",
      "\n",
      "📊 LOADING SUMMARY\n",
      "============================================================\n",
      "✅ Successfully loaded 3 datasets\n",
      "   - pkl_data: 1,665,156 rows × 238 columns\n",
      "   - csv_data: 1,095,086 rows × 77 columns\n",
      "   - ftir_hips: 168 rows × 12 columns\n",
      "============================================================\n",
      "✅ Converting datetime_local from index to column...\n",
      "📊 PKL data ready: (1665156, 239)\n",
      "📅 Date range: 2021-01-09 16:38:00 to 2025-06-26 23:18:00\n"
     ]
    }
   ],
   "source": [
    "print(\"📁 Loading datasets...\")\n",
    "datasets = setup.load_all_data()\n",
    "\n",
    "# Get PKL data and fix datetime_local issue\n",
    "pkl_data_original = setup.get_dataset('pkl_data')\n",
    "\n",
    "# Quick fix for datetime_local issue\n",
    "if 'datetime_local' not in pkl_data_original.columns:\n",
    "    if pkl_data_original.index.name == 'datetime_local':\n",
    "        print(\"✅ Converting datetime_local from index to column...\")\n",
    "        pkl_data_original = pkl_data_original.reset_index()\n",
    "    elif hasattr(pkl_data_original.index, 'tz'):\n",
    "        print(\"✅ Creating datetime_local column from datetime index...\")\n",
    "        pkl_data_original['datetime_local'] = pkl_data_original.index\n",
    "        pkl_data_original = pkl_data_original.reset_index(drop=True)\n",
    "\n",
    "print(f\"📊 PKL data ready: {pkl_data_original.shape}\")\n",
    "print(f\"📅 Date range: {pkl_data_original['datetime_local'].min()} to {pkl_data_original['datetime_local'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a274f632",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Comprehensive Preprocessing Pipeline\n",
      "============================================================\n",
      "Step 1: Processing datetime...\n",
      "\n",
      "Step 2: Fixing column names...\n",
      "✅ Renamed 16 columns\n",
      "\n",
      "Step 3: Converting data types...\n",
      "Converted IR ATN1 to float.\n",
      "Converted UV ATN1 to float.\n",
      "Converted Blue ATN1 to float.\n",
      "Converted Green ATN1 to float.\n",
      "Converted Red ATN1 to float.\n",
      "✅ Applied calibration.convert_to_float()\n",
      "\n",
      "Step 4: Adding Session ID...\n",
      "\n",
      "Step 5: Adding delta calculations...\n",
      "✅ Applied calibration.add_deltas()\n",
      "\n",
      "Step 6: Final adjustments...\n",
      "✅ Filtered to 2022+: 1,665,156 -> 1,627,058 rows\n"
     ]
    }
   ],
   "source": [
    "def comprehensive_preprocessing(df):\n",
    "    \"\"\"Apply all the preprocessing steps that the working pipeline includes\"\"\"\n",
    "    print(\"🔧 Comprehensive Preprocessing Pipeline\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    df_processed = df.copy()\n",
    "    original_size = len(df_processed)\n",
    "    \n",
    "    # Step 1: Fix datetime column\n",
    "    print(\"Step 1: Processing datetime...\")\n",
    "    if 'datetime_local' in df_processed.columns:\n",
    "        if not pd.api.types.is_datetime64_any_dtype(df_processed['datetime_local']):\n",
    "            df_processed['datetime_local'] = pd.to_datetime(\n",
    "                df_processed['datetime_local'], utc=True\n",
    "            ).dt.tz_convert('Africa/Addis_Ababa')\n",
    "            print(\"✅ Converted datetime_local to proper timezone\")\n",
    "    \n",
    "    # Step 2: Column renaming (your existing logic but improved)\n",
    "    print(\"\\nStep 2: Fixing column names...\")\n",
    "    column_mapping = {}\n",
    "    \n",
    "    # Map BC columns (handle both BC1->BCc conversion and dot notation)\n",
    "    for wl in ['IR', 'Blue', 'Green', 'Red', 'UV']:\n",
    "        # First priority: use .BCc if available\n",
    "        if f'{wl}.BCc' in df_processed.columns:\n",
    "            column_mapping[f'{wl}.BCc'] = f'{wl} BCc'\n",
    "        # Second priority: rename BC1 to BCc\n",
    "        elif f'{wl} BC1' in df_processed.columns:\n",
    "            df_processed = df_processed.rename(columns={f'{wl} BC1': f'{wl} BCc'})\n",
    "            print(f\"  Renamed {wl} BC1 -> {wl} BCc\")\n",
    "    \n",
    "    # Map ATN columns (dots to spaces)\n",
    "    for wl in ['IR', 'Blue', 'Green', 'Red', 'UV']:\n",
    "        for spot in [1, 2]:\n",
    "            if f'{wl}.ATN{spot}' in df_processed.columns:\n",
    "                column_mapping[f'{wl}.ATN{spot}'] = f'{wl} ATN{spot}'\n",
    "    \n",
    "    # Map flow columns\n",
    "    if 'Flow.total.mL.min' in df_processed.columns:\n",
    "        column_mapping['Flow.total.mL.min'] = 'Flow total (mL/min)'\n",
    "    \n",
    "    # Apply column renaming\n",
    "    if column_mapping:\n",
    "        df_processed = df_processed.rename(columns=column_mapping)\n",
    "        print(f\"✅ Renamed {len(column_mapping)} columns\")\n",
    "    \n",
    "    # Step 3: Data type conversion\n",
    "    print(\"\\nStep 3: Converting data types...\")\n",
    "    if HAS_CALIBRATION:\n",
    "        df_processed = calibration.convert_to_float(df_processed)\n",
    "        print(\"✅ Applied calibration.convert_to_float()\")\n",
    "    else:\n",
    "        # Manual data type conversion\n",
    "        numeric_cols = []\n",
    "        for col in df_processed.columns:\n",
    "            if any(x in col for x in ['ATN', 'BC', 'Flow', 'temp', 'Temp']):\n",
    "                if df_processed[col].dtype == 'object':\n",
    "                    try:\n",
    "                        df_processed[col] = pd.to_numeric(df_processed[col], errors='coerce')\n",
    "                        numeric_cols.append(col)\n",
    "                    except:\n",
    "                        pass\n",
    "        print(f\"✅ Converted {len(numeric_cols)} columns to numeric\")\n",
    "    \n",
    "    # Step 4: Add Session ID\n",
    "    print(\"\\nStep 4: Adding Session ID...\")\n",
    "    if 'Session ID' not in df_processed.columns and 'Tape position' in df_processed.columns:\n",
    "        position_change = df_processed['Tape position'] != df_processed['Tape position'].shift()\n",
    "        df_processed['Session ID'] = position_change.cumsum()\n",
    "        print(\"✅ Added Session ID based on tape position changes\")\n",
    "    \n",
    "    # Step 5: Add delta calculations\n",
    "    print(\"\\nStep 5: Adding delta calculations...\")\n",
    "    if HAS_CALIBRATION:\n",
    "        df_processed = calibration.add_deltas(df_processed)\n",
    "        print(\"✅ Applied calibration.add_deltas()\")\n",
    "    else:\n",
    "        # Manual delta calculation for critical columns\n",
    "        print(\"⚠️ Manual delta calculation (limited functionality)\")\n",
    "        # Add basic delta calculations for ATN columns\n",
    "        attn_cols = [col for col in df_processed.columns if 'ATN' in col and col.count(' ') == 1]\n",
    "        for col in attn_cols:\n",
    "            try:\n",
    "                if 'Serial number' in df_processed.columns and 'Session ID' in df_processed.columns:\n",
    "                    df_processed[f'delta {col}'] = (\n",
    "                        df_processed.groupby(['Serial number', 'Session ID'])[col].diff()\n",
    "                    )\n",
    "                else:\n",
    "                    df_processed[f'delta {col}'] = df_processed[col].diff()\n",
    "            except:\n",
    "                pass\n",
    "        print(f\"✅ Added basic delta calculations for {len(attn_cols)} ATN columns\")\n",
    "    \n",
    "    # Step 6: Set serial number and filter by year\n",
    "    print(\"\\nStep 6: Final adjustments...\")\n",
    "    df_processed['Serial number'] = \"MA350-0238\"\n",
    "    \n",
    "    if 'datetime_local' in df_processed.columns:\n",
    "        df_processed = df_processed.loc[df_processed['datetime_local'].dt.year >= 2022]\n",
    "        print(f\"✅ Filtered to 2022+: {original_size:,} -> {len(df_processed):,} rows\")\n",
    "    \n",
    "    return df_processed\n",
    "\n",
    "# Apply comprehensive preprocessing\n",
    "pkl_data_preprocessed = comprehensive_preprocessing(pkl_data_original)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59e1ef71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Applying DEMA Smoothing...\n",
      "========================================\n",
      "\n",
      "Processing IR wavelength...\n",
      "  Available BC columns: ['IR BC1', 'IR BC2', 'IR BCc']\n",
      "  ✅ Created IR BC1 smoothed\n",
      "  ✅ Created IR BC2 smoothed\n",
      "  ✅ Created IR BCc smoothed\n",
      "\n",
      "Processing Blue wavelength...\n",
      "  Available BC columns: ['Blue BC1', 'Blue BC2', 'Blue BCc']\n",
      "  ✅ Created Blue BC1 smoothed\n",
      "  ✅ Created Blue BC2 smoothed\n",
      "  ✅ Created Blue BCc smoothed\n"
     ]
    }
   ],
   "source": [
    "def apply_dema_smoothing_working(df, wavelengths=['IR', 'Blue']):\n",
    "    \"\"\"Apply DEMA smoothing that actually works\"\"\"\n",
    "    print(\"🔄 Applying DEMA Smoothing...\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    df_smoothed = df.copy()\n",
    "    \n",
    "    for wl in wavelengths:\n",
    "        print(f\"\\nProcessing {wl} wavelength...\")\n",
    "        \n",
    "        # Check what BC columns we have\n",
    "        bc_cols = [col for col in df_smoothed.columns if wl in col and 'BC' in col and 'smoothed' not in col]\n",
    "        print(f\"  Available BC columns: {bc_cols}\")\n",
    "        \n",
    "        if not bc_cols:\n",
    "            print(f\"  ⚠️ No BC columns found for {wl}\")\n",
    "            continue\n",
    "        \n",
    "        # Process each BC column\n",
    "        for bc_col in bc_cols:\n",
    "            try:\n",
    "                # Group by measurement sessions for proper smoothing\n",
    "                groupby_cols = ['Serial number']\n",
    "                if 'Session ID' in df_smoothed.columns:\n",
    "                    groupby_cols.append('Session ID')\n",
    "                if 'Tape position' in df_smoothed.columns:\n",
    "                    groupby_cols.append('Tape position')\n",
    "                \n",
    "                smoothed_values = []\n",
    "                \n",
    "                for group_keys, group in df_smoothed.groupby(groupby_cols):\n",
    "                    if len(group) > 2:  # Need at least 3 points for smoothing\n",
    "                        values = group[bc_col].dropna()\n",
    "                        if len(values) > 1:\n",
    "                            # Apply DEMA algorithm\n",
    "                            span = min(10, len(values) // 2)  # Adaptive span\n",
    "                            if span < 2:\n",
    "                                span = 2\n",
    "                            \n",
    "                            # First EMA\n",
    "                            ema1 = values.ewm(span=span, adjust=False).mean()\n",
    "                            # Second EMA (EMA of EMA)\n",
    "                            ema2 = ema1.ewm(span=span, adjust=False).mean()\n",
    "                            # DEMA = 2*EMA1 - EMA2\n",
    "                            dema = 2 * ema1 - ema2\n",
    "                            \n",
    "                            # Store results with original indices\n",
    "                            for idx, val in dema.items():\n",
    "                                smoothed_values.append((idx, val))\n",
    "                        else:\n",
    "                            # Not enough data, use original values\n",
    "                            for idx, val in values.items():\n",
    "                                smoothed_values.append((idx, val))\n",
    "                    else:\n",
    "                        # Very small group, use original values\n",
    "                        values = group[bc_col].dropna()\n",
    "                        for idx, val in values.items():\n",
    "                            smoothed_values.append((idx, val))\n",
    "                \n",
    "                # Create smoothed column\n",
    "                smoothed_col = f'{bc_col} smoothed'\n",
    "                df_smoothed[smoothed_col] = np.nan\n",
    "                \n",
    "                for idx, val in smoothed_values:\n",
    "                    df_smoothed.loc[idx, smoothed_col] = val\n",
    "                \n",
    "                print(f\"  ✅ Created {smoothed_col}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  ⚠️ Failed to smooth {bc_col}: {e}\")\n",
    "    \n",
    "    return df_smoothed\n",
    "\n",
    "# Apply DEMA smoothing\n",
    "pkl_data_with_smoothing = apply_dema_smoothing_working(pkl_data_preprocessed, ['IR', 'Blue'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9911e380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🧹 Final Cleaning Pipeline\n",
      "============================================================\n",
      "Starting PKL data cleaning pipeline...\n",
      "==================================================\n",
      "🔍 Data Structure Diagnosis:\n",
      "------------------------------\n",
      "DataFrame shape: (1627058, 284)\n",
      "Date range: 2022-04-12 09:12:00 to 2025-06-26 23:18:00\n",
      "BC columns: 15 (e.g., ['Blue BC1', 'Blue BC2', 'Blue BCc'])\n",
      "BC smoothed columns: 6 (e.g., ['IR BC1 smoothed', 'IR BC2 smoothed', 'IR BCc smoothed'])\n",
      "ATN columns: 40 (e.g., ['Blue ATN1', 'Blue ATN2', 'Green ATN1'])\n",
      "Flow columns: 4 (e.g., ['Flow setpoint (mL/min)', 'Flow total (mL/min)', 'Flow1 (mL/min)'])\n",
      "\n",
      "Targeted wavelengths: ['IR', 'Blue']\n",
      "  IR: ✅ BC | ✅ BC smoothed | ✅ ATN\n",
      "  Blue: ✅ BC | ✅ BC smoothed | ✅ ATN\n",
      "------------------------------\n",
      "\n",
      "🧹 Starting cleaning steps...\n",
      "1919 datapoints removed due to Start up or Tape advance status\n",
      "Statuses of concern, count by device and status:\n",
      "\n",
      "MA350-0238 Flow unstable 750\n",
      "MA350-0238 Optical saturation 0\n",
      "MA350-0238 Sample timing error 0\n",
      "Number of datapoints with invalid optics values\n",
      "AFTER dropping data with 'Optical saturation' status values: 802\n",
      "Removed 56128 datapoints for optics\n",
      "Status cleaning: Removed 58797 rows (3.61%)\n",
      "Extreme BCc cleaning: Removed 13900 rows (0.89%)\n",
      "Flow range cleaning: Removed 0 rows (0.00%)\n",
      "Abnormal flow ratio: Removed 29362 rows (1.89%)\n",
      "Leak ratio cleaning: Removed 507 rows (0.03%)\n",
      "BCc denominator cleaning: Removed 25537 rows (1.68%)\n",
      "Sharp change 605\n",
      "noise 1425\n",
      "Temperature change cleaning: Removed 2030 rows (0.14%)\n",
      "IR ATN1_roughness: threshold=0.1147, high periods flagged: 17945 rows so far\n",
      "IR ATN2_roughness: threshold=0.1008, high periods flagged: 18439 rows so far\n",
      "Blue ATN1_roughness: threshold=0.1859, high periods flagged: 19114 rows so far\n",
      "Blue ATN2_roughness: threshold=0.1664, high periods flagged: 19142 rows so far\n",
      "==================================================\n",
      "Cleaning complete! Final data shape: (1477783, 293)\n",
      "\n",
      "📊 Cleaning Results Summary:\n",
      "============================================================\n",
      "Original data points: 1,665,156\n",
      "After preprocessing: 1,627,058\n",
      "After smoothing: 1,627,058\n",
      "Final cleaned: 1,477,783\n",
      "Total removed: 187,373 (11.25%)\n",
      "\n",
      "✅ PKL data cleaning completed successfully!\n",
      "\n",
      "📊 Final data verification:\n",
      "Shape: (1477783, 293)\n",
      "Date range: 2022-04-12 09:54:00 to 2025-06-26 23:18:00\n",
      "  ✅ IR ATN1\n",
      "  ✅ IR BCc\n",
      "  ✅ Blue ATN1\n",
      "  ✅ Blue BCc\n",
      "  ✅ Flow total (mL/min)\n",
      "  ✅ Smoothed columns: 6\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n🧹 Final Cleaning Pipeline\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    # Initialize cleaner\n",
    "    cleaner = PKLDataCleaner(wavelengths_to_filter=['IR', 'Blue'], verbose=True)\n",
    "    \n",
    "    # Apply cleaning pipeline, skipping preprocessing since we did it comprehensively\n",
    "    pkl_data_cleaned = cleaner.clean_pipeline(pkl_data_with_smoothing, skip_preprocessing=True)\n",
    "    \n",
    "    print(\"\\n📊 Cleaning Results Summary:\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Original data points: {len(pkl_data_original):,}\")\n",
    "    print(f\"After preprocessing: {len(pkl_data_preprocessed):,}\")\n",
    "    print(f\"After smoothing: {len(pkl_data_with_smoothing):,}\")\n",
    "    print(f\"Final cleaned: {len(pkl_data_cleaned):,}\")\n",
    "    \n",
    "    total_removed = len(pkl_data_original) - len(pkl_data_cleaned)\n",
    "    removal_pct = (total_removed / len(pkl_data_original) * 100)\n",
    "    print(f\"Total removed: {total_removed:,} ({removal_pct:.2f}%)\")\n",
    "    \n",
    "    print(\"\\n✅ PKL data cleaning completed successfully!\")\n",
    "    \n",
    "    # Quick verification\n",
    "    print(f\"\\n📊 Final data verification:\")\n",
    "    print(f\"Shape: {pkl_data_cleaned.shape}\")\n",
    "    if 'datetime_local' in pkl_data_cleaned.columns:\n",
    "        print(f\"Date range: {pkl_data_cleaned['datetime_local'].min()} to {pkl_data_cleaned['datetime_local'].max()}\")\n",
    "    \n",
    "    # Check for key columns\n",
    "    key_cols = ['IR ATN1', 'IR BCc', 'Blue ATN1', 'Blue BCc', 'Flow total (mL/min)']\n",
    "    for col in key_cols:\n",
    "        status = \"✅\" if col in pkl_data_cleaned.columns else \"❌\"\n",
    "        print(f\"  {status} {col}\")\n",
    "    \n",
    "    # Check smoothed columns\n",
    "    smoothed_cols = [col for col in pkl_data_cleaned.columns if 'smoothed' in col]\n",
    "    print(f\"  ✅ Smoothed columns: {len(smoothed_cols)}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Final cleaning failed: {e}\")\n",
    "    print(\"Using preprocessed and smoothed data as fallback\")\n",
    "    pkl_data_cleaned = pkl_data_with_smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36f0731f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "💾 Cleaned data exported:\n",
      "  📄 CSV: pkl_data_cleaned_working.csv\n",
      "  📦 Pickle: pkl_data_cleaned_working.pkl\n",
      "\n",
      "🎉 PKL Data Cleaning Complete!\n",
      "📊 Final shape: (1477783, 293)\n",
      "📅 Date range: 2022-04-12 09:54:00 to 2025-06-26 23:18:00\n",
      "🚀 Ready for further analysis!\n"
     ]
    }
   ],
   "source": [
    "if 'pkl_data_cleaned' in locals() and len(pkl_data_cleaned) > 0:\n",
    "    # Export cleaned data\n",
    "    output_csv = 'pkl_data_cleaned_working.csv'\n",
    "    output_pkl = 'pkl_data_cleaned_working.pkl'\n",
    "    \n",
    "    pkl_data_cleaned.to_csv(output_csv, index=False)\n",
    "    pkl_data_cleaned.to_pickle(output_pkl)\n",
    "    \n",
    "    print(f\"\\n💾 Cleaned data exported:\")\n",
    "    print(f\"  📄 CSV: {output_csv}\")\n",
    "    print(f\"  📦 Pickle: {output_pkl}\")\n",
    "    \n",
    "    print(f\"\\n🎉 PKL Data Cleaning Complete!\")\n",
    "    print(f\"📊 Final shape: {pkl_data_cleaned.shape}\")\n",
    "    print(f\"📅 Date range: {pkl_data_cleaned['datetime_local'].min()} to {pkl_data_cleaned['datetime_local'].max()}\")\n",
    "    print(\"🚀 Ready for further analysis!\")\n",
    "    \n",
    "else:\n",
    "    print(\"❌ No cleaned data available to export\")\n",
    "    print(\"Please check the error messages above\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
