{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e9dffb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Advanced plotting style configured\n",
      "🚀 Aethalometer-FTIR/HIPS Pipeline with Simplified Setup\n",
      "============================================================\n",
      "📊 Configuration Summary:\n",
      "   Site: ETAD\n",
      "   Wavelength: Red\n",
      "   Output format: jpl\n",
      "   Quality threshold: 10 minutes\n",
      "   Output directory: outputs\n",
      "\n",
      "📁 File paths:\n",
      "   pkl_data: ✅ df_uncleaned_Jacros_API_and_OG.pkl\n",
      "   csv_data: ✅ Jacros_MA350_1-min_2022-2024_Cleaned.csv\n",
      "   FTIR DB: ✅ spartan_ftir_hips.db\n",
      "🧹 Enhanced setup with PKL cleaning capabilities loaded\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.insert(0, os.path.join(os.path.dirname(os.getcwd()), 'src'))\n",
    "\n",
    "# Import enhanced setup with PKL cleaning capabilities\n",
    "from notebook_utils.pkl_cleaning_integration import create_enhanced_setup\n",
    "from config.notebook_config import NotebookConfig\n",
    "from data.qc.pkl_cleaning import PKLDataCleaner\n",
    "\n",
    "# Your existing configuration\n",
    "config = NotebookConfig(\n",
    "    site_code='ETAD',\n",
    "    wavelength='Red',\n",
    "    quality_threshold=10,\n",
    "    output_format='jpl',\n",
    "    min_samples_for_analysis=30,\n",
    "    confidence_level=0.95,\n",
    "    outlier_threshold=3.0,\n",
    "    figure_size=(12, 8),\n",
    "    font_size=10,\n",
    "    dpi=300\n",
    ")\n",
    "\n",
    "# Set your data paths (same as before)\n",
    "base_data_path = \"/Users/ahzs645/Library/CloudStorage/GoogleDrive-ahzs645@gmail.com/My Drive/University/Research/Grad/UC Davis Ann/NASA MAIA/Data\"\n",
    "\n",
    "config.aethalometer_files = {\n",
    "    'pkl_data': os.path.join(\n",
    "        base_data_path,\n",
    "        \"Aethelometry Data/Kyan Data/Mergedcleaned and uncleaned MA350 data20250707030704\",\n",
    "        \"df_uncleaned_Jacros_API_and_OG.pkl\"\n",
    "    ),\n",
    "    'csv_data': os.path.join(\n",
    "        base_data_path,\n",
    "        \"Aethelometry Data/Raw\",\n",
    "        \"Jacros_MA350_1-min_2022-2024_Cleaned.csv\"\n",
    "    )\n",
    "}\n",
    "\n",
    "config.ftir_db_path = os.path.join(\n",
    "    base_data_path,\n",
    "    \"EC-HIPS-Aeth Comparison/Data/Original Data/Combined Database\",\n",
    "    \"spartan_ftir_hips.db\"\n",
    ")\n",
    "\n",
    "# Create enhanced setup with PKL cleaning capabilities\n",
    "setup = create_enhanced_setup(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f42c9f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📁 Loading datasets...\n",
      "📦 Setting up modular system...\n",
      "✅ Aethalometer loaders imported\n",
      "✅ Database loader imported\n",
      "✅ Plotting utilities imported\n",
      "✅ Plotting style configured\n",
      "✅ Successfully imported 5 modular components\n",
      "\n",
      "============================================================\n",
      "📁 LOADING DATASETS\n",
      "============================================================\n",
      "📁 Loading all datasets...\n",
      "\n",
      "==================================================\n",
      "📊 Loading pkl_data\n",
      "==================================================\n",
      "📁 Loading pkl_data: df_uncleaned_Jacros_API_and_OG.pkl\n",
      "Detected format: standard\n",
      "Set 'datetime_local' as DatetimeIndex for time series operations\n",
      "Converted 17 columns to JPL format\n",
      "Warning: Missing recommended columns: ['datetime_local', 'Biomass.BCc', 'Fossil.fuel.BCc']\n",
      "✅ Modular load: 1,665,156 rows × 238 columns\n",
      "📊 Method: modular\n",
      "📊 Format: jpl\n",
      "📊 Memory: 7443.05 MB\n",
      "🧮 BC columns: 30\n",
      "📈 ATN columns: 25\n",
      "📅 Time range: 2021-01-09 16:38:00 to 2025-06-26 23:18:00\n",
      "✅ pkl_data loaded successfully\n",
      "\n",
      "==================================================\n",
      "📊 Loading csv_data\n",
      "==================================================\n",
      "📁 Loading csv_data: Jacros_MA350_1-min_2022-2024_Cleaned.csv\n",
      "Set 'Time (Local)' as DatetimeIndex for time series operations\n",
      "Converted 5 columns to JPL format\n",
      "✅ Modular load: 1,095,086 rows × 77 columns\n",
      "📊 Method: modular\n",
      "📊 Format: jpl\n",
      "📊 Memory: 884.83 MB\n",
      "🧮 BC columns: 15\n",
      "📈 ATN columns: 10\n",
      "📅 Time range: 2022-04-12 12:46:01+03:00 to 2024-08-20 12:01:00+03:00\n",
      "✅ csv_data loaded successfully\n",
      "\n",
      "==================================================\n",
      "🗃️ Loading FTIR/HIPS data\n",
      "==================================================\n",
      "🗃️ Loading FTIR/HIPS data for site ETAD...\n",
      "📊 Available sites: ['ILNZ', 'ILHA', 'ZAJB', 'CAHA', 'CASH', 'AEAZ', 'AUMN', 'KRUL', 'MXMC', 'ZAPR', 'CHTS', 'ETAD', 'INDH', 'TWTA', 'USPA', 'TWKA', 'KRSE', 'PRFJ', 'BDDU', 'BIBU', 'USNO', 'IDBD', None]\n",
      "✅ Modular FTIR load: 168 samples\n",
      "📅 Date range: 2022-12-07 00:00:00 to 2024-05-12 00:00:00\n",
      "✅ FTIR/HIPS data loaded successfully\n",
      "\n",
      "📊 Loading summary: 3 datasets loaded\n",
      "\n",
      "📊 LOADING SUMMARY\n",
      "============================================================\n",
      "✅ Successfully loaded 3 datasets\n",
      "   - pkl_data: 1,665,156 rows × 238 columns\n",
      "   - csv_data: 1,095,086 rows × 77 columns\n",
      "   - ftir_hips: 168 rows × 12 columns\n",
      "============================================================\n",
      "✅ Converting datetime_local from index to column...\n",
      "📊 PKL data ready: (1665156, 239)\n",
      "📅 Date range: 2021-01-09 16:38:00 to 2025-06-26 23:18:00\n"
     ]
    }
   ],
   "source": [
    "# Load all datasets\n",
    "print(\"📁 Loading datasets...\")\n",
    "datasets = setup.load_all_data()\n",
    "\n",
    "# Get PKL data and fix datetime_local issue\n",
    "pkl_data_original = setup.get_dataset('pkl_data')\n",
    "\n",
    "# Quick fix for datetime_local issue\n",
    "if 'datetime_local' not in pkl_data_original.columns:\n",
    "    if pkl_data_original.index.name == 'datetime_local':\n",
    "        print(\"✅ Converting datetime_local from index to column...\")\n",
    "        pkl_data_original = pkl_data_original.reset_index()\n",
    "    elif hasattr(pkl_data_original.index, 'tz'):\n",
    "        print(\"✅ Creating datetime_local column from datetime index...\")\n",
    "        pkl_data_original['datetime_local'] = pkl_data_original.index\n",
    "        pkl_data_original = pkl_data_original.reset_index(drop=True)\n",
    "\n",
    "print(f\"📊 PKL data ready: {pkl_data_original.shape}\")\n",
    "print(f\"📅 Date range: {pkl_data_original['datetime_local'].min()} to {pkl_data_original['datetime_local'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a274f632",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 PKL Data Cleaning Troubleshooting\n",
      "==================================================\n",
      "✅ Data loaded: (1665156, 239)\n",
      "✅ datetime_local found in columns\n",
      "\n",
      "📊 Column availability check:\n",
      "  BC: 15 columns found\n",
      "  ATN: 10 columns found\n",
      "  Flow: 4 columns found\n",
      "  Serial number: 1 columns found\n",
      "    Examples: ['Serial number']\n",
      "  Tape position: 1 columns found\n",
      "    Examples: ['Tape position']\n",
      "\n",
      "🧹 Creating cleaner with diagnostics...\n",
      "🔍 Data Structure Diagnosis:\n",
      "------------------------------\n",
      "DataFrame shape: (1665156, 239)\n",
      "Date range: 2021-01-09 16:38:00 to 2025-06-26 23:18:00\n",
      "BC columns: 10 (e.g., ['Blue BC1', 'Blue BC2', 'Green BC1'])\n",
      "BC smoothed columns: 0 (e.g., None)\n",
      "ATN columns: 0 (e.g., None)\n",
      "Flow columns: 4 (e.g., ['Flow setpoint (mL/min)', 'Flow.total.mL.min', 'Flow1 (mL/min)'])\n",
      "\n",
      "Targeted wavelengths: ['IR', 'Blue']\n",
      "  IR: ❌ BC | ❌ BC smoothed | ❌ ATN\n",
      "  Blue: ❌ BC | ❌ BC smoothed | ❌ ATN\n",
      "------------------------------\n",
      "✅ Cleaner diagnostics completed\n",
      "\n",
      "💡 You should now be able to run the cleaning pipeline\n",
      "\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Simple Troubleshooting Cell - Run this if you get errors\n",
    "\n",
    "print(\"🔧 PKL Data Cleaning Troubleshooting\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Check data structure\n",
    "if 'pkl_data_original' in locals():\n",
    "    print(f\"✅ Data loaded: {pkl_data_original.shape}\")\n",
    "    \n",
    "    # Check datetime column\n",
    "    if 'datetime_local' in pkl_data_original.columns:\n",
    "        print(\"✅ datetime_local found in columns\")\n",
    "    elif pkl_data_original.index.name == 'datetime_local':\n",
    "        print(\"⚠️ datetime_local is in index, converting...\")\n",
    "        pkl_data_original = pkl_data_original.reset_index()\n",
    "        print(\"✅ Converted to column\")\n",
    "    else:\n",
    "        print(\"❌ No datetime_local found\")\n",
    "        time_cols = [col for col in pkl_data_original.columns if 'time' in col.lower()]\n",
    "        print(f\"Available time columns: {time_cols}\")\n",
    "    \n",
    "    # Check for required columns\n",
    "    print(\"\\n📊 Column availability check:\")\n",
    "    required_patterns = ['BC', 'ATN', 'Flow', 'Serial number', 'Tape position']\n",
    "    for pattern in required_patterns:\n",
    "        matching_cols = [col for col in pkl_data_original.columns if pattern in col]\n",
    "        print(f\"  {pattern}: {len(matching_cols)} columns found\")\n",
    "        if matching_cols and len(matching_cols) <= 3:\n",
    "            print(f\"    Examples: {matching_cols}\")\n",
    "    \n",
    "    # Create cleaner with diagnostics\n",
    "    print(\"\\n🧹 Creating cleaner with diagnostics...\")\n",
    "    cleaner_test = PKLDataCleaner(wavelengths_to_filter=['IR', 'Blue'], verbose=True)\n",
    "    \n",
    "    # Test cleaner initialization\n",
    "    try:\n",
    "        cleaner_test.diagnose_data_structure(pkl_data_original)\n",
    "        print(\"✅ Cleaner diagnostics completed\")\n",
    "        print(\"\\n💡 You should now be able to run the cleaning pipeline\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Diagnostics failed: {e}\")\n",
    "        print(\"Please check your data structure manually\")\n",
    "\n",
    "else:\n",
    "    print(\"❌ pkl_data_original not found\")\n",
    "    print(\"Please run the data loading cells first\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59e1ef71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🧹 Starting PKL data cleaning with full diagnostics...\n",
      "============================================================\n",
      "Starting PKL data cleaning pipeline...\n",
      "==================================================\n",
      "🔄 Applying DEMA smoothing...\n",
      "⚠️ Column Blue BCc not found, skipping\n",
      "⚠️ Error in DEMA smoothing for Blue: index 0 is out of bounds for axis 0 with size 0\n",
      "✅ DEMA smoothing applied for Blue\n",
      "⚠️ Column Green BCc not found, skipping\n",
      "⚠️ Error in DEMA smoothing for Green: index 0 is out of bounds for axis 0 with size 0\n",
      "⚠️ Column Red BCc not found, skipping\n",
      "⚠️ Error in DEMA smoothing for Red: index 0 is out of bounds for axis 0 with size 0\n",
      "⚠️ Column UV BCc not found, skipping\n",
      "⚠️ Error in DEMA smoothing for UV: index 0 is out of bounds for axis 0 with size 0\n",
      "⚠️ Column IR BCc not found, skipping\n",
      "⚠️ Error in DEMA smoothing for IR: index 0 is out of bounds for axis 0 with size 0\n",
      "✅ DEMA smoothing applied for IR\n",
      "🔍 Data Structure Diagnosis:\n",
      "------------------------------\n",
      "DataFrame shape: (1665156, 239)\n",
      "Date range: 2021-01-09 16:38:00 to 2025-06-26 23:18:00\n",
      "BC columns: 10 (e.g., ['Blue BC1', 'Blue BC2', 'Green BC1'])\n",
      "BC smoothed columns: 0 (e.g., None)\n",
      "ATN columns: 0 (e.g., None)\n",
      "Flow columns: 4 (e.g., ['Flow setpoint (mL/min)', 'Flow.total.mL.min', 'Flow1 (mL/min)'])\n",
      "\n",
      "Targeted wavelengths: ['IR', 'Blue']\n",
      "  IR: ❌ BC | ❌ BC smoothed | ❌ ATN\n",
      "  Blue: ❌ BC | ❌ BC smoothed | ❌ ATN\n",
      "------------------------------\n",
      "\n",
      "🧹 Starting cleaning steps...\n",
      "1934 datapoints removed due to Start up or Tape advance status\n",
      "Statuses of concern, count by device and status:\n",
      "\n",
      "MA350-0238 Flow unstable 1\n",
      "MA350-0238 Optical saturation 0\n",
      "MA350-0238 Sample timing error 0\n",
      "WF0013 Flow unstable 749\n",
      "WF0013 Optical saturation 0\n",
      "WF0013 Sample timing error 0\n",
      "Number of datapoints with invalid optics values\n",
      "AFTER dropping data with 'Optical saturation' status values: 823\n",
      "Removed 71954 datapoints for optics\n",
      "Status cleaning: Removed 74638 rows (4.48%)\n",
      "⚠️ Missing columns for IR extreme BCc cleaning: IR BCc smoothed or IR ATN1\n",
      "⚠️ Missing columns for Blue extreme BCc cleaning: Blue BCc smoothed or Blue ATN1\n",
      "Extreme BCc cleaning: Removed 0 rows (0.00%)\n",
      "❌ Cleaning failed: 'Flow total (mL/min)'\n",
      "Please run the troubleshooting cell above and check the error details\n"
     ]
    }
   ],
   "source": [
    "# Initialize cleaner with verbose diagnostics\n",
    "cleaner = PKLDataCleaner(wavelengths_to_filter=['IR', 'Blue'], verbose=True)\n",
    "\n",
    "print(\"\\n🧹 Starting PKL data cleaning with full diagnostics...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    # Apply cleaning pipeline (now includes DEMA smoothing automatically)\n",
    "    pkl_data_cleaned = cleaner.clean_pipeline(pkl_data_original.copy())\n",
    "    \n",
    "    print(\"\\n📊 Cleaning Results Summary:\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Original data points: {len(pkl_data_original):,}\")\n",
    "    print(f\"Cleaned data points: {len(pkl_data_cleaned):,}\")\n",
    "    print(f\"Total removed: {len(pkl_data_original) - len(pkl_data_cleaned):,}\")\n",
    "    print(f\"Removal percentage: {((len(pkl_data_original) - len(pkl_data_cleaned)) / len(pkl_data_original) * 100):.2f}%\")\n",
    "    \n",
    "    # Store cleaned data in setup\n",
    "    setup.datasets['pkl_data_cleaned'] = pkl_data_cleaned\n",
    "    \n",
    "    print(\"✅ PKL data cleaning completed successfully!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Cleaning failed: {e}\")\n",
    "    print(\"Please run the troubleshooting cell above and check the error details\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9911e380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Analyzing your PKL data column structure...\n",
      "🔍 Column Pattern Analysis\n",
      "==================================================\n",
      "Total columns: 239\n",
      "\n",
      "BC_related: 30 columns\n",
      "  Blue BC1\n",
      "  Blue BC2\n",
      "  Blue.BCc\n",
      "  Green BC1\n",
      "  Green BC2\n",
      "  ... and 25 more\n",
      "\n",
      "ATN_related: 25 columns\n",
      "  Blue.ATN1\n",
      "  Blue.ATN2\n",
      "  Green.ATN1\n",
      "  Green.ATN2\n",
      "  IR.ATN1\n",
      "  ... and 20 more\n",
      "\n",
      "Flow_related: 8 columns\n",
      "  Flow setpoint (mL/min)\n",
      "  Flow.total.mL.min\n",
      "  Flow1 (mL/min)\n",
      "  Flow2 (mL/min)\n",
      "  ma.flow.if1\n",
      "  ... and 3 more\n",
      "\n",
      "Temperature_related: 6 columns\n",
      "  Internal temp (C)\n",
      "  Sample temp (C)\n",
      "  Sample temp (C) - scd\n",
      "  co2.temperature\n",
      "  ma.env.iinternal.temp\n",
      "  ... and 1 more\n",
      "\n",
      "Time_related: 12 columns\n",
      "  datetime_local\n",
      "  Timebase.s\n",
      "  Timezone offset (mins)\n",
      "  dateTime\n",
      "  datetime\n",
      "  ... and 7 more\n",
      "\n",
      "Status_related: 5 columns\n",
      "  Readable status\n",
      "  Readable status - fw format\n",
      "  Status\n",
      "  lastStatus\n",
      "  statusId\n",
      "\n",
      "Other: 153 columns\n",
      "  index\n",
      "  Unnamed: 0\n",
      "  Accel X\n",
      "  Accel Y\n",
      "  Accel Z\n",
      "  ... and 148 more\n",
      "\n",
      "============================================================\n",
      "\n",
      "🗺️ Creating Column Mapping\n",
      "==================================================\n",
      "BC Columns:\n",
      "  IR BCc -> IR BC1\n",
      "  Blue BCc -> Blue BC1\n",
      "  Green BCc -> Green BC1\n",
      "  Red BCc -> Red BC1\n",
      "  UV BCc -> UV BC1\n",
      "\n",
      "ATN Columns:\n",
      "  IR ATN1 -> IR.ATN1\n",
      "  IR ATN2 -> IR.ATN2\n",
      "  Blue ATN1 -> Blue.ATN1\n",
      "  Blue ATN2 -> Blue.ATN2\n",
      "  Green ATN1 -> Green.ATN1\n",
      "  Green ATN2 -> Green.ATN2\n",
      "  Red ATN1 -> Red.ATN1\n",
      "  Red ATN2 -> Red.ATN2\n",
      "  UV ATN1 -> UV.ATN1\n",
      "  UV ATN2 -> UV.ATN2\n",
      "\n",
      "Flow Columns:\n",
      "  Flow total (mL/min) -> Flow.total.mL.min\n",
      "  Flow1 (mL/min) -> Flow1 (mL/min)\n",
      "  Flow2 (mL/min) -> Flow2 (mL/min)\n",
      "\n",
      "Temperature Columns:\n",
      "  Sample temp (C) -> Sample temp (C)\n",
      "\n",
      "Total mappings found: 19\n",
      "\n",
      "============================================================\n",
      "\n",
      "🔄 Renaming columns for cleaning compatibility...\n",
      "  IR BC1 -> IR BCc\n",
      "  Blue BC1 -> Blue BCc\n",
      "  Green BC1 -> Green BCc\n",
      "  Red BC1 -> Red BCc\n",
      "  UV BC1 -> UV BCc\n",
      "  IR.ATN1 -> IR ATN1\n",
      "  IR.ATN2 -> IR ATN2\n",
      "  Blue.ATN1 -> Blue ATN1\n",
      "  Blue.ATN2 -> Blue ATN2\n",
      "  Green.ATN1 -> Green ATN1\n",
      "  Green.ATN2 -> Green ATN2\n",
      "  Red.ATN1 -> Red ATN1\n",
      "  Red.ATN2 -> Red ATN2\n",
      "  UV.ATN1 -> UV ATN1\n",
      "  UV.ATN2 -> UV ATN2\n",
      "  Flow.total.mL.min -> Flow total (mL/min)\n",
      "  Flow1 (mL/min) -> Flow1 (mL/min)\n",
      "  Flow2 (mL/min) -> Flow2 (mL/min)\n",
      "  Sample temp (C) -> Sample temp (C)\n",
      "Renamed 19 columns\n",
      "\n",
      "✅ Column analysis complete!\n",
      "Original shape: (1665156, 239)\n",
      "Renamed shape: (1665156, 239)\n",
      "Found mappings for 19 expected columns\n",
      "\n",
      "📊 After renaming - checking for key columns:\n",
      "  IR: ✅ BCc | ✅ ATN1 | ✅ ATN2\n",
      "  Blue: ✅ BCc | ✅ ATN1 | ✅ ATN2\n",
      "  Flow: ✅ Flow total (mL/min)\n",
      "\n",
      "💡 Use 'pkl_data_renamed' for cleaning if the mappings look good!\n"
     ]
    }
   ],
   "source": [
    "# Column Mapping and Debug Tool\n",
    "# Run this to understand and fix column naming issues\n",
    "\n",
    "def analyze_column_patterns(df):\n",
    "    \"\"\"\n",
    "    Analyze column patterns in the loaded data to understand the naming convention.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Loaded DataFrame\n",
    "    \"\"\"\n",
    "    print(\"🔍 Column Pattern Analysis\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Total columns: {len(df.columns)}\")\n",
    "    \n",
    "    # Group columns by pattern\n",
    "    patterns = {\n",
    "        'BC_related': [],\n",
    "        'ATN_related': [],\n",
    "        'Flow_related': [],\n",
    "        'Temperature_related': [],\n",
    "        'Time_related': [],\n",
    "        'Status_related': [],\n",
    "        'Other': []\n",
    "    }\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_lower = col.lower()\n",
    "        if 'bc' in col_lower:\n",
    "            patterns['BC_related'].append(col)\n",
    "        elif 'atn' in col_lower:\n",
    "            patterns['ATN_related'].append(col)\n",
    "        elif 'flow' in col_lower:\n",
    "            patterns['Flow_related'].append(col)\n",
    "        elif 'temp' in col_lower or 'temperature' in col_lower:\n",
    "            patterns['Temperature_related'].append(col)\n",
    "        elif 'time' in col_lower or 'date' in col_lower:\n",
    "            patterns['Time_related'].append(col)\n",
    "        elif 'status' in col_lower or 'error' in col_lower:\n",
    "            patterns['Status_related'].append(col)\n",
    "        else:\n",
    "            patterns['Other'].append(col)\n",
    "    \n",
    "    for pattern_name, cols in patterns.items():\n",
    "        if cols:\n",
    "            print(f\"\\n{pattern_name}: {len(cols)} columns\")\n",
    "            # Show first 5 columns as examples\n",
    "            for i, col in enumerate(cols[:5]):\n",
    "                print(f\"  {col}\")\n",
    "            if len(cols) > 5:\n",
    "                print(f\"  ... and {len(cols) - 5} more\")\n",
    "    \n",
    "    return patterns\n",
    "\n",
    "def create_column_mapping(df):\n",
    "    \"\"\"\n",
    "    Create a mapping from expected column names to actual column names.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Loaded DataFrame\n",
    "        \n",
    "    Returns:\n",
    "        dict: Mapping of expected -> actual column names\n",
    "    \"\"\"\n",
    "    print(\"\\n🗺️ Creating Column Mapping\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    mapping = {}\n",
    "    reverse_mapping = {}  # actual -> expected\n",
    "    \n",
    "    # Wavelengths we're interested in\n",
    "    wavelengths = ['IR', 'Blue', 'Green', 'Red', 'UV']\n",
    "    \n",
    "    # Look for BC columns\n",
    "    print(\"BC Columns:\")\n",
    "    for wl in wavelengths:\n",
    "        # Look for different possible patterns\n",
    "        possible_patterns = [\n",
    "            f'{wl} BCc',      # Expected format\n",
    "            f'{wl} BC1',      # Alternative 1\n",
    "            f'{wl} BC2',      # Alternative 2\n",
    "            f'{wl}.BCc',      # With dot separator\n",
    "            f'{wl}.BC1',      # With dot separator\n",
    "            f'{wl}.BC2',      # With dot separator\n",
    "        ]\n",
    "        \n",
    "        for pattern in possible_patterns:\n",
    "            if pattern in df.columns:\n",
    "                if f'{wl} BCc' not in mapping:  # Prefer BCc if available\n",
    "                    mapping[f'{wl} BCc'] = pattern\n",
    "                    reverse_mapping[pattern] = f'{wl} BCc'\n",
    "                    print(f\"  {wl} BCc -> {pattern}\")\n",
    "                    break\n",
    "    \n",
    "    # Look for ATN columns\n",
    "    print(\"\\nATN Columns:\")\n",
    "    for wl in wavelengths:\n",
    "        for spot in [1, 2]:\n",
    "            expected = f'{wl} ATN{spot}'\n",
    "            possible_patterns = [\n",
    "                f'{wl} ATN{spot}',\n",
    "                f'{wl}.ATN{spot}',\n",
    "                f'{wl}_ATN{spot}',\n",
    "            ]\n",
    "            \n",
    "            for pattern in possible_patterns:\n",
    "                if pattern in df.columns:\n",
    "                    mapping[expected] = pattern\n",
    "                    reverse_mapping[pattern] = expected\n",
    "                    print(f\"  {expected} -> {pattern}\")\n",
    "                    break\n",
    "    \n",
    "    # Look for Flow columns\n",
    "    print(\"\\nFlow Columns:\")\n",
    "    flow_mappings = {\n",
    "        'Flow total (mL/min)': ['Flow.total.mL.min', 'Flow total (mL/min)', 'Flow_total_mL_min'],\n",
    "        'Flow1 (mL/min)': ['Flow1 (mL/min)', 'Flow1.mL.min', 'Flow1_mL_min'],\n",
    "        'Flow2 (mL/min)': ['Flow2 (mL/min)', 'Flow2.mL.min', 'Flow2_mL_min'],\n",
    "    }\n",
    "    \n",
    "    for expected, possibles in flow_mappings.items():\n",
    "        for possible in possibles:\n",
    "            if possible in df.columns:\n",
    "                mapping[expected] = possible\n",
    "                reverse_mapping[possible] = expected\n",
    "                print(f\"  {expected} -> {possible}\")\n",
    "                break\n",
    "    \n",
    "    # Look for Temperature columns\n",
    "    print(\"\\nTemperature Columns:\")\n",
    "    temp_mappings = {\n",
    "        'Sample temp (C)': ['Sample temp (C)', 'Sample.temp.C', 'Sample_temp_C', 'Temperature'],\n",
    "        'delta Sample temp (C)': ['delta Sample temp (C)', 'delta.Sample.temp.C', 'delta_Sample_temp_C']\n",
    "    }\n",
    "    \n",
    "    for expected, possibles in temp_mappings.items():\n",
    "        for possible in possibles:\n",
    "            if possible in df.columns:\n",
    "                mapping[expected] = possible\n",
    "                reverse_mapping[possible] = expected\n",
    "                print(f\"  {expected} -> {possible}\")\n",
    "                break\n",
    "    \n",
    "    print(f\"\\nTotal mappings found: {len(mapping)}\")\n",
    "    return mapping, reverse_mapping\n",
    "\n",
    "def rename_columns_for_cleaning(df, mapping):\n",
    "    \"\"\"\n",
    "    Rename columns in DataFrame to match expected naming convention.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame to rename\n",
    "        mapping (dict): Column mapping (expected -> actual)\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with renamed columns\n",
    "    \"\"\"\n",
    "    print(\"\\n🔄 Renaming columns for cleaning compatibility...\")\n",
    "    \n",
    "    # Create reverse mapping (actual -> expected)\n",
    "    reverse_mapping = {v: k for k, v in mapping.items()}\n",
    "    \n",
    "    # Rename columns\n",
    "    df_renamed = df.rename(columns=reverse_mapping)\n",
    "    \n",
    "    # Report what was renamed\n",
    "    renamed_count = 0\n",
    "    for actual, expected in reverse_mapping.items():\n",
    "        if actual in df.columns:\n",
    "            print(f\"  {actual} -> {expected}\")\n",
    "            renamed_count += 1\n",
    "    \n",
    "    print(f\"Renamed {renamed_count} columns\")\n",
    "    return df_renamed\n",
    "\n",
    "# Run the analysis\n",
    "if 'pkl_data_original' in locals():\n",
    "    print(\"🔍 Analyzing your PKL data column structure...\")\n",
    "    patterns = analyze_column_patterns(pkl_data_original)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    mapping, reverse_mapping = create_column_mapping(pkl_data_original)\n",
    "    \n",
    "    # Create a version with renamed columns\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    pkl_data_renamed = rename_columns_for_cleaning(pkl_data_original, mapping)\n",
    "    \n",
    "    print(f\"\\n✅ Column analysis complete!\")\n",
    "    print(f\"Original shape: {pkl_data_original.shape}\")\n",
    "    print(f\"Renamed shape: {pkl_data_renamed.shape}\")\n",
    "    print(f\"Found mappings for {len(mapping)} expected columns\")\n",
    "    \n",
    "    # Quick test to see what we have now\n",
    "    print(f\"\\n📊 After renaming - checking for key columns:\")\n",
    "    for wl in ['IR', 'Blue']:\n",
    "        bc_col = f'{wl} BCc'\n",
    "        atn1_col = f'{wl} ATN1'\n",
    "        atn2_col = f'{wl} ATN2'\n",
    "        \n",
    "        bc_status = \"✅\" if bc_col in pkl_data_renamed.columns else \"❌\"\n",
    "        atn1_status = \"✅\" if atn1_col in pkl_data_renamed.columns else \"❌\"\n",
    "        atn2_status = \"✅\" if atn2_col in pkl_data_renamed.columns else \"❌\"\n",
    "        \n",
    "        print(f\"  {wl}: {bc_status} BCc | {atn1_status} ATN1 | {atn2_status} ATN2\")\n",
    "    \n",
    "    flow_col = 'Flow total (mL/min)'\n",
    "    flow_status = \"✅\" if flow_col in pkl_data_renamed.columns else \"❌\"\n",
    "    print(f\"  Flow: {flow_status} Flow total (mL/min)\")\n",
    "    \n",
    "    print(\"\\n💡 Use 'pkl_data_renamed' for cleaning if the mappings look good!\")\n",
    "    \n",
    "else:\n",
    "    print(\"❌ pkl_data_original not found. Please load your data first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36f0731f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Using renamed columns for PKL data cleaning\n",
      "============================================================\n",
      "✅ Using pkl_data_renamed with shape: (1665156, 239)\n",
      "\n",
      "🔧 Applying additional preprocessing...\n",
      "✅ Filtered to 2022+, shape now: (1627058, 239)\n",
      "\n",
      "🧹 Initializing cleaner with renamed data...\n",
      "\n",
      "🧹 Starting cleaning pipeline...\n",
      "Starting PKL data cleaning pipeline...\n",
      "==================================================\n",
      "🔄 Applying DEMA smoothing...\n",
      "⚠️ Column Blue BC1 not found, skipping\n",
      "⚠️ Error in DEMA smoothing for Blue: index 0 is out of bounds for axis 0 with size 0\n",
      "✅ DEMA smoothing applied for Blue\n",
      "⚠️ Column Green BC1 not found, skipping\n",
      "⚠️ Error in DEMA smoothing for Green: index 0 is out of bounds for axis 0 with size 0\n",
      "⚠️ Column Red BC1 not found, skipping\n",
      "⚠️ Error in DEMA smoothing for Red: index 0 is out of bounds for axis 0 with size 0\n",
      "⚠️ Column UV BC1 not found, skipping\n",
      "⚠️ Error in DEMA smoothing for UV: index 0 is out of bounds for axis 0 with size 0\n",
      "⚠️ Column IR BC1 not found, skipping\n",
      "⚠️ Error in DEMA smoothing for IR: index 0 is out of bounds for axis 0 with size 0\n",
      "✅ DEMA smoothing applied for IR\n",
      "🔍 Data Structure Diagnosis:\n",
      "------------------------------\n",
      "DataFrame shape: (1627058, 239)\n",
      "Date range: 2022-04-12 09:12:00 to 2025-06-26 23:18:00\n",
      "BC columns: 10 (e.g., ['Blue BCc', 'Blue BC2', 'Green BCc'])\n",
      "BC smoothed columns: 0 (e.g., None)\n",
      "ATN columns: 10 (e.g., ['Blue ATN1', 'Blue ATN2', 'Green ATN1'])\n",
      "Flow columns: 4 (e.g., ['Flow setpoint (mL/min)', 'Flow total (mL/min)', 'Flow1 (mL/min)'])\n",
      "\n",
      "Targeted wavelengths: ['IR', 'Blue']\n",
      "  IR: ✅ BC | ❌ BC smoothed | ✅ ATN\n",
      "  Blue: ✅ BC | ❌ BC smoothed | ✅ ATN\n",
      "------------------------------\n",
      "\n",
      "🧹 Starting cleaning steps...\n",
      "1919 datapoints removed due to Start up or Tape advance status\n",
      "Statuses of concern, count by device and status:\n",
      "\n",
      "MA350-0238 Flow unstable 750\n",
      "MA350-0238 Optical saturation 0\n",
      "MA350-0238 Sample timing error 0\n",
      "Number of datapoints with invalid optics values\n",
      "AFTER dropping data with 'Optical saturation' status values: 802\n",
      "Removed 56128 datapoints for optics\n",
      "Status cleaning: Removed 58797 rows (3.61%)\n",
      "⚠️ Missing columns for IR extreme BCc cleaning: IR BCc smoothed or IR ATN1\n",
      "⚠️ Missing columns for Blue extreme BCc cleaning: Blue BCc smoothed or Blue ATN1\n",
      "Extreme BCc cleaning: Removed 0 rows (0.00%)\n",
      "Flow range cleaning: Removed 0 rows (0.00%)\n",
      "Abnormal flow ratio: Removed 37659 rows (2.40%)\n",
      "⚠️ Missing columns for IR leak ratio cleaning: ['delta IR ATN1 rolling mean', 'delta IR ATN2 rolling mean']\n",
      "⚠️ Missing columns for Blue leak ratio cleaning: ['delta Blue ATN1 rolling mean', 'delta Blue ATN2 rolling mean']\n",
      "Leak ratio cleaning: Removed 0 rows (0.00%)\n",
      "❌ Cleaning failed: can't multiply sequence by non-int of type 'float'\n",
      "Error type: TypeError\n",
      "\n",
      "🔧 Trying alternative approach...\n",
      "Full error traceback:\n",
      "\n",
      "🔧 Testing DEMA smoothing only...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/ahzs645/Github/aethmodular/.venv/lib/python3.13/site-packages/pandas/core/ops/array_ops.py\", line 218, in _na_arithmetic_op\n",
      "    result = func(left, right)\n",
      "  File \"/Users/ahzs645/Github/aethmodular/.venv/lib/python3.13/site-packages/pandas/core/computation/expressions.py\", line 242, in evaluate\n",
      "    return _evaluate(op, op_str, a, b)  # type: ignore[misc]\n",
      "  File \"/Users/ahzs645/Github/aethmodular/.venv/lib/python3.13/site-packages/pandas/core/computation/expressions.py\", line 73, in _evaluate_standard\n",
      "    return op(a, b)\n",
      "TypeError: can't multiply sequence by non-int of type 'float'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/5y/nymdg1zj29dglz6b4sf4v02c0000gn/T/ipykernel_19051/1510792257.py\", line 42, in <module>\n",
      "    pkl_data_cleaned = cleaner.clean_pipeline(df_for_cleaning)\n",
      "  File \"/Users/ahzs645/Github/aethmodular/src/data/qc/pkl_cleaning.py\", line 474, in clean_pipeline\n",
      "    df_cleaned = self.clean_bcc_denominator(df_cleaned)\n",
      "  File \"/Users/ahzs645/Github/aethmodular/src/data/qc/pkl_cleaning.py\", line 225, in clean_bcc_denominator\n",
      "    df_cleaned[f'{wl} BCc denominator'] = 1 - df_cleaned[f'{wl} K'] * df_cleaned[f'{wl} ATN1']\n",
      "                                              ~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  File \"/Users/ahzs645/Github/aethmodular/.venv/lib/python3.13/site-packages/pandas/core/ops/common.py\", line 76, in new_method\n",
      "    return method(self, other)\n",
      "  File \"/Users/ahzs645/Github/aethmodular/.venv/lib/python3.13/site-packages/pandas/core/arraylike.py\", line 202, in __mul__\n",
      "    return self._arith_method(other, operator.mul)\n",
      "           ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/ahzs645/Github/aethmodular/.venv/lib/python3.13/site-packages/pandas/core/series.py\", line 6146, in _arith_method\n",
      "    return base.IndexOpsMixin._arith_method(self, other, op)\n",
      "           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/ahzs645/Github/aethmodular/.venv/lib/python3.13/site-packages/pandas/core/base.py\", line 1391, in _arith_method\n",
      "    result = ops.arithmetic_op(lvalues, rvalues, op)\n",
      "  File \"/Users/ahzs645/Github/aethmodular/.venv/lib/python3.13/site-packages/pandas/core/ops/array_ops.py\", line 283, in arithmetic_op\n",
      "    res_values = _na_arithmetic_op(left, right, op)  # type: ignore[arg-type]\n",
      "  File \"/Users/ahzs645/Github/aethmodular/.venv/lib/python3.13/site-packages/pandas/core/ops/array_ops.py\", line 227, in _na_arithmetic_op\n",
      "    result = _masked_arith_op(left, right, op)\n",
      "  File \"/Users/ahzs645/Github/aethmodular/.venv/lib/python3.13/site-packages/pandas/core/ops/array_ops.py\", line 163, in _masked_arith_op\n",
      "    result[mask] = op(xrav[mask], yrav[mask])\n",
      "                   ~~^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: can't multiply sequence by non-int of type 'float'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Column IR BC1 not found, skipping\n",
      "⚠️ Error in DEMA smoothing for IR: index 0 is out of bounds for axis 0 with size 0\n",
      "✅ DEMA test successful, shape: (1627058, 239)\n",
      "IR smoothed columns created: []\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Working Solution Using Renamed Columns\n",
    "# Use this after running the column mapping debug tool\n",
    "\n",
    "print(\"🚀 Using renamed columns for PKL data cleaning\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# First, let's verify we have the renamed data\n",
    "if 'pkl_data_renamed' not in locals():\n",
    "    print(\"❌ pkl_data_renamed not found. Please run the column mapping debug tool first.\")\n",
    "else:\n",
    "    print(f\"✅ Using pkl_data_renamed with shape: {pkl_data_renamed.shape}\")\n",
    "    \n",
    "    # Let's also add any missing processing that might be needed\n",
    "    df_for_cleaning = pkl_data_renamed.copy()\n",
    "    \n",
    "    # Ensure we have proper data types and any missing processing\n",
    "    print(\"\\n🔧 Applying additional preprocessing...\")\n",
    "    \n",
    "    # Add Session ID if missing (from original script)\n",
    "    if 'Session ID' not in df_for_cleaning.columns and 'Tape position' in df_for_cleaning.columns:\n",
    "        position_change = df_for_cleaning['Tape position'] != df_for_cleaning['Tape position'].shift()\n",
    "        df_for_cleaning['Session ID'] = position_change.cumsum()\n",
    "        print(\"✅ Added Session ID\")\n",
    "    \n",
    "    # Set serial number (from original script)\n",
    "    df_for_cleaning['Serial number'] = \"MA350-0238\"\n",
    "    \n",
    "    # Filter to 2022 and later (from original script)\n",
    "    if 'datetime_local' in df_for_cleaning.columns:\n",
    "        df_for_cleaning = df_for_cleaning.loc[df_for_cleaning['datetime_local'].dt.year >= 2022]\n",
    "        print(f\"✅ Filtered to 2022+, shape now: {df_for_cleaning.shape}\")\n",
    "    \n",
    "    # Now try cleaning with the properly named columns\n",
    "    print(\"\\n🧹 Initializing cleaner with renamed data...\")\n",
    "    \n",
    "    try:\n",
    "        # Initialize cleaner\n",
    "        cleaner = PKLDataCleaner(wavelengths_to_filter=['IR', 'Blue'], verbose=True)\n",
    "        \n",
    "        # Apply cleaning pipeline\n",
    "        print(\"\\n🧹 Starting cleaning pipeline...\")\n",
    "        pkl_data_cleaned = cleaner.clean_pipeline(df_for_cleaning)\n",
    "        \n",
    "        print(\"\\n📊 Cleaning Results Summary:\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"Original data points: {len(df_for_cleaning):,}\")\n",
    "        print(f\"Cleaned data points: {len(pkl_data_cleaned):,}\")\n",
    "        print(f\"Total removed: {len(df_for_cleaning) - len(pkl_data_cleaned):,}\")\n",
    "        removal_pct = ((len(df_for_cleaning) - len(pkl_data_cleaned)) / len(df_for_cleaning) * 100)\n",
    "        print(f\"Removal percentage: {removal_pct:.2f}%\")\n",
    "        \n",
    "        print(\"\\n✅ PKL data cleaning completed successfully!\")\n",
    "        print(\"🎉 Your cleaned data is now available as 'pkl_data_cleaned'\")\n",
    "        \n",
    "        # Quick verification of cleaned data\n",
    "        print(f\"\\n📊 Cleaned data verification:\")\n",
    "        print(f\"Shape: {pkl_data_cleaned.shape}\")\n",
    "        if 'datetime_local' in pkl_data_cleaned.columns:\n",
    "            print(f\"Date range: {pkl_data_cleaned['datetime_local'].min()} to {pkl_data_cleaned['datetime_local'].max()}\")\n",
    "        \n",
    "        # Check for smoothed columns\n",
    "        smoothed_cols = [col for col in pkl_data_cleaned.columns if 'smoothed' in col]\n",
    "        print(f\"Smoothed columns created: {len(smoothed_cols)}\")\n",
    "        if smoothed_cols:\n",
    "            print(f\"Examples: {smoothed_cols[:3]}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Cleaning failed: {e}\")\n",
    "        print(f\"Error type: {type(e).__name__}\")\n",
    "        \n",
    "        # Let's try a more targeted approach\n",
    "        print(\"\\n🔧 Trying alternative approach...\")\n",
    "        \n",
    "        # Check what specific error we're getting\n",
    "        import traceback\n",
    "        print(\"Full error traceback:\")\n",
    "        traceback.print_exc()\n",
    "        \n",
    "        # Try just the DEMA smoothing step to see if that works\n",
    "        print(\"\\n🔧 Testing DEMA smoothing only...\")\n",
    "        try:\n",
    "            test_df = df_for_cleaning.copy()\n",
    "            \n",
    "            # Try DEMA smoothing for just IR first\n",
    "            cleaner_test = PKLDataCleaner(wavelengths_to_filter=['IR'], verbose=True)\n",
    "            test_df_smoothed = cleaner_test.dema_bc_and_atn(test_df, DEMA_span_min=10, wl='IR')\n",
    "            \n",
    "            print(f\"✅ DEMA test successful, shape: {test_df_smoothed.shape}\")\n",
    "            \n",
    "            # Check if smoothed columns were created\n",
    "            ir_smoothed_cols = [col for col in test_df_smoothed.columns if 'IR' in col and 'smoothed' in col]\n",
    "            print(f\"IR smoothed columns created: {ir_smoothed_cols}\")\n",
    "            \n",
    "        except Exception as e2:\n",
    "            print(f\"❌ DEMA test also failed: {e2}\")\n",
    "            \n",
    "            # If DEMA fails, we might need to use the external calibration module differently\n",
    "            print(\"\\n💡 The issue might be with the calibration module import.\")\n",
    "            print(\"Let's check if the calibration module is properly accessible...\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "89cd4ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'pkl_data_cleaned' in locals() and len(pkl_data_cleaned) > 0:\n",
    "    # Use the enhanced setup's built-in cleaning method for comparison/validation\n",
    "    print(\"\\n🔄 Using enhanced setup integration...\")\n",
    "    \n",
    "    try:\n",
    "        # This method provides additional integration features\n",
    "        pkl_alternative = setup.clean_pkl_dataset(\n",
    "            dataset_name='pkl_data',\n",
    "            wavelengths_to_filter=['IR', 'Blue'],\n",
    "            store_as='pkl_data_cleaned_setup',\n",
    "            run_quality_assessment=True\n",
    "        )\n",
    "        \n",
    "        # Print enhanced summary\n",
    "        setup.print_enhanced_summary()\n",
    "        \n",
    "        # Access your cleaned data\n",
    "        print(f\"\\n📊 Available datasets: {list(setup.datasets.keys())}\")\n",
    "        \n",
    "        # Get BC data for specific wavelength\n",
    "        red_bc_cleaned = setup.get_bc_data_for_wavelength('pkl_data_cleaned', 'Red')\n",
    "        \n",
    "        # Get excellent quality periods\n",
    "        excellent_periods = setup.get_excellent_periods('pkl_data_cleaned')\n",
    "        \n",
    "        # Access FTIR data for merging\n",
    "        ftir_data = setup.get_ftir_data()\n",
    "        \n",
    "        print(\"\\n✅ Ready for analysis with cleaned PKL data!\")\n",
    "        print(f\"🔬 Red BC data shape: {red_bc_cleaned.shape if red_bc_cleaned is not None else 'Not available'}\")\n",
    "        print(f\"⭐ Excellent periods: {len(excellent_periods) if excellent_periods is not None else 'Not available'}\")\n",
    "        print(f\"🔬 FTIR data shape: {ftir_data.shape if ftir_data is not None else 'Not available'}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Enhanced setup integration had issues: {e}\")\n",
    "        print(\"But your direct cleaning method worked, so you can continue with pkl_data_cleaned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "47ccf5f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ No cleaned data available to export\n",
      "Please check the error messages above and run the troubleshooting cell\n"
     ]
    }
   ],
   "source": [
    "if 'pkl_data_cleaned' in locals() and len(pkl_data_cleaned) > 0:\n",
    "    # Export cleaned data\n",
    "    pkl_data_cleaned.to_csv('pkl_data_cleaned_simple.csv', index=False)\n",
    "    pkl_data_cleaned.to_pickle('pkl_data_cleaned_simple.pkl')\n",
    "    \n",
    "    print(\"\\n💾 Cleaned data exported:\")\n",
    "    print(\"- pkl_data_cleaned_simple.csv\")\n",
    "    print(\"- pkl_data_cleaned_simple.pkl\")\n",
    "    \n",
    "    print(f\"\\n🎉 PKL Data Cleaning Complete!\")\n",
    "    print(f\"📊 Final shape: {pkl_data_cleaned.shape}\")\n",
    "    print(f\"📅 Date range: {pkl_data_cleaned['datetime_local'].min()} to {pkl_data_cleaned['datetime_local'].max()}\")\n",
    "    print(\"🚀 Ready for further analysis!\")\n",
    "    \n",
    "else:\n",
    "    print(\"❌ No cleaned data available to export\")\n",
    "    print(\"Please check the error messages above and run the troubleshooting cell\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "deb8938f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Step-by-step cleaning analysis...\n",
      "============================================================\n",
      "Starting with: 1,665,156 data points\n",
      "1934 datapoints removed due to Start up or Tape advance status\n",
      "Statuses of concern, count by device and status:\n",
      "\n",
      "MA350-0238 Flow unstable 1\n",
      "MA350-0238 Optical saturation 0\n",
      "MA350-0238 Sample timing error 0\n",
      "WF0013 Flow unstable 749\n",
      "WF0013 Optical saturation 0\n",
      "WF0013 Sample timing error 0\n",
      "Number of datapoints with invalid optics values\n",
      "AFTER dropping data with 'Optical saturation' status values: 823\n",
      "Removed 71954 datapoints for optics\n",
      "Status cleaning: Removed 74638 rows (4.48%)\n",
      "⚠️ Missing columns for IR extreme BCc cleaning: IR BCc smoothed or IR ATN1\n",
      "⚠️ Missing columns for Blue extreme BCc cleaning: Blue BCc smoothed or Blue ATN1\n",
      "Extreme BCc cleaning: Removed 0 rows (0.00%)\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Flow total (mL/min)'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Github/aethmodular/.venv/lib/python3.13/site-packages/pandas/core/indexes/base.py:3812\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3811\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3812\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:167\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:196\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7096\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'Flow total (mL/min)'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     13\u001b[39m df_step = cleaner.clean_extreme_bcc(df_step)\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# Step 3: Flow range cleaning\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m df_step = \u001b[43mcleaner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mclean_flow_range\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_step\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# Step 4: Flow ratio cleaning\u001b[39;00m\n\u001b[32m     19\u001b[39m df_step = cleaner.clean_flow_ratio(df_step)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Github/aethmodular/src/data/qc/pkl_cleaning.py:183\u001b[39m, in \u001b[36mPKLDataCleaner.clean_flow_range\u001b[39m\u001b[34m(self, df, flow_threshold, setpoint)\u001b[39m\n\u001b[32m    180\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mclean_flow_range\u001b[39m(\u001b[38;5;28mself\u001b[39m, df, flow_threshold=\u001b[32m0.1\u001b[39m, setpoint=\u001b[32m100\u001b[39m):\n\u001b[32m    181\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Clean data based on flow range violations.\"\"\"\u001b[39;00m\n\u001b[32m    182\u001b[39m     df_cleaned = df.loc[\n\u001b[32m--> \u001b[39m\u001b[32m183\u001b[39m         (\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mFlow total (mL/min)\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m <= setpoint * (\u001b[32m1\u001b[39m + flow_threshold)) &\n\u001b[32m    184\u001b[39m         (df[\u001b[33m'\u001b[39m\u001b[33mFlow total (mL/min)\u001b[39m\u001b[33m'\u001b[39m] >= setpoint * (\u001b[32m1\u001b[39m - flow_threshold))\n\u001b[32m    185\u001b[39m     ]\n\u001b[32m    186\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.report_removal(df, df_cleaned, \u001b[33m\"\u001b[39m\u001b[33mFlow range cleaning\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Github/aethmodular/.venv/lib/python3.13/site-packages/pandas/core/frame.py:4107\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4105\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4106\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4107\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4108\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4109\u001b[39m     indexer = [indexer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Github/aethmodular/.venv/lib/python3.13/site-packages/pandas/core/indexes/base.py:3819\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3814\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3815\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3816\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3817\u001b[39m     ):\n\u001b[32m   3818\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3819\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3820\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3821\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3822\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3823\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3824\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'Flow total (mL/min)'"
     ]
    }
   ],
   "source": [
    "# For detailed analysis, you can run individual cleaning steps\n",
    "print(\"\\n🔍 Step-by-step cleaning analysis...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Start with original data\n",
    "df_step = pkl_data_original.copy()\n",
    "print(f\"Starting with: {len(df_step):,} data points\")\n",
    "\n",
    "# Step 1: Status cleaning\n",
    "df_step = cleaner.clean_by_status(df_step)\n",
    "\n",
    "# Step 2: Extreme BCc cleaning  \n",
    "df_step = cleaner.clean_extreme_bcc(df_step)\n",
    "\n",
    "# Step 3: Flow range cleaning\n",
    "df_step = cleaner.clean_flow_range(df_step)\n",
    "\n",
    "# Step 4: Flow ratio cleaning\n",
    "df_step = cleaner.clean_flow_ratio(df_step)\n",
    "\n",
    "# Step 5: Leak ratio cleaning\n",
    "df_step = cleaner.clean_leak_ratio(df_step)\n",
    "\n",
    "# Step 6: BCc denominator cleaning\n",
    "df_step = cleaner.clean_bcc_denominator(df_step)\n",
    "\n",
    "# Step 7: Temperature change cleaning\n",
    "df_step = cleaner.clean_temperature_change(df_step)\n",
    "\n",
    "# Step 8: Roughness-based cleaning\n",
    "df_step = cleaner.add_roughness_columns(df_step)\n",
    "df_step, roughness_stds = cleaner.flag_high_roughness_periods(df_step)\n",
    "\n",
    "print(f\"Final cleaned data: {len(df_step):,} data points\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b323a43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the cleaning results\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Plot 1: Data availability over time\n",
    "pkl_data_cleaned.set_index('datetime_local').resample('D').size().plot(ax=axes[0,0])\n",
    "axes[0,0].set_title('Daily Data Availability After Cleaning')\n",
    "axes[0,0].set_ylabel('Data Points per Day')\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: BC values for selected wavelength\n",
    "wavelength = config.wavelength\n",
    "if f'{wavelength} BCc smoothed' in pkl_data_cleaned.columns:\n",
    "    pkl_data_cleaned[f'{wavelength} BCc smoothed'].hist(bins=50, alpha=0.7, ax=axes[0,1])\n",
    "    axes[0,1].set_title(f'{wavelength} BCc Values (Cleaned)')\n",
    "    axes[0,1].set_xlabel('BCc (µg/m³)')\n",
    "    axes[0,1].set_ylabel('Frequency')\n",
    "    axes[0,1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Temperature data\n",
    "if 'Sample temp (C)' in pkl_data_cleaned.columns:\n",
    "    pkl_data_cleaned['Sample temp (C)'].plot(ax=axes[1,0], alpha=0.7)\n",
    "    axes[1,0].set_title('Sample Temperature Over Time')\n",
    "    axes[1,0].set_ylabel('Temperature (°C)')\n",
    "    axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Flow data\n",
    "if 'Flow total (mL/min)' in pkl_data_cleaned.columns:\n",
    "    pkl_data_cleaned['Flow total (mL/min)'].hist(bins=50, alpha=0.7, ax=axes[1,1])\n",
    "    axes[1,1].set_title('Flow Rate Distribution')\n",
    "    axes[1,1].set_xlabel('Flow Rate (mL/min)')\n",
    "    axes[1,1].set_ylabel('Frequency')\n",
    "    axes[1,1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4136f154",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the cleaned data\n",
    "output_path = \"pkl_data_cleaned.csv\"\n",
    "pkl_data_cleaned.to_csv(output_path, index=False)\n",
    "print(f\"✅ Cleaned data exported to: {output_path}\")\n",
    "\n",
    "# Also save as pickle for faster loading\n",
    "pickle_output_path = \"pkl_data_cleaned.pkl\"\n",
    "pkl_data_cleaned.to_pickle(pickle_output_path)\n",
    "print(f\"✅ Cleaned data exported to: {pickle_output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3272c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the cleaned data with the existing quality assessment system\n",
    "print(\"\\n🔍 Assessing quality of cleaned data...\")\n",
    "setup.datasets['pkl_data_cleaned'] = pkl_data_cleaned\n",
    "\n",
    "# Run quality assessment on cleaned data\n",
    "quality_results = setup.assess_data_quality()\n",
    "\n",
    "# Get excellent quality periods from cleaned data\n",
    "excellent_periods = setup.get_excellent_periods('pkl_data_cleaned')\n",
    "print(f\"Excellent quality periods found: {len(excellent_periods)}\")\n",
    "\n",
    "# Get BC data for the configured wavelength from cleaned data\n",
    "wavelength_bc = setup.get_bc_data_for_wavelength('pkl_data_cleaned', config.wavelength)\n",
    "if wavelength_bc is not None:\n",
    "    print(f\"{config.wavelength} BC data shape: {wavelength_bc.shape}\")\n",
    "else:\n",
    "    print(f\"No {config.wavelength} BC data available in cleaned dataset\")\n",
    "\n",
    "# Print final summary\n",
    "print(\"\\n🎉 PKL Data Cleaning Complete!\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"📊 Original data: {len(pkl_data_original):,} points\")\n",
    "print(f\"🧹 Cleaned data: {len(pkl_data_cleaned):,} points\")\n",
    "print(f\"🗑️ Removed: {len(pkl_data_original) - len(pkl_data_cleaned):,} points ({((len(pkl_data_original) - len(pkl_data_cleaned)) / len(pkl_data_original) * 100):.2f}%)\")\n",
    "print(f\"📅 Date range: {pkl_data_cleaned['datetime_local'].min()} to {pkl_data_cleaned['datetime_local'].max()}\")\n",
    "print(f\"🌈 Wavelengths processed: {cleaner.wls_to_filter}\")\n",
    "print(f\"💾 Cleaned data saved to: {output_path} and {pickle_output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940b338f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of using custom cleaning parameters\n",
    "print(\"\\n🛠️ Advanced Usage: Custom Cleaning Parameters\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Initialize cleaner with custom parameters\n",
    "custom_cleaner = PKLDataCleaner(\n",
    "    wavelengths_to_filter=['IR', 'Blue', 'Red'],  # More wavelengths\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Apply individual cleaning steps with custom parameters\n",
    "df_custom = pkl_data_original.copy()\n",
    "\n",
    "# Custom flow range cleaning with tighter tolerance\n",
    "df_custom = custom_cleaner.clean_flow_range(df_custom, flow_threshold=0.05, setpoint=100)\n",
    "\n",
    "# Custom temperature change cleaning (this would need the method to accept parameters)\n",
    "# df_custom = custom_cleaner.clean_temperature_change(df_custom)\n",
    "\n",
    "# Custom roughness cleaning with different z-threshold\n",
    "df_custom = custom_cleaner.add_roughness_columns(df_custom)\n",
    "df_custom, _ = custom_cleaner.flag_high_roughness_periods(\n",
    "    df_custom, \n",
    "    z_threshold=1.5,  # More sensitive\n",
    "    min_len=5,        # Shorter periods\n",
    "    min_frac_high=0.5 # Lower fraction threshold\n",
    ")\n",
    "\n",
    "print(f\"Custom cleaning result: {len(df_custom):,} data points remaining\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d8f5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative approach: Load and clean data in one step using the convenience function\n",
    "print(\"\\n🚀 Alternative: Load and Clean in One Step\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# If you want to load directly from directory and clean\n",
    "# (Adjust the directory path to your JPL_aeth directory)\n",
    "try:\n",
    "    # This function loads from directory and applies all cleaning steps\n",
    "    df_loaded_and_cleaned = load_and_clean_pkl_data(\n",
    "        directory_path=\"../JPL_aeth/\",  # Adjust path as needed\n",
    "        wavelengths_to_filter=['IR', 'Blue'],\n",
    "        verbose=True,\n",
    "        summary=True\n",
    "    )\n",
    "    print(f\"Loaded and cleaned data shape: {df_loaded_and_cleaned.shape}\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"Directory not found. Using already loaded data instead.\")\n",
    "    df_loaded_and_cleaned = pkl_data_cleaned\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error in load_and_clean_pkl_data: {e}\")\n",
    "    print(\"Using already cleaned data instead.\")\n",
    "    df_loaded_and_cleaned = pkl_data_cleaned\n",
    "\n",
    "print(\"\\n✅ PKL Data Cleaning Pipeline Complete!\")\n",
    "print(\"Your cleaned data is now available in the following variables:\")\n",
    "print(\"- pkl_data_cleaned: Cleaned version of your original pkl data\")\n",
    "print(\"- df_loaded_and_cleaned: Alternative cleaned dataset\")\n",
    "print(\"- Use these datasets for further analysis, merging with FTIR data, etc.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
