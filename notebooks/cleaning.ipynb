{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e9dffb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Calibration module loaded successfully\n",
      "âœ… Advanced plotting style configured\n",
      "ğŸš€ Aethalometer-FTIR/HIPS Pipeline with Simplified Setup\n",
      "============================================================\n",
      "ğŸ“Š Configuration Summary:\n",
      "   Site: ETAD\n",
      "   Wavelength: Red\n",
      "   Output format: jpl\n",
      "   Quality threshold: 10 minutes\n",
      "   Output directory: outputs\n",
      "\n",
      "ğŸ“ File paths:\n",
      "   pkl_data: âœ… df_uncleaned_Jacros_API_and_OG.pkl\n",
      "   csv_data: âœ… Jacros_MA350_1-min_2022-2024_Cleaned.csv\n",
      "   FTIR DB: âœ… spartan_ftir_hips.db\n",
      "ğŸ§¹ Enhanced setup with PKL cleaning capabilities loaded\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.insert(0, os.path.join(os.path.dirname(os.getcwd()), 'src'))\n",
    "\n",
    "# Import enhanced setup with PKL cleaning capabilities\n",
    "from notebook_utils.pkl_cleaning_integration import create_enhanced_setup\n",
    "from config.notebook_config import NotebookConfig\n",
    "from data.qc.pkl_cleaning import PKLDataCleaner\n",
    "\n",
    "# ADD THIS: Import calibration module\n",
    "import importlib.util\n",
    "try:\n",
    "    # Try to import calibration module (adjust path as needed)\n",
    "    module_path = os.path.join(os.path.dirname(os.getcwd()), \"src\", \"external\", \"calibration.py\")\n",
    "    if not os.path.exists(module_path):\n",
    "        # Alternative paths to try\n",
    "        alt_paths = [\n",
    "            os.path.join(os.getcwd(), \"calibration.py\"),\n",
    "            os.path.join(os.path.dirname(os.getcwd()), \"calibration.py\"),\n",
    "            \"calibration.py\"\n",
    "        ]\n",
    "        for alt_path in alt_paths:\n",
    "            if os.path.exists(alt_path):\n",
    "                module_path = alt_path\n",
    "                break\n",
    "    \n",
    "    if os.path.exists(module_path):\n",
    "        module_name = \"calibration\"\n",
    "        spec = importlib.util.spec_from_file_location(module_name, module_path)\n",
    "        module = importlib.util.module_from_spec(spec)\n",
    "        sys.modules[module_name] = module\n",
    "        spec.loader.exec_module(module)\n",
    "        import calibration\n",
    "        print(\"âœ… Calibration module loaded successfully\")\n",
    "        HAS_CALIBRATION = True\n",
    "    else:\n",
    "        print(\"âš ï¸ Calibration module not found, will use alternative preprocessing\")\n",
    "        HAS_CALIBRATION = False\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ Could not import calibration module: {e}\")\n",
    "    HAS_CALIBRATION = False\n",
    "\n",
    "# Your existing configuration\n",
    "config = NotebookConfig(\n",
    "    site_code='ETAD',\n",
    "    wavelength='Red',\n",
    "    quality_threshold=10,\n",
    "    output_format='jpl',\n",
    "    min_samples_for_analysis=30,\n",
    "    confidence_level=0.95,\n",
    "    outlier_threshold=3.0,\n",
    "    figure_size=(12, 8),\n",
    "    font_size=10,\n",
    "    dpi=300\n",
    ")\n",
    "\n",
    "# Set your data paths (same as before)\n",
    "base_data_path = \"/Users/ahzs645/Library/CloudStorage/GoogleDrive-ahzs645@gmail.com/My Drive/University/Research/Grad/UC Davis Ann/NASA MAIA/Data\"\n",
    "\n",
    "config.aethalometer_files = {\n",
    "    'pkl_data': os.path.join(\n",
    "        base_data_path,\n",
    "        \"Aethelometry Data/Kyan Data/Mergedcleaned and uncleaned MA350 data20250707030704\",\n",
    "        \"df_uncleaned_Jacros_API_and_OG.pkl\"\n",
    "    ),\n",
    "    'csv_data': os.path.join(\n",
    "        base_data_path,\n",
    "        \"Aethelometry Data/Raw\",\n",
    "        \"Jacros_MA350_1-min_2022-2024_Cleaned.csv\"\n",
    "    )\n",
    "}\n",
    "\n",
    "config.ftir_db_path = os.path.join(\n",
    "    base_data_path,\n",
    "    \"EC-HIPS-Aeth Comparison/Data/Original Data/Combined Database\",\n",
    "    \"spartan_ftir_hips.db\"\n",
    ")\n",
    "\n",
    "# Create enhanced setup with PKL cleaning capabilities\n",
    "setup = create_enhanced_setup(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f42c9f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ Loading datasets...\n",
      "ğŸ“¦ Setting up modular system...\n",
      "âœ… Aethalometer loaders imported\n",
      "âœ… Database loader imported\n",
      "âœ… Plotting utilities imported\n",
      "âœ… Plotting style configured\n",
      "âœ… Successfully imported 5 modular components\n",
      "\n",
      "============================================================\n",
      "ğŸ“ LOADING DATASETS\n",
      "============================================================\n",
      "ğŸ“ Loading all datasets...\n",
      "\n",
      "==================================================\n",
      "ğŸ“Š Loading pkl_data\n",
      "==================================================\n",
      "ğŸ“ Loading pkl_data: df_uncleaned_Jacros_API_and_OG.pkl\n",
      "Detected format: standard\n",
      "Set 'datetime_local' as DatetimeIndex for time series operations\n",
      "Converted 17 columns to JPL format\n",
      "Warning: Missing recommended columns: ['datetime_local', 'Biomass.BCc', 'Fossil.fuel.BCc']\n",
      "âœ… Modular load: 1,665,156 rows Ã— 238 columns\n",
      "ğŸ“Š Method: modular\n",
      "ğŸ“Š Format: jpl\n",
      "ğŸ“Š Memory: 7443.05 MB\n",
      "ğŸ§® BC columns: 30\n",
      "ğŸ“ˆ ATN columns: 25\n",
      "ğŸ“… Time range: 2021-01-09 16:38:00 to 2025-06-26 23:18:00\n",
      "âœ… pkl_data loaded successfully\n",
      "\n",
      "==================================================\n",
      "ğŸ“Š Loading csv_data\n",
      "==================================================\n",
      "ğŸ“ Loading csv_data: Jacros_MA350_1-min_2022-2024_Cleaned.csv\n",
      "Set 'Time (Local)' as DatetimeIndex for time series operations\n",
      "Converted 5 columns to JPL format\n",
      "âœ… Modular load: 1,095,086 rows Ã— 77 columns\n",
      "ğŸ“Š Method: modular\n",
      "ğŸ“Š Format: jpl\n",
      "ğŸ“Š Memory: 884.83 MB\n",
      "ğŸ§® BC columns: 15\n",
      "ğŸ“ˆ ATN columns: 10\n",
      "ğŸ“… Time range: 2022-04-12 12:46:01+03:00 to 2024-08-20 12:01:00+03:00\n",
      "âœ… csv_data loaded successfully\n",
      "\n",
      "==================================================\n",
      "ğŸ—ƒï¸ Loading FTIR/HIPS data\n",
      "==================================================\n",
      "ğŸ—ƒï¸ Loading FTIR/HIPS data for site ETAD...\n",
      "ğŸ“Š Available sites: ['ILNZ', 'ILHA', 'ZAJB', 'CAHA', 'CASH', 'AEAZ', 'AUMN', 'KRUL', 'MXMC', 'ZAPR', 'CHTS', 'ETAD', 'INDH', 'TWTA', 'USPA', 'TWKA', 'KRSE', 'PRFJ', 'BDDU', 'BIBU', 'USNO', 'IDBD', None]\n",
      "âœ… Modular FTIR load: 168 samples\n",
      "ğŸ“… Date range: 2022-12-07 00:00:00 to 2024-05-12 00:00:00\n",
      "âœ… FTIR/HIPS data loaded successfully\n",
      "\n",
      "ğŸ“Š Loading summary: 3 datasets loaded\n",
      "\n",
      "ğŸ“Š LOADING SUMMARY\n",
      "============================================================\n",
      "âœ… Successfully loaded 3 datasets\n",
      "   - pkl_data: 1,665,156 rows Ã— 238 columns\n",
      "   - csv_data: 1,095,086 rows Ã— 77 columns\n",
      "   - ftir_hips: 168 rows Ã— 12 columns\n",
      "============================================================\n",
      "âœ… Converting datetime_local from index to column...\n",
      "ğŸ“Š PKL data ready: (1665156, 239)\n",
      "ğŸ“… Date range: 2021-01-09 16:38:00 to 2025-06-26 23:18:00\n"
     ]
    }
   ],
   "source": [
    "print(\"ğŸ“ Loading datasets...\")\n",
    "datasets = setup.load_all_data()\n",
    "\n",
    "# Get PKL data and fix datetime_local issue\n",
    "pkl_data_original = setup.get_dataset('pkl_data')\n",
    "\n",
    "# Quick fix for datetime_local issue\n",
    "if 'datetime_local' not in pkl_data_original.columns:\n",
    "    if pkl_data_original.index.name == 'datetime_local':\n",
    "        print(\"âœ… Converting datetime_local from index to column...\")\n",
    "        pkl_data_original = pkl_data_original.reset_index()\n",
    "    elif hasattr(pkl_data_original.index, 'tz'):\n",
    "        print(\"âœ… Creating datetime_local column from datetime index...\")\n",
    "        pkl_data_original['datetime_local'] = pkl_data_original.index\n",
    "        pkl_data_original = pkl_data_original.reset_index(drop=True)\n",
    "\n",
    "print(f\"ğŸ“Š PKL data ready: {pkl_data_original.shape}\")\n",
    "print(f\"ğŸ“… Date range: {pkl_data_original['datetime_local'].min()} to {pkl_data_original['datetime_local'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a274f632",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”§ Comprehensive Preprocessing Pipeline\n",
      "============================================================\n",
      "Step 1: Processing datetime...\n",
      "\n",
      "Step 2: Fixing column names...\n",
      "âœ… Renamed 16 columns\n",
      "\n",
      "Step 3: Converting data types...\n",
      "Converted IR ATN1 to float.\n",
      "Converted UV ATN1 to float.\n",
      "Converted Blue ATN1 to float.\n",
      "Converted Green ATN1 to float.\n",
      "Converted Red ATN1 to float.\n",
      "âœ… Applied calibration.convert_to_float()\n",
      "\n",
      "Step 4: Adding Session ID...\n",
      "\n",
      "Step 5: Adding delta calculations...\n",
      "âœ… Applied calibration.add_deltas()\n",
      "\n",
      "Step 6: Final adjustments...\n",
      "âœ… Filtered to 2022+: 1,665,156 -> 1,627,058 rows\n"
     ]
    }
   ],
   "source": [
    "def comprehensive_preprocessing(df):\n",
    "    \"\"\"Apply all the preprocessing steps that the working pipeline includes\"\"\"\n",
    "    print(\"ğŸ”§ Comprehensive Preprocessing Pipeline\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    df_processed = df.copy()\n",
    "    original_size = len(df_processed)\n",
    "    \n",
    "    # Step 1: Fix datetime column\n",
    "    print(\"Step 1: Processing datetime...\")\n",
    "    if 'datetime_local' in df_processed.columns:\n",
    "        if not pd.api.types.is_datetime64_any_dtype(df_processed['datetime_local']):\n",
    "            df_processed['datetime_local'] = pd.to_datetime(\n",
    "                df_processed['datetime_local'], utc=True\n",
    "            ).dt.tz_convert('Africa/Addis_Ababa')\n",
    "            print(\"âœ… Converted datetime_local to proper timezone\")\n",
    "    \n",
    "    # Step 2: Column renaming (your existing logic but improved)\n",
    "    print(\"\\nStep 2: Fixing column names...\")\n",
    "    column_mapping = {}\n",
    "    \n",
    "    # Map BC columns (handle both BC1->BCc conversion and dot notation)\n",
    "    for wl in ['IR', 'Blue', 'Green', 'Red', 'UV']:\n",
    "        # First priority: use .BCc if available\n",
    "        if f'{wl}.BCc' in df_processed.columns:\n",
    "            column_mapping[f'{wl}.BCc'] = f'{wl} BCc'\n",
    "        # Second priority: rename BC1 to BCc\n",
    "        elif f'{wl} BC1' in df_processed.columns:\n",
    "            df_processed = df_processed.rename(columns={f'{wl} BC1': f'{wl} BCc'})\n",
    "            print(f\"  Renamed {wl} BC1 -> {wl} BCc\")\n",
    "    \n",
    "    # Map ATN columns (dots to spaces)\n",
    "    for wl in ['IR', 'Blue', 'Green', 'Red', 'UV']:\n",
    "        for spot in [1, 2]:\n",
    "            if f'{wl}.ATN{spot}' in df_processed.columns:\n",
    "                column_mapping[f'{wl}.ATN{spot}'] = f'{wl} ATN{spot}'\n",
    "    \n",
    "    # Map flow columns\n",
    "    if 'Flow.total.mL.min' in df_processed.columns:\n",
    "        column_mapping['Flow.total.mL.min'] = 'Flow total (mL/min)'\n",
    "    \n",
    "    # Apply column renaming\n",
    "    if column_mapping:\n",
    "        df_processed = df_processed.rename(columns=column_mapping)\n",
    "        print(f\"âœ… Renamed {len(column_mapping)} columns\")\n",
    "    \n",
    "    # Step 3: Data type conversion\n",
    "    print(\"\\nStep 3: Converting data types...\")\n",
    "    if HAS_CALIBRATION:\n",
    "        df_processed = calibration.convert_to_float(df_processed)\n",
    "        print(\"âœ… Applied calibration.convert_to_float()\")\n",
    "    else:\n",
    "        # Manual data type conversion\n",
    "        numeric_cols = []\n",
    "        for col in df_processed.columns:\n",
    "            if any(x in col for x in ['ATN', 'BC', 'Flow', 'temp', 'Temp']):\n",
    "                if df_processed[col].dtype == 'object':\n",
    "                    try:\n",
    "                        df_processed[col] = pd.to_numeric(df_processed[col], errors='coerce')\n",
    "                        numeric_cols.append(col)\n",
    "                    except:\n",
    "                        pass\n",
    "        print(f\"âœ… Converted {len(numeric_cols)} columns to numeric\")\n",
    "    \n",
    "    # Step 4: Add Session ID\n",
    "    print(\"\\nStep 4: Adding Session ID...\")\n",
    "    if 'Session ID' not in df_processed.columns and 'Tape position' in df_processed.columns:\n",
    "        position_change = df_processed['Tape position'] != df_processed['Tape position'].shift()\n",
    "        df_processed['Session ID'] = position_change.cumsum()\n",
    "        print(\"âœ… Added Session ID based on tape position changes\")\n",
    "    \n",
    "    # Step 5: Add delta calculations\n",
    "    print(\"\\nStep 5: Adding delta calculations...\")\n",
    "    if HAS_CALIBRATION:\n",
    "        df_processed = calibration.add_deltas(df_processed)\n",
    "        print(\"âœ… Applied calibration.add_deltas()\")\n",
    "    else:\n",
    "        # Manual delta calculation for critical columns\n",
    "        print(\"âš ï¸ Manual delta calculation (limited functionality)\")\n",
    "        # Add basic delta calculations for ATN columns\n",
    "        attn_cols = [col for col in df_processed.columns if 'ATN' in col and col.count(' ') == 1]\n",
    "        for col in attn_cols:\n",
    "            try:\n",
    "                if 'Serial number' in df_processed.columns and 'Session ID' in df_processed.columns:\n",
    "                    df_processed[f'delta {col}'] = (\n",
    "                        df_processed.groupby(['Serial number', 'Session ID'])[col].diff()\n",
    "                    )\n",
    "                else:\n",
    "                    df_processed[f'delta {col}'] = df_processed[col].diff()\n",
    "            except:\n",
    "                pass\n",
    "        print(f\"âœ… Added basic delta calculations for {len(attn_cols)} ATN columns\")\n",
    "    \n",
    "    # Step 6: Set serial number and filter by year\n",
    "    print(\"\\nStep 6: Final adjustments...\")\n",
    "    df_processed['Serial number'] = \"MA350-0238\"\n",
    "    \n",
    "    if 'datetime_local' in df_processed.columns:\n",
    "        df_processed = df_processed.loc[df_processed['datetime_local'].dt.year >= 2022]\n",
    "        print(f\"âœ… Filtered to 2022+: {original_size:,} -> {len(df_processed):,} rows\")\n",
    "    \n",
    "    return df_processed\n",
    "\n",
    "# Apply comprehensive preprocessing\n",
    "pkl_data_preprocessed = comprehensive_preprocessing(pkl_data_original)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59e1ef71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ Applying DEMA Smoothing...\n",
      "========================================\n",
      "\n",
      "Processing IR wavelength...\n",
      "  Available BC columns: ['IR BC1', 'IR BC2', 'IR BCc']\n",
      "  âœ… Created IR BC1 smoothed\n",
      "  âœ… Created IR BC2 smoothed\n",
      "  âœ… Created IR BCc smoothed\n",
      "\n",
      "Processing Blue wavelength...\n",
      "  Available BC columns: ['Blue BC1', 'Blue BC2', 'Blue BCc']\n",
      "  âœ… Created Blue BC1 smoothed\n",
      "  âœ… Created Blue BC2 smoothed\n",
      "  âœ… Created Blue BCc smoothed\n"
     ]
    }
   ],
   "source": [
    "def apply_dema_smoothing_working(df, wavelengths=['IR', 'Blue']):\n",
    "    \"\"\"Apply DEMA smoothing that actually works\"\"\"\n",
    "    print(\"ğŸ”„ Applying DEMA Smoothing...\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    df_smoothed = df.copy()\n",
    "    \n",
    "    for wl in wavelengths:\n",
    "        print(f\"\\nProcessing {wl} wavelength...\")\n",
    "        \n",
    "        # Check what BC columns we have\n",
    "        bc_cols = [col for col in df_smoothed.columns if wl in col and 'BC' in col and 'smoothed' not in col]\n",
    "        print(f\"  Available BC columns: {bc_cols}\")\n",
    "        \n",
    "        if not bc_cols:\n",
    "            print(f\"  âš ï¸ No BC columns found for {wl}\")\n",
    "            continue\n",
    "        \n",
    "        # Process each BC column\n",
    "        for bc_col in bc_cols:\n",
    "            try:\n",
    "                # Group by measurement sessions for proper smoothing\n",
    "                groupby_cols = ['Serial number']\n",
    "                if 'Session ID' in df_smoothed.columns:\n",
    "                    groupby_cols.append('Session ID')\n",
    "                if 'Tape position' in df_smoothed.columns:\n",
    "                    groupby_cols.append('Tape position')\n",
    "                \n",
    "                smoothed_values = []\n",
    "                \n",
    "                for group_keys, group in df_smoothed.groupby(groupby_cols):\n",
    "                    if len(group) > 2:  # Need at least 3 points for smoothing\n",
    "                        values = group[bc_col].dropna()\n",
    "                        if len(values) > 1:\n",
    "                            # Apply DEMA algorithm\n",
    "                            span = min(10, len(values) // 2)  # Adaptive span\n",
    "                            if span < 2:\n",
    "                                span = 2\n",
    "                            \n",
    "                            # First EMA\n",
    "                            ema1 = values.ewm(span=span, adjust=False).mean()\n",
    "                            # Second EMA (EMA of EMA)\n",
    "                            ema2 = ema1.ewm(span=span, adjust=False).mean()\n",
    "                            # DEMA = 2*EMA1 - EMA2\n",
    "                            dema = 2 * ema1 - ema2\n",
    "                            \n",
    "                            # Store results with original indices\n",
    "                            for idx, val in dema.items():\n",
    "                                smoothed_values.append((idx, val))\n",
    "                        else:\n",
    "                            # Not enough data, use original values\n",
    "                            for idx, val in values.items():\n",
    "                                smoothed_values.append((idx, val))\n",
    "                    else:\n",
    "                        # Very small group, use original values\n",
    "                        values = group[bc_col].dropna()\n",
    "                        for idx, val in values.items():\n",
    "                            smoothed_values.append((idx, val))\n",
    "                \n",
    "                # Create smoothed column\n",
    "                smoothed_col = f'{bc_col} smoothed'\n",
    "                df_smoothed[smoothed_col] = np.nan\n",
    "                \n",
    "                for idx, val in smoothed_values:\n",
    "                    df_smoothed.loc[idx, smoothed_col] = val\n",
    "                \n",
    "                print(f\"  âœ… Created {smoothed_col}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  âš ï¸ Failed to smooth {bc_col}: {e}\")\n",
    "    \n",
    "    return df_smoothed\n",
    "\n",
    "# Apply DEMA smoothing\n",
    "pkl_data_with_smoothing = apply_dema_smoothing_working(pkl_data_preprocessed, ['IR', 'Blue'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9911e380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ§¹ Final Cleaning Pipeline\n",
      "============================================================\n",
      "Starting PKL data cleaning pipeline...\n",
      "==================================================\n",
      "ğŸ” Data Structure Diagnosis:\n",
      "------------------------------\n",
      "DataFrame shape: (1627058, 284)\n",
      "Date range: 2022-04-12 09:12:00 to 2025-06-26 23:18:00\n",
      "BC columns: 15 (e.g., ['Blue BC1', 'Blue BC2', 'Blue BCc'])\n",
      "BC smoothed columns: 6 (e.g., ['IR BC1 smoothed', 'IR BC2 smoothed', 'IR BCc smoothed'])\n",
      "ATN columns: 40 (e.g., ['Blue ATN1', 'Blue ATN2', 'Green ATN1'])\n",
      "Flow columns: 4 (e.g., ['Flow setpoint (mL/min)', 'Flow total (mL/min)', 'Flow1 (mL/min)'])\n",
      "\n",
      "Targeted wavelengths: ['IR', 'Blue']\n",
      "  IR: âœ… BC | âœ… BC smoothed | âœ… ATN\n",
      "  Blue: âœ… BC | âœ… BC smoothed | âœ… ATN\n",
      "------------------------------\n",
      "\n",
      "ğŸ§¹ Starting cleaning steps...\n",
      "1919 datapoints removed due to Start up or Tape advance status\n",
      "Statuses of concern, count by device and status:\n",
      "\n",
      "MA350-0238 Flow unstable 750\n",
      "MA350-0238 Optical saturation 0\n",
      "MA350-0238 Sample timing error 0\n",
      "Number of datapoints with invalid optics values\n",
      "AFTER dropping data with 'Optical saturation' status values: 802\n",
      "Removed 56128 datapoints for optics\n",
      "Status cleaning: Removed 58797 rows (3.61%)\n",
      "Extreme BCc cleaning: Removed 13900 rows (0.89%)\n",
      "Flow range cleaning: Removed 0 rows (0.00%)\n",
      "Abnormal flow ratio: Removed 29362 rows (1.89%)\n",
      "Leak ratio cleaning: Removed 507 rows (0.03%)\n",
      "BCc denominator cleaning: Removed 25537 rows (1.68%)\n",
      "Sharp change 605\n",
      "noise 1425\n",
      "Temperature change cleaning: Removed 2030 rows (0.14%)\n",
      "IR ATN1_roughness: threshold=0.1147, high periods flagged: 17945 rows so far\n",
      "IR ATN2_roughness: threshold=0.1008, high periods flagged: 18439 rows so far\n",
      "Blue ATN1_roughness: threshold=0.1859, high periods flagged: 19114 rows so far\n",
      "Blue ATN2_roughness: threshold=0.1664, high periods flagged: 19142 rows so far\n",
      "==================================================\n",
      "Cleaning complete! Final data shape: (1477783, 293)\n",
      "\n",
      "ğŸ“Š Cleaning Results Summary:\n",
      "============================================================\n",
      "Original data points: 1,665,156\n",
      "After preprocessing: 1,627,058\n",
      "After smoothing: 1,627,058\n",
      "Final cleaned: 1,477,783\n",
      "Total removed: 187,373 (11.25%)\n",
      "\n",
      "âœ… PKL data cleaning completed successfully!\n",
      "\n",
      "ğŸ“Š Final data verification:\n",
      "Shape: (1477783, 293)\n",
      "Date range: 2022-04-12 09:54:00 to 2025-06-26 23:18:00\n",
      "  âœ… IR ATN1\n",
      "  âœ… IR BCc\n",
      "  âœ… Blue ATN1\n",
      "  âœ… Blue BCc\n",
      "  âœ… Flow total (mL/min)\n",
      "  âœ… Smoothed columns: 6\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nğŸ§¹ Final Cleaning Pipeline\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    # Initialize cleaner\n",
    "    cleaner = PKLDataCleaner(wavelengths_to_filter=['IR', 'Blue'], verbose=True)\n",
    "    \n",
    "    # Apply cleaning pipeline, skipping preprocessing since we did it comprehensively\n",
    "    pkl_data_cleaned = cleaner.clean_pipeline(pkl_data_with_smoothing, skip_preprocessing=True)\n",
    "    \n",
    "    print(\"\\nğŸ“Š Cleaning Results Summary:\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Original data points: {len(pkl_data_original):,}\")\n",
    "    print(f\"After preprocessing: {len(pkl_data_preprocessed):,}\")\n",
    "    print(f\"After smoothing: {len(pkl_data_with_smoothing):,}\")\n",
    "    print(f\"Final cleaned: {len(pkl_data_cleaned):,}\")\n",
    "    \n",
    "    total_removed = len(pkl_data_original) - len(pkl_data_cleaned)\n",
    "    removal_pct = (total_removed / len(pkl_data_original) * 100)\n",
    "    print(f\"Total removed: {total_removed:,} ({removal_pct:.2f}%)\")\n",
    "    \n",
    "    print(\"\\nâœ… PKL data cleaning completed successfully!\")\n",
    "    \n",
    "    # Quick verification\n",
    "    print(f\"\\nğŸ“Š Final data verification:\")\n",
    "    print(f\"Shape: {pkl_data_cleaned.shape}\")\n",
    "    if 'datetime_local' in pkl_data_cleaned.columns:\n",
    "        print(f\"Date range: {pkl_data_cleaned['datetime_local'].min()} to {pkl_data_cleaned['datetime_local'].max()}\")\n",
    "    \n",
    "    # Check for key columns\n",
    "    key_cols = ['IR ATN1', 'IR BCc', 'Blue ATN1', 'Blue BCc', 'Flow total (mL/min)']\n",
    "    for col in key_cols:\n",
    "        status = \"âœ…\" if col in pkl_data_cleaned.columns else \"âŒ\"\n",
    "        print(f\"  {status} {col}\")\n",
    "    \n",
    "    # Check smoothed columns\n",
    "    smoothed_cols = [col for col in pkl_data_cleaned.columns if 'smoothed' in col]\n",
    "    print(f\"  âœ… Smoothed columns: {len(smoothed_cols)}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Final cleaning failed: {e}\")\n",
    "    print(\"Using preprocessed and smoothed data as fallback\")\n",
    "    pkl_data_cleaned = pkl_data_with_smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36f0731f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ’¾ Cleaned data exported:\n",
      "  ğŸ“„ CSV: pkl_data_cleaned_working.csv\n",
      "  ğŸ“¦ Pickle: pkl_data_cleaned_working.pkl\n",
      "\n",
      "ğŸ‰ PKL Data Cleaning Complete!\n",
      "ğŸ“Š Final shape: (1477783, 293)\n",
      "ğŸ“… Date range: 2022-04-12 09:54:00 to 2025-06-26 23:18:00\n",
      "ğŸš€ Ready for further analysis!\n"
     ]
    }
   ],
   "source": [
    "if 'pkl_data_cleaned' in locals() and len(pkl_data_cleaned) > 0:\n",
    "    # Export cleaned data\n",
    "    output_csv = 'pkl_data_cleaned_working.csv'\n",
    "    output_pkl = 'pkl_data_cleaned_working.pkl'\n",
    "    \n",
    "    pkl_data_cleaned.to_csv(output_csv, index=False)\n",
    "    pkl_data_cleaned.to_pickle(output_pkl)\n",
    "    \n",
    "    print(f\"\\nğŸ’¾ Cleaned data exported:\")\n",
    "    print(f\"  ğŸ“„ CSV: {output_csv}\")\n",
    "    print(f\"  ğŸ“¦ Pickle: {output_pkl}\")\n",
    "    \n",
    "    print(f\"\\nğŸ‰ PKL Data Cleaning Complete!\")\n",
    "    print(f\"ğŸ“Š Final shape: {pkl_data_cleaned.shape}\")\n",
    "    print(f\"ğŸ“… Date range: {pkl_data_cleaned['datetime_local'].min()} to {pkl_data_cleaned['datetime_local'].max()}\")\n",
    "    print(\"ğŸš€ Ready for further analysis!\")\n",
    "    \n",
    "else:\n",
    "    print(\"âŒ No cleaned data available to export\")\n",
    "    print(\"Please check the error messages above\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
