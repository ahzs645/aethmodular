{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17ca5e71",
   "metadata": {},
   "source": [
    "# Complete Aethalometer-FTIR/HIPS Data Pipeline\n",
    "\n",
    "This notebook provides a complete pipeline for loading, processing, and merging:\n",
    "- **Aethalometer data** (PKL and CSV formats)\n",
    "- **FTIR/HIPS filter data** (SQLite database)\n",
    "- **Time-matched merging** with quality assessment\n",
    "- **Statistical analysis** and visualization\n",
    "\n",
    "## Pipeline Overview:\n",
    "1. **Setup and Configuration**\n",
    "2. **Database Loading** (FTIR/HIPS data)\n",
    "3. **Aethalometer Loading** (PKL and CSV files)\n",
    "4. **Quality Assessment** (9am-9am period analysis)\n",
    "5. **Time-Matched Merging**\n",
    "6. **Statistical Analysis**\n",
    "7. **Visualization and Export**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1dde30",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "863c1d21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Aethalometer-FTIR/HIPS Pipeline with Modular System\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Core imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sqlite3\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Dict, List, Union, Optional, Any, Tuple\n",
    "import pickle\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add src directory to path for modular system access\n",
    "src_path = str(Path('../src').resolve())\n",
    "if src_path not in sys.path:\n",
    "    sys.path.insert(0, src_path)\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "print(\"ğŸš€ Aethalometer-FTIR/HIPS Pipeline with Modular System\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "580f07db",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2. Configuration Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fbbafc37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Configuration:\n",
      "   Site: ETAD\n",
      "   Wavelength: Red\n",
      "   Output format: jpl\n",
      "   Quality threshold: 10 minutes\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CONFIGURATION SECTION - UPDATE THESE PATHS FOR YOUR DATA\n",
    "# =============================================================================\n",
    "\n",
    "# File paths - UPDATE THESE TO YOUR ACTUAL FILE PATHS\n",
    "AETHALOMETER_FILES = {\n",
    "    'pkl_data': \"/Users/ahzs645/Library/CloudStorage/GoogleDrive-ahzs645@gmail.com/My Drive/University/Research/Grad/UC Davis Ann/NASA MAIA/Data/Aethelometry Data/Kyan Data/Mergedcleaned and uncleaned MA350 data20250707030704/df_uncleaned_Jacros_API_and_OG.pkl\",\n",
    "    'csv_data': \"/Users/ahzs645/Library/CloudStorage/GoogleDrive-ahzs645@gmail.com/My Drive/University/Research/Grad/UC Davis Ann/NASA MAIA/Data/Aethelometry Data/Raw/Jacros_MA350_1-min_2022-2024_Cleaned.csv\"\n",
    "}\n",
    "\n",
    "FTIR_DB_PATH = \"/Users/ahzs645/Library/CloudStorage/GoogleDrive-ahzs645@gmail.com/My Drive/University/Research/Grad/UC Davis Ann/NASA MAIA/Data/EC-HIPS-Aeth Comparison/Data/Original Data/Combined Database/spartan_ftir_hips.db\"\n",
    "\n",
    "# Analysis parameters\n",
    "SITE_CODE = 'ETAD'\n",
    "WAVELENGTH = 'Red'  # Options: 'Red', 'Blue', 'Green', 'UV', 'IR'\n",
    "QUALITY_THRESHOLD = 10  # Maximum missing minutes for \"excellent\" quality\n",
    "OUTPUT_DIR = \"outputs\"\n",
    "OUTPUT_FORMAT = \"jpl\"  # 'jpl' or 'standard' format\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"ğŸ“Š Configuration:\")\n",
    "print(f\"   Site: {SITE_CODE}\")\n",
    "print(f\"   Wavelength: {WAVELENGTH}\")\n",
    "print(f\"   Output format: {OUTPUT_FORMAT}\")\n",
    "print(f\"   Quality threshold: {QUALITY_THRESHOLD} minutes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "034f7692",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¦ Setting up modular system...\n",
      "âœ… Added /Users/ahzs645/Github/aethmodular/src to Python path\n",
      "âœ… Aethalometer loaders imported\n",
      "âœ… Database loader imported\n",
      "âœ… Black carbon analyzer imported\n",
      "âœ… Source apportionment analyzer imported\n",
      "âœ… Aethalometer plotter imported\n",
      "âœ… Plotting style configured\n",
      "âœ… File I/O utilities imported\n",
      "âœ… Data validation functions imported\n",
      "âœ… Successfully imported 12 components\n",
      "\n",
      "ğŸ‰ Modular system available!\n",
      "ğŸ“Š Available components:\n",
      "   loaders: ['AethalometerPKLLoader', 'AethalometerCSVLoader', 'load_aethalometer_data', 'FTIRHIPSLoader']\n",
      "   analysis: ['BlackCarbonAnalyzer', 'SourceApportionmentAnalyzer']\n",
      "   utils: ['AethalometerPlotter', 'ensure_output_directory', 'validate_columns_exist', 'get_valid_data_mask', 'validate_sample_size', 'check_data_range']\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# MODULAR SYSTEM IMPORTS AND SETUP\n",
    "# =============================================================================\n",
    "\n",
    "def setup_modular_system():\n",
    "    \"\"\"Import and setup the modular aethalometer system\"\"\"\n",
    "    \n",
    "    print(\"ğŸ“¦ Setting up modular system...\")\n",
    "    \n",
    "    # Add the src directory to Python path\n",
    "    import sys\n",
    "    import os\n",
    "    \n",
    "    # Get the parent directory (aethmodular) and add src to path\n",
    "    notebook_dir = os.path.dirname(os.path.abspath(''))\n",
    "    if notebook_dir.endswith('notebooks'):\n",
    "        project_root = os.path.dirname(notebook_dir)\n",
    "    else:\n",
    "        project_root = notebook_dir\n",
    "    \n",
    "    src_path = os.path.join(project_root, 'src')\n",
    "    if src_path not in sys.path:\n",
    "        sys.path.insert(0, src_path)\n",
    "    print(f\"âœ… Added {src_path} to Python path\")\n",
    "    \n",
    "    # Dictionary to store successfully imported components\n",
    "    imported_components = {\n",
    "        'loaders': {},\n",
    "        'analysis': {},\n",
    "        'utils': {}\n",
    "    }\n",
    "    \n",
    "    # Try importing aethalometer loaders\n",
    "    try:\n",
    "        from data.loaders.aethalometer import (\n",
    "            AethalometerPKLLoader, \n",
    "            AethalometerCSVLoader,\n",
    "            load_aethalometer_data\n",
    "        )\n",
    "        imported_components['loaders'].update({\n",
    "            'AethalometerPKLLoader': AethalometerPKLLoader,\n",
    "            'AethalometerCSVLoader': AethalometerCSVLoader,\n",
    "            'load_aethalometer_data': load_aethalometer_data\n",
    "        })\n",
    "        print(\"âœ… Aethalometer loaders imported\")\n",
    "    except ImportError as e:\n",
    "        print(f\"âš ï¸ Aethalometer loaders failed: {e}\")\n",
    "    \n",
    "    # Try importing database loader\n",
    "    try:\n",
    "        from data.loaders.database import FTIRHIPSLoader\n",
    "        imported_components['loaders']['FTIRHIPSLoader'] = FTIRHIPSLoader\n",
    "        print(\"âœ… Database loader imported\")\n",
    "    except ImportError as e:\n",
    "        print(f\"âš ï¸ Database loader failed: {e}\")\n",
    "    \n",
    "    # Try importing analysis modules\n",
    "    try:\n",
    "        from analysis.bc.black_carbon_analyzer import BlackCarbonAnalyzer\n",
    "        imported_components['analysis']['BlackCarbonAnalyzer'] = BlackCarbonAnalyzer\n",
    "        print(\"âœ… Black carbon analyzer imported\")\n",
    "    except ImportError as e:\n",
    "        print(f\"âš ï¸ Black carbon analyzer failed: {e}\")\n",
    "    \n",
    "    try:\n",
    "        from analysis.bc.source_apportionment import SourceApportionmentAnalyzer\n",
    "        imported_components['analysis']['SourceApportionmentAnalyzer'] = SourceApportionmentAnalyzer\n",
    "        print(\"âœ… Source apportionment analyzer imported\")\n",
    "    except ImportError as e:\n",
    "        print(f\"âš ï¸ Source apportionment analyzer failed: {e}\")\n",
    "    \n",
    "    # Try importing visualization utilities\n",
    "    try:\n",
    "        from utils.plotting import AethalometerPlotter\n",
    "        imported_components['utils']['AethalometerPlotter'] = AethalometerPlotter\n",
    "        print(\"âœ… Aethalometer plotter imported\")\n",
    "    except ImportError as e:\n",
    "        print(f\"âš ï¸ Aethalometer plotter failed: {e}\")\n",
    "    \n",
    "    try:\n",
    "        from config.plotting import setup_plotting_style\n",
    "        setup_plotting_style()\n",
    "        print(\"âœ… Plotting style configured\")\n",
    "    except ImportError as e:\n",
    "        print(f\"âš ï¸ Plotting style config failed: {e}\")\n",
    "    \n",
    "    # Try importing utility functions\n",
    "    try:\n",
    "        from utils.file_io import ensure_output_directory\n",
    "        imported_components['utils']['ensure_output_directory'] = ensure_output_directory\n",
    "        print(\"âœ… File I/O utilities imported\")\n",
    "    except ImportError as e:\n",
    "        print(f\"âš ï¸ File I/O utilities failed: {e}\")\n",
    "    \n",
    "    try:\n",
    "        # Import validation functions from data processors (only existing functions)\n",
    "        from data.processors.validation import (\n",
    "            validate_columns_exist, \n",
    "            get_valid_data_mask,\n",
    "            validate_sample_size,\n",
    "            check_data_range\n",
    "        )\n",
    "        imported_components['utils'].update({\n",
    "            'validate_columns_exist': validate_columns_exist,\n",
    "            'get_valid_data_mask': get_valid_data_mask,\n",
    "            'validate_sample_size': validate_sample_size,\n",
    "            'check_data_range': check_data_range\n",
    "        })\n",
    "        print(\"âœ… Data validation functions imported\")\n",
    "    except ImportError as e:\n",
    "        print(f\"âš ï¸ Data validation failed: {e}\")\n",
    "    \n",
    "    # Return what we successfully imported\n",
    "    if any(imported_components.values()):\n",
    "        success_count = sum(len(v) for v in imported_components.values())\n",
    "        print(f\"âœ… Successfully imported {success_count} components\")\n",
    "        return imported_components\n",
    "    else:\n",
    "        print(\"âš ï¸ No modular components could be imported\")\n",
    "        print(f\"ğŸ“ Current working directory: {os.getcwd()}\")\n",
    "        print(f\"ğŸ“ Python path includes: {sys.path[:3]}...\")\n",
    "        return None\n",
    "\n",
    "# Setup the modular system\n",
    "modular_components = setup_modular_system()\n",
    "MODULAR_AVAILABLE = modular_components is not None\n",
    "\n",
    "if MODULAR_AVAILABLE:\n",
    "    print(f\"\\nğŸ‰ Modular system available!\")\n",
    "    print(f\"ğŸ“Š Available components:\")\n",
    "    for category, components in modular_components.items():\n",
    "        if components:\n",
    "            print(f\"   {category}: {list(components.keys())}\")\n",
    "else:\n",
    "    print(\"\\nğŸ“ Falling back to basic functionality...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3d5e8be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ENHANCED AETHALOMETER LOADING WITH MODULAR SYSTEM\n",
    "# =============================================================================\n",
    "\n",
    "def load_aethalometer_with_modular_system(file_path: str, \n",
    "                                        output_format: str = \"jpl\",\n",
    "                                        site_filter: Optional[str] = None) -> Tuple[Optional[pd.DataFrame], Dict]:\n",
    "    \"\"\"\n",
    "    Load aethalometer data using the modular system\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    file_path : str\n",
    "        Path to aethalometer data file (.pkl or .csv)\n",
    "    output_format : str\n",
    "        Output format ('jpl' or 'standard')\n",
    "    site_filter : str, optional\n",
    "        Filter data by site\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple\n",
    "        (DataFrame, summary_dict)\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"ğŸ“ Loading with modular system: {Path(file_path).name}\")\n",
    "    print(f\"ğŸ”§ Output format: {output_format}\")\n",
    "    \n",
    "    if not MODULAR_AVAILABLE:\n",
    "        raise ImportError(\"Modular system not available\")\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"âŒ File not found: {file_path}\")\n",
    "        return None, {}\n",
    "    \n",
    "    try:\n",
    "        # Use the unified loading function from modular system\n",
    "        load_function = modular_components['loaders']['load_aethalometer_data']\n",
    "        \n",
    "        df = load_function(\n",
    "            file_path,\n",
    "            output_format=output_format,\n",
    "            site_filter=site_filter,\n",
    "            set_datetime_index=True\n",
    "        )\n",
    "        \n",
    "        if df is None or len(df) == 0:\n",
    "            print(f\"âŒ No data loaded from {file_path}\")\n",
    "            return None, {}\n",
    "        \n",
    "        # Generate comprehensive summary\n",
    "        summary = {\n",
    "            'file_name': Path(file_path).name,\n",
    "            'file_type': Path(file_path).suffix,\n",
    "            'format': output_format,\n",
    "            'shape': df.shape,\n",
    "            'columns': len(df.columns),\n",
    "            'memory_mb': df.memory_usage(deep=True).sum() / 1024**2,\n",
    "            'bc_columns': [col for col in df.columns if '.BCc' in col or 'BC' in col.upper()],\n",
    "            'atn_columns': [col for col in df.columns if '.ATN' in col or 'ATN' in col.upper()],\n",
    "            'time_range': (df.index.min(), df.index.max()) if hasattr(df.index, 'min') else None,\n",
    "            'missing_data_pct': (df.isnull().sum().sum() / df.size) * 100,\n",
    "            'has_datetime_index': isinstance(df.index, pd.DatetimeIndex)\n",
    "        }\n",
    "        \n",
    "        print(f\"âœ… Successfully loaded: {df.shape[0]:,} rows Ã— {df.shape[1]} columns\")\n",
    "        print(f\"ğŸ“Š Format: {output_format}\")\n",
    "        print(f\"ğŸ“Š Memory usage: {summary['memory_mb']:.2f} MB\")\n",
    "        print(f\"ğŸ§® BC columns found: {len(summary['bc_columns'])}\")\n",
    "        print(f\"ğŸ“ˆ ATN columns found: {len(summary['atn_columns'])}\")\n",
    "        \n",
    "        if summary['time_range']:\n",
    "            print(f\"ğŸ“… Time range: {summary['time_range'][0]} to {summary['time_range'][1]}\")\n",
    "        \n",
    "        return df, summary\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error loading with modular system: {e}\")\n",
    "        return None, {}\n",
    "\n",
    "def load_aethalometer_fallback(file_path: str) -> Tuple[Optional[pd.DataFrame], Dict]:\n",
    "    \"\"\"\n",
    "    Fallback direct loading method when modular system fails\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"ğŸ”„ Using fallback loading for: {Path(file_path).name}\")\n",
    "    \n",
    "    try:\n",
    "        file_ext = Path(file_path).suffix.lower()\n",
    "        \n",
    "        if file_ext == '.pkl':\n",
    "            with open(file_path, 'rb') as f:\n",
    "                df = pickle.load(f)\n",
    "            print(f\"âœ… Direct PKL load: {df.shape[0]:,} rows Ã— {df.shape[1]} columns\")\n",
    "            \n",
    "        elif file_ext == '.csv':\n",
    "            df = pd.read_csv(file_path)\n",
    "            print(f\"âœ… Direct CSV load: {df.shape[0]:,} rows Ã— {df.shape[1]} columns\")\n",
    "            \n",
    "            # Handle datetime conversion for CSV\n",
    "            if 'Time (UTC)' in df.columns:\n",
    "                df['Time (UTC)'] = pd.to_datetime(df['Time (UTC)'], utc=True)\n",
    "                df['Time (Local)'] = df['Time (UTC)'].dt.tz_convert('Africa/Addis_Ababa')\n",
    "                df.set_index('Time (Local)', inplace=True)\n",
    "            elif 'datetime' in df.columns:\n",
    "                df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "                df.set_index('datetime', inplace=True)\n",
    "        \n",
    "        else:\n",
    "            print(f\"âŒ Unsupported file format: {file_ext}\")\n",
    "            return None, {}\n",
    "        \n",
    "        # Ensure datetime index\n",
    "        if not isinstance(df.index, pd.DatetimeIndex):\n",
    "            # Try to convert index or find datetime column\n",
    "            datetime_cols = ['datetime', 'timestamp', 'Time', 'Date']\n",
    "            for col in datetime_cols:\n",
    "                if col in df.columns:\n",
    "                    df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "                    df = df[df[col].notna()]\n",
    "                    df.set_index(col, inplace=True)\n",
    "                    break\n",
    "        \n",
    "        # Generate basic summary\n",
    "        summary = {\n",
    "            'file_name': Path(file_path).name,\n",
    "            'file_type': Path(file_path).suffix,\n",
    "            'format': 'fallback',\n",
    "            'shape': df.shape,\n",
    "            'bc_columns': [col for col in df.columns if 'BC' in col.upper()],\n",
    "            'atn_columns': [col for col in df.columns if 'ATN' in col.upper()],\n",
    "            'time_range': (df.index.min(), df.index.max()) if isinstance(df.index, pd.DatetimeIndex) else None,\n",
    "            'has_datetime_index': isinstance(df.index, pd.DatetimeIndex)\n",
    "        }\n",
    "        \n",
    "        return df, summary\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Fallback loading failed: {e}\")\n",
    "        return None, {}\n",
    "\n",
    "def load_aethalometer_robust(file_path: str, \n",
    "                           output_format: str = \"jpl\",\n",
    "                           site_filter: Optional[str] = None) -> Tuple[Optional[pd.DataFrame], Dict]:\n",
    "    \"\"\"\n",
    "    Robust aethalometer loading with modular system + fallback\n",
    "    \"\"\"\n",
    "    \n",
    "    # Try modular system first\n",
    "    if MODULAR_AVAILABLE:\n",
    "        df, summary = load_aethalometer_with_modular_system(file_path, output_format, site_filter)\n",
    "        if df is not None:\n",
    "            return df, summary\n",
    "        print(\"âš ï¸ Modular system failed, trying fallback...\")\n",
    "    \n",
    "    # Fallback to direct loading\n",
    "    df, summary = load_aethalometer_fallback(file_path)\n",
    "    \n",
    "    if df is not None:\n",
    "        print(\"âœ… Fallback loading successful\")\n",
    "    else:\n",
    "        print(\"âŒ All loading methods failed\")\n",
    "    \n",
    "    return df, summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "89fd0e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ENHANCED FTIR/HIPS LOADING WITH MODULAR SYSTEM\n",
    "# =============================================================================\n",
    "\n",
    "def load_ftir_hips_with_modular_system(db_path: str, site_code: str) -> Optional[pd.DataFrame]:\n",
    "    \"\"\"Load FTIR/HIPS data using modular system\"\"\"\n",
    "    \n",
    "    print(f\"ğŸ—ƒï¸ Loading FTIR/HIPS data with modular system...\")\n",
    "    \n",
    "    if not MODULAR_AVAILABLE:\n",
    "        print(\"âš ï¸ Modular system not available for FTIR loading\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        FTIRHIPSLoader = modular_components['loaders']['FTIRHIPSLoader']\n",
    "        \n",
    "        loader = FTIRHIPSLoader(db_path)\n",
    "        \n",
    "        # Get available sites\n",
    "        available_sites = loader.get_available_sites()\n",
    "        print(f\"ğŸ“Š Available sites: {available_sites}\")\n",
    "        \n",
    "        if site_code not in available_sites:\n",
    "            print(f\"âš ï¸ Site '{site_code}' not found in database\")\n",
    "            return None\n",
    "        \n",
    "        # Load data\n",
    "        df = loader.load(site_code)\n",
    "        \n",
    "        if len(df) > 0:\n",
    "            print(f\"âœ… Loaded {len(df)} FTIR/HIPS samples\")\n",
    "            print(f\"ğŸ“… Date range: {df['sample_date'].min()} to {df['sample_date'].max()}\")\n",
    "            return df\n",
    "        else:\n",
    "            print(\"âŒ No FTIR/HIPS data found\")\n",
    "            return None\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Modular FTIR loading failed: {e}\")\n",
    "        return None\n",
    "\n",
    "def load_ftir_hips_fallback(db_path: str, site_code: str) -> Optional[pd.DataFrame]:\n",
    "    \"\"\"Fallback FTIR/HIPS loading\"\"\"\n",
    "    \n",
    "    print(f\"ğŸ”„ Using fallback FTIR loading...\")\n",
    "    \n",
    "    try:\n",
    "        conn = sqlite3.connect(db_path)\n",
    "        \n",
    "        # Try standard query\n",
    "        query = \"\"\"\n",
    "        SELECT \n",
    "            f.filter_id, f.sample_date, f.site_code, f.filter_type,\n",
    "            m.volume_m3, m.ec_ftir, m.ec_ftir_mdl, m.oc_ftir, m.oc_ftir_mdl,\n",
    "            m.fabs, m.fabs_mdl, m.fabs_uncertainty, m.ftir_batch_id\n",
    "        FROM filters f\n",
    "        JOIN ftir_sample_measurements m ON f.filter_id = m.filter_id\n",
    "        WHERE f.site_code = ?\n",
    "        ORDER BY f.sample_date\n",
    "        \"\"\"\n",
    "        \n",
    "        df = pd.read_sql_query(query, conn, params=(site_code,))\n",
    "        conn.close()\n",
    "        \n",
    "        if len(df) > 0:\n",
    "            df['sample_date'] = pd.to_datetime(df['sample_date'])\n",
    "            print(f\"âœ… Fallback FTIR load: {len(df)} samples\")\n",
    "            return df\n",
    "        else:\n",
    "            print(\"âŒ No FTIR data found with fallback method\")\n",
    "            return None\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Fallback FTIR loading failed: {e}\")\n",
    "        return None\n",
    "\n",
    "def load_ftir_hips_robust(db_path: str, site_code: str) -> Optional[pd.DataFrame]:\n",
    "    \"\"\"Robust FTIR/HIPS loading with modular + fallback\"\"\"\n",
    "    \n",
    "    # Try modular system first\n",
    "    if MODULAR_AVAILABLE:\n",
    "        df = load_ftir_hips_with_modular_system(db_path, site_code)\n",
    "        if df is not None:\n",
    "            return df\n",
    "        print(\"âš ï¸ Modular FTIR loading failed, trying fallback...\")\n",
    "    \n",
    "    # Fallback method\n",
    "    return load_ftir_hips_fallback(db_path, site_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "400d18c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_aethalometer_robust_patched(file_path: str, \n",
    "                                   output_format: str = \"jpl\",\n",
    "                                   site_filter: Optional[str] = None) -> Tuple[Optional[pd.DataFrame], Dict]:\n",
    "    \"\"\"\n",
    "    Patched robust aethalometer loading with modular system + improved fallback\n",
    "    \"\"\"\n",
    "    print(f\"ğŸ“ Loading with patched system: {Path(file_path).name}\")\n",
    "    print(f\"ğŸ”§ Output format: {output_format}\")\n",
    "    \n",
    "    file_ext = Path(file_path).suffix.lower()\n",
    "    \n",
    "    # For CSV files, use the modular CSV loader if available\n",
    "    if file_ext == '.csv':\n",
    "        print(f\"ğŸ¯ Using improved CSV loading...\")\n",
    "        \n",
    "        if MODULAR_AVAILABLE and 'AethalometerCSVLoader' in modular_components.get('loaders', {}):\n",
    "            try:\n",
    "                AethalometerCSVLoader = modular_components['loaders']['AethalometerCSVLoader']\n",
    "                csv_loader = AethalometerCSVLoader(file_path)\n",
    "                df = csv_loader.load(set_datetime_index=True)\n",
    "                \n",
    "                # Check if datetime index was properly set\n",
    "                if df is not None and not isinstance(df.index, pd.DatetimeIndex):\n",
    "                    print(\"ğŸ”§ Fixing datetime index for CSV data...\")\n",
    "                    \n",
    "                    # Try to find and parse datetime columns\n",
    "                    datetime_candidates = []\n",
    "                    for col in df.columns:\n",
    "                        if any(keyword in col.lower() for keyword in ['date', 'time', 'datetime']):\n",
    "                            datetime_candidates.append(col)\n",
    "                    \n",
    "                    print(f\"ğŸ” Found datetime candidates: {datetime_candidates}\")\n",
    "                    \n",
    "                    # Try to create datetime index from candidates\n",
    "                    datetime_col = None\n",
    "                    for col in datetime_candidates:\n",
    "                        try:\n",
    "                            # Try parsing the column as datetime\n",
    "                            df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "                            if df[col].notna().sum() > len(df) * 0.8:  # At least 80% valid dates\n",
    "                                datetime_col = col\n",
    "                                break\n",
    "                        except Exception as e:\n",
    "                            print(f\"   Failed to parse {col}: {e}\")\n",
    "                    \n",
    "                    # If we found a valid datetime column, set it as index\n",
    "                    if datetime_col:\n",
    "                        print(f\"âœ… Using {datetime_col} as datetime index\")\n",
    "                        df = df.set_index(datetime_col)\n",
    "                        df = df.sort_index()\n",
    "                    else:\n",
    "                        print(\"âš ï¸ Could not find valid datetime column, checking for combined date/time columns...\")\n",
    "                        \n",
    "                        # Look for separate date and time columns\n",
    "                        date_cols = [col for col in df.columns if 'date' in col.lower()]\n",
    "                        time_cols = [col for col in df.columns if 'time' in col.lower() and 'date' not in col.lower()]\n",
    "                        \n",
    "                        if date_cols and time_cols:\n",
    "                            try:\n",
    "                                # Combine first date and time columns\n",
    "                                date_col = date_cols[0]\n",
    "                                time_col = time_cols[0]\n",
    "                                print(f\"ğŸ”§ Combining {date_col} + {time_col}\")\n",
    "                                \n",
    "                                datetime_combined = pd.to_datetime(df[date_col].astype(str) + ' ' + df[time_col].astype(str), errors='coerce')\n",
    "                                \n",
    "                                if datetime_combined.notna().sum() > len(df) * 0.8:\n",
    "                                    df['datetime_combined'] = datetime_combined\n",
    "                                    df = df.set_index('datetime_combined')\n",
    "                                    df = df.sort_index()\n",
    "                                    print(\"âœ… Successfully created combined datetime index\")\n",
    "                                else:\n",
    "                                    print(\"âŒ Combined datetime parsing failed\")\n",
    "                            except Exception as e:\n",
    "                                print(f\"âŒ Error combining date/time columns: {e}\")\n",
    "                \n",
    "                if df is not None and len(df) > 0:\n",
    "                    # Apply format conversion if needed\n",
    "                    if output_format == 'jpl':\n",
    "                        # Convert standard column names to JPL format if needed\n",
    "                        column_mapping = {\n",
    "                            'IR BCc': 'IR.BCc',\n",
    "                            'Blue BCc': 'Blue.BCc', \n",
    "                            'Green BCc': 'Green.BCc',\n",
    "                            'Red BCc': 'Red.BCc',\n",
    "                            'UV BCc': 'UV.BCc',\n",
    "                            'Biomass BCc': 'Biomass.BCc',\n",
    "                            'Fossil fuel BCc': 'Fossil.fuel.BCc',\n",
    "                        }\n",
    "                        \n",
    "                        rename_dict = {}\n",
    "                        for std_col, jpl_col in column_mapping.items():\n",
    "                            if std_col in df.columns:\n",
    "                                rename_dict[std_col] = jpl_col\n",
    "                        \n",
    "                        if rename_dict:\n",
    "                            df = df.rename(columns=rename_dict)\n",
    "                            print(f\"Converted {len(rename_dict)} columns to JPL format\")\n",
    "                    \n",
    "                    # Generate comprehensive summary\n",
    "                    summary = {\n",
    "                        'file_name': Path(file_path).name,\n",
    "                        'file_type': Path(file_path).suffix,\n",
    "                        'format': output_format,\n",
    "                        'shape': df.shape,\n",
    "                        'columns': len(df.columns),\n",
    "                        'memory_mb': df.memory_usage(deep=True).sum() / 1024**2,\n",
    "                        'bc_columns': [col for col in df.columns if '.BCc' in col or 'BC' in col.upper()],\n",
    "                        'atn_columns': [col for col in df.columns if '.ATN' in col or 'ATN' in col.upper()],\n",
    "                        'time_range': (df.index.min(), df.index.max()) if hasattr(df.index, 'min') else None,\n",
    "                        'missing_data_pct': (df.isnull().sum().sum() / df.size) * 100,\n",
    "                        'has_datetime_index': isinstance(df.index, pd.DatetimeIndex)\n",
    "                    }\n",
    "                    \n",
    "                    print(f\"âœ… Successfully loaded: {df.shape[0]:,} rows Ã— {df.shape[1]} columns\")\n",
    "                    print(f\"ğŸ“Š Format: {output_format}\")\n",
    "                    print(f\"ğŸ“Š Memory usage: {summary['memory_mb']:.2f} MB\")\n",
    "                    print(f\"ğŸ§® BC columns found: {len(summary['bc_columns'])}\")\n",
    "                    print(f\"ğŸ“ˆ ATN columns found: {len(summary['atn_columns'])}\")\n",
    "                    \n",
    "                    if summary['time_range']:\n",
    "                        print(f\"ğŸ“… Time range: {summary['time_range'][0]} to {summary['time_range'][1]}\")\n",
    "                    \n",
    "                    return df, summary\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"âŒ Modular CSV loader failed: {e}\")\n",
    "        \n",
    "        # Fallback for CSV files\n",
    "        print(f\"ğŸ”„ Using CSV fallback...\")\n",
    "        return load_aethalometer_fallback(file_path)\n",
    "    \n",
    "    # For PKL files, try modular system first\n",
    "    elif file_ext == '.pkl':\n",
    "        if MODULAR_AVAILABLE:\n",
    "            try:\n",
    "                load_function = modular_components['loaders']['load_aethalometer_data']\n",
    "                \n",
    "                df = load_function(\n",
    "                    file_path,\n",
    "                    output_format=output_format,\n",
    "                    site_filter=site_filter,\n",
    "                    set_datetime_index=True\n",
    "                )\n",
    "                \n",
    "                if df is not None and len(df) > 0:\n",
    "                    # Generate comprehensive summary\n",
    "                    summary = {\n",
    "                        'file_name': Path(file_path).name,\n",
    "                        'file_type': Path(file_path).suffix,\n",
    "                        'format': output_format,\n",
    "                        'shape': df.shape,\n",
    "                        'columns': len(df.columns),\n",
    "                        'memory_mb': df.memory_usage(deep=True).sum() / 1024**2,\n",
    "                        'bc_columns': [col for col in df.columns if '.BCc' in col or 'BC' in col.upper()],\n",
    "                        'atn_columns': [col for col in df.columns if '.ATN' in col or 'ATN' in col.upper()],\n",
    "                        'time_range': (df.index.min(), df.index.max()) if hasattr(df.index, 'min') else None,\n",
    "                        'missing_data_pct': (df.isnull().sum().sum() / df.size) * 100,\n",
    "                        'has_datetime_index': isinstance(df.index, pd.DatetimeIndex)\n",
    "                    }\n",
    "                    \n",
    "                    print(f\"âœ… Successfully loaded: {df.shape[0]:,} rows Ã— {df.shape[1]} columns\")\n",
    "                    print(f\"ğŸ“Š Format: {output_format}\")\n",
    "                    print(f\"ğŸ“Š Memory usage: {summary['memory_mb']:.2f} MB\")\n",
    "                    print(f\"ğŸ§® BC columns found: {len(summary['bc_columns'])}\")\n",
    "                    print(f\"ğŸ“ˆ ATN columns found: {len(summary['atn_columns'])}\")\n",
    "                    \n",
    "                    if summary['time_range']:\n",
    "                        print(f\"ğŸ“… Time range: {summary['time_range'][0]} to {summary['time_range'][1]}\")\n",
    "                    \n",
    "                    return df, summary\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"âŒ Modular system failed: {e}\")\n",
    "        \n",
    "        # Fallback for PKL files\n",
    "        print(f\"ğŸ”„ Using fallback for PKL...\")\n",
    "        return load_aethalometer_fallback(file_path)\n",
    "    \n",
    "    else:\n",
    "        print(f\"âŒ Unsupported file format: {file_ext}\")\n",
    "        return None, {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59d9771",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bc65b43f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ğŸ“ LOADING DATASETS WITH MODULAR SYSTEM\n",
      "============================================================\n",
      "ğŸ§¹ Clearing previous dataset cache...\n",
      "\n",
      "ğŸ—ƒï¸ Loading FTIR/HIPS database...\n",
      "ğŸ—ƒï¸ Loading FTIR/HIPS data with modular system...\n",
      "ğŸ“Š Available sites: ['ILNZ', 'ILHA', 'ZAJB', 'CAHA', 'CASH', 'AEAZ', 'AUMN', 'KRUL', 'MXMC', 'ZAPR', 'CHTS', 'ETAD', 'INDH', 'TWTA', 'USPA', 'TWKA', 'KRSE', 'PRFJ', 'BDDU', 'BIBU', 'USNO', 'IDBD', None]\n",
      "âœ… Loaded 168 FTIR/HIPS samples\n",
      "ğŸ“… Date range: 2022-12-07 00:00:00 to 2024-05-12 00:00:00\n",
      "âœ… FTIR/HIPS data loaded successfully\n",
      "\n",
      "ğŸ“‹ Sample FTIR/HIPS data:\n",
      "     filter_id sample_date  ec_ftir  oc_ftir  fabs\n",
      "0  ETAD-0122-2         NaT      NaN      NaN   NaN\n",
      "1  ETAD-0123-3         NaT      NaN      NaN   NaN\n",
      "2  ETAD-0124-4         NaT      NaN      NaN   NaN\n",
      "3  ETAD-0125-5         NaT      NaN      NaN   NaN\n",
      "4  ETAD-0126-6         NaT      NaN      NaN   NaN\n",
      "\n",
      "ğŸ“Š Loading aethalometer datasets...\n",
      "ğŸ“‹ Files to process: ['pkl_data', 'csv_data']\n",
      "\n",
      "============================================================\n",
      "ğŸ“ Processing pkl_data\n",
      "ğŸ“‚ File: df_uncleaned_Jacros_API_and_OG.pkl\n",
      "ğŸ“ Path exists: True\n",
      "ğŸ“Š File extension: .pkl\n",
      "============================================================\n",
      "ğŸ“ Loading with patched system: df_uncleaned_Jacros_API_and_OG.pkl\n",
      "ğŸ”§ Output format: jpl\n",
      "Detected format: standard\n",
      "Detected format: standard\n",
      "Set 'datetime_local' as DatetimeIndex for time series operations\n",
      "Set 'datetime_local' as DatetimeIndex for time series operations\n",
      "Converted 17 columns to JPL format\n",
      "Warning: Missing recommended columns: ['datetime_local', 'Biomass.BCc', 'Fossil.fuel.BCc']\n",
      "Converted 17 columns to JPL format\n",
      "Warning: Missing recommended columns: ['datetime_local', 'Biomass.BCc', 'Fossil.fuel.BCc']\n",
      "âœ… Successfully loaded: 1,665,156 rows Ã— 238 columns\n",
      "ğŸ“Š Format: jpl\n",
      "ğŸ“Š Memory usage: 7443.05 MB\n",
      "ğŸ§® BC columns found: 30\n",
      "ğŸ“ˆ ATN columns found: 25\n",
      "ğŸ“… Time range: 2021-01-09 16:38:00 to 2025-06-26 23:18:00\n",
      "\n",
      "âœ… Raw data loaded: 1,665,156 rows Ã— 238 columns\n",
      "âœ… pkl_data successfully stored in datasets\n",
      "ğŸ¯ Red BC columns found: ['Red BC1', 'Red BC2', 'Red.BCc']\n",
      "ğŸ“Š Dataset info:\n",
      "   - Shape: (1665156, 238)\n",
      "   - Index type: DatetimeIndex\n",
      "   - Date range: 2021-01-09 16:38:00 to 2025-06-26 23:18:00\n",
      "\n",
      "============================================================\n",
      "ğŸ“ Processing csv_data\n",
      "ğŸ“‚ File: Jacros_MA350_1-min_2022-2024_Cleaned.csv\n",
      "ğŸ“ Path exists: True\n",
      "ğŸ“Š File extension: .csv\n",
      "============================================================\n",
      "ğŸ“ Loading with patched system: Jacros_MA350_1-min_2022-2024_Cleaned.csv\n",
      "ğŸ”§ Output format: jpl\n",
      "ğŸ¯ Using improved CSV loading...\n",
      "âœ… Successfully loaded: 1,665,156 rows Ã— 238 columns\n",
      "ğŸ“Š Format: jpl\n",
      "ğŸ“Š Memory usage: 7443.05 MB\n",
      "ğŸ§® BC columns found: 30\n",
      "ğŸ“ˆ ATN columns found: 25\n",
      "ğŸ“… Time range: 2021-01-09 16:38:00 to 2025-06-26 23:18:00\n",
      "\n",
      "âœ… Raw data loaded: 1,665,156 rows Ã— 238 columns\n",
      "âœ… pkl_data successfully stored in datasets\n",
      "ğŸ¯ Red BC columns found: ['Red BC1', 'Red BC2', 'Red.BCc']\n",
      "ğŸ“Š Dataset info:\n",
      "   - Shape: (1665156, 238)\n",
      "   - Index type: DatetimeIndex\n",
      "   - Date range: 2021-01-09 16:38:00 to 2025-06-26 23:18:00\n",
      "\n",
      "============================================================\n",
      "ğŸ“ Processing csv_data\n",
      "ğŸ“‚ File: Jacros_MA350_1-min_2022-2024_Cleaned.csv\n",
      "ğŸ“ Path exists: True\n",
      "ğŸ“Š File extension: .csv\n",
      "============================================================\n",
      "ğŸ“ Loading with patched system: Jacros_MA350_1-min_2022-2024_Cleaned.csv\n",
      "ğŸ”§ Output format: jpl\n",
      "ğŸ¯ Using improved CSV loading...\n",
      "ğŸ”§ Fixing datetime index for CSV data...\n",
      "ğŸ” Found datetime candidates: ['Time (UTC)', 'Timezone offset (mins)', 'Date local (yyyy/MM/dd)', 'Time local (hh:mm:ss)', 'Timebase (s)']\n",
      "âœ… Using Time (UTC) as datetime index\n",
      "ğŸ”§ Fixing datetime index for CSV data...\n",
      "ğŸ” Found datetime candidates: ['Time (UTC)', 'Timezone offset (mins)', 'Date local (yyyy/MM/dd)', 'Time local (hh:mm:ss)', 'Timebase (s)']\n",
      "âœ… Using Time (UTC) as datetime index\n",
      "Converted 5 columns to JPL format\n",
      "Converted 5 columns to JPL format\n",
      "âœ… Successfully loaded: 1,095,086 rows Ã— 76 columns\n",
      "ğŸ“Š Format: jpl\n",
      "ğŸ“Š Memory usage: 876.48 MB\n",
      "ğŸ§® BC columns found: 15\n",
      "ğŸ“ˆ ATN columns found: 10\n",
      "ğŸ“… Time range: 2022-04-12 09:46:01+00:00 to 2024-08-20 09:01:00+00:00\n",
      "\n",
      "âœ… Raw data loaded: 1,095,086 rows Ã— 76 columns\n",
      "âœ… csv_data successfully stored in datasets\n",
      "ğŸ¯ Red BC columns found: ['Red BC1', 'Red BC2', 'Red.BCc']\n",
      "ğŸ“Š Dataset info:\n",
      "   - Shape: (1095086, 76)\n",
      "   - Index type: DatetimeIndex\n",
      "   - Date range: 2022-04-12 09:46:01+00:00 to 2024-08-20 09:01:00+00:00\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š LOADING SUMMARY\n",
      "============================================================\n",
      "âœ… Successfully loaded 2 aethalometer datasets\n",
      "ğŸ“‹ Dataset names: ['pkl_data', 'csv_data']\n",
      "   - pkl_data: 1,665,156 rows Ã— 238 columns\n",
      "   - csv_data: 1,095,086 rows Ã— 76 columns\n",
      "============================================================\n",
      "âœ… Successfully loaded: 1,095,086 rows Ã— 76 columns\n",
      "ğŸ“Š Format: jpl\n",
      "ğŸ“Š Memory usage: 876.48 MB\n",
      "ğŸ§® BC columns found: 15\n",
      "ğŸ“ˆ ATN columns found: 10\n",
      "ğŸ“… Time range: 2022-04-12 09:46:01+00:00 to 2024-08-20 09:01:00+00:00\n",
      "\n",
      "âœ… Raw data loaded: 1,095,086 rows Ã— 76 columns\n",
      "âœ… csv_data successfully stored in datasets\n",
      "ğŸ¯ Red BC columns found: ['Red BC1', 'Red BC2', 'Red.BCc']\n",
      "ğŸ“Š Dataset info:\n",
      "   - Shape: (1095086, 76)\n",
      "   - Index type: DatetimeIndex\n",
      "   - Date range: 2022-04-12 09:46:01+00:00 to 2024-08-20 09:01:00+00:00\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š LOADING SUMMARY\n",
      "============================================================\n",
      "âœ… Successfully loaded 2 aethalometer datasets\n",
      "ğŸ“‹ Dataset names: ['pkl_data', 'csv_data']\n",
      "   - pkl_data: 1,665,156 rows Ã— 238 columns\n",
      "   - csv_data: 1,095,086 rows Ã— 76 columns\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# LOAD ALL DATASETS WITH ENHANCED SYSTEM (CLEANED VERSION)\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ“ LOADING DATASETS WITH MODULAR SYSTEM\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Clear any previous data to prevent confusion\n",
    "if 'aethalometer_datasets' in globals():\n",
    "    print(\"ğŸ§¹ Clearing previous dataset cache...\")\n",
    "    del aethalometer_datasets\n",
    "if 'aethalometer_summaries' in globals():\n",
    "    del aethalometer_summaries\n",
    "\n",
    "# Load FTIR/HIPS data\n",
    "print(f\"\\nğŸ—ƒï¸ Loading FTIR/HIPS database...\")\n",
    "ftir_data = load_ftir_hips_robust(FTIR_DB_PATH, SITE_CODE)\n",
    "\n",
    "if ftir_data is not None:\n",
    "    print(f\"âœ… FTIR/HIPS data loaded successfully\")\n",
    "    \n",
    "    # Display sample\n",
    "    print(f\"\\nğŸ“‹ Sample FTIR/HIPS data:\")\n",
    "    display_cols = ['filter_id', 'sample_date', 'ec_ftir', 'oc_ftir', 'fabs']\n",
    "    available_cols = [col for col in display_cols if col in ftir_data.columns]\n",
    "    print(ftir_data[available_cols].head())\n",
    "else:\n",
    "    print(\"âŒ Failed to load FTIR/HIPS data\")\n",
    "\n",
    "# Initialize fresh datasets dictionary\n",
    "aethalometer_datasets = {}\n",
    "aethalometer_summaries = {}\n",
    "\n",
    "# Load aethalometer datasets with explicit tracking\n",
    "print(f\"\\nğŸ“Š Loading aethalometer datasets...\")\n",
    "print(f\"ğŸ“‹ Files to process: {list(AETHALOMETER_FILES.keys())}\")\n",
    "\n",
    "for dataset_name, file_path in AETHALOMETER_FILES.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"ğŸ“ Processing {dataset_name}\")\n",
    "    print(f\"ğŸ“‚ File: {Path(file_path).name}\")\n",
    "    print(f\"ğŸ“ Path exists: {os.path.exists(file_path)}\")\n",
    "    print(f\"ğŸ“Š File extension: {Path(file_path).suffix}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    if os.path.exists(file_path):\n",
    "        # Call the loading function\n",
    "        df, summary = load_aethalometer_robust_patched(\n",
    "            file_path, \n",
    "            output_format=OUTPUT_FORMAT,\n",
    "            site_filter=None\n",
    "        )\n",
    "        \n",
    "        # Validate the results\n",
    "        if df is not None and len(df) > 0:\n",
    "            print(f\"\\nâœ… Raw data loaded: {df.shape[0]:,} rows Ã— {df.shape[1]} columns\")\n",
    "            \n",
    "            # Check if it has a proper datetime index\n",
    "            if isinstance(df.index, pd.DatetimeIndex):\n",
    "                aethalometer_datasets[dataset_name] = df\n",
    "                aethalometer_summaries[dataset_name] = summary\n",
    "                print(f\"âœ… {dataset_name} successfully stored in datasets\")\n",
    "                \n",
    "                # Show available BC columns for selected wavelength\n",
    "                bc_cols = [col for col in df.columns if WAVELENGTH in col and 'BC' in col]\n",
    "                print(f\"ğŸ¯ {WAVELENGTH} BC columns found: {bc_cols}\")\n",
    "                \n",
    "                # Show basic dataset info\n",
    "                print(f\"ğŸ“Š Dataset info:\")\n",
    "                print(f\"   - Shape: {df.shape}\")\n",
    "                print(f\"   - Index type: {type(df.index).__name__}\")\n",
    "                print(f\"   - Date range: {df.index.min()} to {df.index.max()}\")\n",
    "                \n",
    "            else:\n",
    "                print(f\"âš ï¸ {dataset_name} has invalid datetime index - type: {type(df.index).__name__}\")\n",
    "                print(f\"   Index sample: {df.index[:5].tolist()}\")\n",
    "        else:\n",
    "            print(f\"âŒ Failed to load {dataset_name} properly - df is None or empty\")\n",
    "    else:\n",
    "        print(f\"âš ï¸ File not found: {file_path}\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(f\"ğŸ“Š LOADING SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"âœ… Successfully loaded {len(aethalometer_datasets)} aethalometer datasets\")\n",
    "print(f\"ğŸ“‹ Dataset names: {list(aethalometer_datasets.keys())}\")\n",
    "\n",
    "for name, df in aethalometer_datasets.items():\n",
    "    print(f\"   - {name}: {df.shape[0]:,} rows Ã— {df.shape[1]} columns\")\n",
    "    \n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9fbe348",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1960b2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 4. Aethalometer Data Loading Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4a1e14",
   "metadata": {},
   "source": [
    "## 5. Quality Assessment Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1853bc4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ğŸ” CLEAN QUALITY ASSESSMENT\n",
      "================================================================================\n",
      "\n",
      "============================================================\n",
      "ğŸ” Analyzing pkl_data data quality...\n",
      "ğŸ“Š Quality threshold: â‰¤10 missing minutes per 24h period\n",
      "ğŸ“… Time range: 2021-01-09 16:38:00 to 2025-06-26 23:18:00\n",
      "ğŸ“Š Actual data points: 1,665,156\n",
      "ğŸ“Š Expected data points (1-min resolution): 2,346,161\n",
      "âš ï¸ Missing data points: 681,005\n",
      "ğŸ“Š Data completeness: 71.0%\n",
      "ğŸ“… Analyzing 1630 24-hour periods...\n",
      "âœ… Found 1036 excellent quality periods\n",
      "ğŸ“… Excellent periods range: 2021-02-18 09:00:00 to 2025-06-25 09:00:00\n",
      "ğŸ“Š Missing minutes distribution:\n",
      "   0 minutes missing: 778 periods\n",
      "   1-5 minutes missing: 239 periods\n",
      "   6-10 minutes missing: 19 periods\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "ğŸ” Analyzing csv_data data quality...\n",
      "ğŸ“Š Quality threshold: â‰¤10 missing minutes per 24h period\n",
      "ğŸ“… Time range: 2022-04-12 09:46:01+00:00 to 2024-08-20 09:01:00+00:00\n",
      "ğŸ“Š Actual data points: 1,095,086\n",
      "ğŸ“Š Expected data points (1-min resolution): 1,239,795\n",
      "âš ï¸ Missing data points: 144,709\n",
      "ğŸ“Š Data completeness: 88.3%\n",
      "ğŸ“… Analyzing 862 24-hour periods...\n",
      "âœ… Found 718 excellent quality periods\n",
      "ğŸ“… Excellent periods range: 2022-04-14 09:00:00+00:00 to 2024-08-19 09:00:00+00:00\n",
      "ğŸ“Š Missing minutes distribution:\n",
      "   0 minutes missing: 112 periods\n",
      "   1-5 minutes missing: 594 periods\n",
      "   6-10 minutes missing: 12 periods\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Clear any cached variables to ensure clean calculation\n",
    "if 'excellent_periods_dict' in globals():\n",
    "    del excellent_periods_dict\n",
    "\n",
    "def assess_data_quality_clean(aethalometer_df: pd.DataFrame, \n",
    "                             dataset_name: str,\n",
    "                             quality_threshold: int = 10) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Clean implementation of quality assessment for 24-hour periods (9am-to-9am).\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    aethalometer_df : pd.DataFrame\n",
    "        Aethalometer data with datetime index\n",
    "    dataset_name : str\n",
    "        Name of the dataset for logging\n",
    "    quality_threshold : int\n",
    "        Maximum missing minutes per 24h period for \"excellent\" quality\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        DataFrame with excellent periods (start_time, end_time, missing_minutes)\n",
    "    \"\"\"\n",
    "    print(f\"ğŸ” Analyzing {dataset_name} data quality...\")\n",
    "    print(f\"ğŸ“Š Quality threshold: â‰¤{quality_threshold} missing minutes per 24h period\")\n",
    "    \n",
    "    # Ensure datetime index\n",
    "    if not isinstance(aethalometer_df.index, pd.DatetimeIndex):\n",
    "        print(f\"âŒ Invalid index type: {type(aethalometer_df.index)}\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Get basic info\n",
    "    df_start = aethalometer_df.index.min()\n",
    "    df_end = aethalometer_df.index.max()\n",
    "    actual_points = len(aethalometer_df.index.unique())\n",
    "    \n",
    "    print(f\"ğŸ“… Time range: {df_start} to {df_end}\")\n",
    "    print(f\"ğŸ“Š Actual data points: {actual_points:,}\")\n",
    "    \n",
    "    # Calculate expected points based on 1-minute resolution\n",
    "    total_minutes = int((df_end - df_start).total_seconds() / 60) + 1\n",
    "    print(f\"ğŸ“Š Expected data points (1-min resolution): {total_minutes:,}\")\n",
    "    \n",
    "    # Calculate missing points\n",
    "    missing_points = total_minutes - actual_points\n",
    "    completeness = (actual_points / total_minutes) * 100\n",
    "    \n",
    "    print(f\"âš ï¸ Missing data points: {missing_points:,}\")\n",
    "    print(f\"ğŸ“Š Data completeness: {completeness:.1f}%\")\n",
    "    \n",
    "    # Quick sanity check\n",
    "    if missing_points < 0:\n",
    "        print(\"âš ï¸ Warning: More data points than expected - possible duplicates or sub-minute data\")\n",
    "        missing_points = 0\n",
    "    \n",
    "    # Create all possible 9am-to-9am periods\n",
    "    first_9am = df_start.normalize() + pd.Timedelta(hours=9)\n",
    "    if df_start.hour < 9:\n",
    "        first_9am -= pd.Timedelta(days=1)\n",
    "    \n",
    "    last_9am = df_end.normalize() + pd.Timedelta(hours=9)\n",
    "    if df_end.hour < 9:\n",
    "        last_9am -= pd.Timedelta(days=1)\n",
    "    \n",
    "    all_period_starts = pd.date_range(first_9am, last_9am, freq='D')\n",
    "    \n",
    "    print(f\"ğŸ“… Analyzing {len(all_period_starts)} 24-hour periods...\")\n",
    "    \n",
    "    # For each period, count missing minutes\n",
    "    excellent_periods_list = []\n",
    "    \n",
    "    for period_start in all_period_starts:\n",
    "        period_end = period_start + pd.Timedelta(days=1)\n",
    "        \n",
    "        # Get data for this period\n",
    "        period_data = aethalometer_df.loc[period_start:period_end]\n",
    "        actual_minutes = len(period_data)\n",
    "        expected_minutes = 1440  # 24 hours * 60 minutes\n",
    "        missing_minutes = max(0, expected_minutes - actual_minutes)\n",
    "        \n",
    "        # Check if this period qualifies as excellent\n",
    "        if missing_minutes <= quality_threshold:\n",
    "            excellent_periods_list.append({\n",
    "                'start_time': period_start,\n",
    "                'end_time': period_end,\n",
    "                'missing_minutes': missing_minutes\n",
    "            })\n",
    "    \n",
    "    # Create DataFrame\n",
    "    excellent_periods = pd.DataFrame(excellent_periods_list)\n",
    "    \n",
    "    print(f\"âœ… Found {len(excellent_periods)} excellent quality periods\")\n",
    "    \n",
    "    if len(excellent_periods) > 0:\n",
    "        print(f\"ğŸ“… Excellent periods range: {excellent_periods['start_time'].min()} to {excellent_periods['start_time'].max()}\")\n",
    "        print(f\"ğŸ“Š Missing minutes distribution:\")\n",
    "        print(f\"   0 minutes missing: {(excellent_periods['missing_minutes'] == 0).sum()} periods\")\n",
    "        print(f\"   1-5 minutes missing: {((excellent_periods['missing_minutes'] >= 1) & (excellent_periods['missing_minutes'] <= 5)).sum()} periods\")\n",
    "        print(f\"   6-10 minutes missing: {((excellent_periods['missing_minutes'] >= 6) & (excellent_periods['missing_minutes'] <= 10)).sum()} periods\")\n",
    "    else:\n",
    "        print(\"âŒ No excellent quality periods found\")\n",
    "    \n",
    "    return excellent_periods\n",
    "\n",
    "# Run clean quality assessment\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ” CLEAN QUALITY ASSESSMENT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "excellent_periods_dict = {}\n",
    "\n",
    "for dataset_name, df in aethalometer_datasets.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    excellent_periods = assess_data_quality_clean(df, dataset_name, QUALITY_THRESHOLD)\n",
    "    excellent_periods_dict[dataset_name] = excellent_periods\n",
    "    print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf66502a",
   "metadata": {},
   "source": [
    "## 6. Time-Matched Merging Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "15f8db43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” FTIR Data Inspection:\n",
      "==================================================\n",
      "ğŸ“Š FTIR data shape: (168, 12)\n",
      "ğŸ“Š Columns: ['filter_id', 'sample_date', 'site_code', 'volume_m3', 'ec_ftir', 'ec_ftir_mdl', 'oc_ftir', 'oc_ftir_mdl', 'fabs', 'fabs_mdl', 'fabs_uncertainty', 'ftir_batch_id']\n",
      "\n",
      "ğŸ“… Sample date info:\n",
      "   - Total samples: 168\n",
      "   - Non-null sample_date: 162\n",
      "   - NaT/null sample_date: 6\n",
      "   - Data type: datetime64[ns]\n",
      "   - First 5 values:\n",
      "     [0]: NaT (type: <class 'pandas._libs.tslibs.nattype.NaTType'>)\n",
      "     [1]: NaT (type: <class 'pandas._libs.tslibs.nattype.NaTType'>)\n",
      "     [2]: NaT (type: <class 'pandas._libs.tslibs.nattype.NaTType'>)\n",
      "     [3]: NaT (type: <class 'pandas._libs.tslibs.nattype.NaTType'>)\n",
      "     [4]: NaT (type: <class 'pandas._libs.tslibs.nattype.NaTType'>)\n",
      "\n",
      "âš ï¸ Found 6 rows with NaT sample_date:\n",
      "     filter_id sample_date\n",
      "0  ETAD-0122-2         NaT\n",
      "1  ETAD-0123-3         NaT\n",
      "2  ETAD-0124-4         NaT\n",
      "3  ETAD-0125-5         NaT\n",
      "4  ETAD-0126-6         NaT\n",
      "\n",
      "ğŸ“… Valid date range: 2022-12-07 00:00:00 to 2024-05-12 00:00:00\n"
     ]
    }
   ],
   "source": [
    "# Inspect FTIR data for NaT values and data quality\n",
    "print(\"ğŸ” FTIR Data Inspection:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if ftir_data is not None:\n",
    "    print(f\"ğŸ“Š FTIR data shape: {ftir_data.shape}\")\n",
    "    print(f\"ğŸ“Š Columns: {list(ftir_data.columns)}\")\n",
    "    \n",
    "    # Check sample_date column\n",
    "    print(f\"\\nğŸ“… Sample date info:\")\n",
    "    print(f\"   - Total samples: {len(ftir_data)}\")\n",
    "    print(f\"   - Non-null sample_date: {ftir_data['sample_date'].notna().sum()}\")\n",
    "    print(f\"   - NaT/null sample_date: {ftir_data['sample_date'].isna().sum()}\")\n",
    "    \n",
    "    # Show sample_date data type and first few values\n",
    "    print(f\"   - Data type: {ftir_data['sample_date'].dtype}\")\n",
    "    print(f\"   - First 5 values:\")\n",
    "    for i, date in enumerate(ftir_data['sample_date'].head()):\n",
    "        print(f\"     [{i}]: {date} (type: {type(date)})\")\n",
    "    \n",
    "    # Show rows with NaT values if any\n",
    "    nat_rows = ftir_data[ftir_data['sample_date'].isna()]\n",
    "    if len(nat_rows) > 0:\n",
    "        print(f\"\\nâš ï¸ Found {len(nat_rows)} rows with NaT sample_date:\")\n",
    "        print(nat_rows[['filter_id', 'sample_date']].head())\n",
    "    \n",
    "    # Date range for valid dates\n",
    "    valid_dates = ftir_data['sample_date'].dropna()\n",
    "    if len(valid_dates) > 0:\n",
    "        print(f\"\\nğŸ“… Valid date range: {valid_dates.min()} to {valid_dates.max()}\")\n",
    "    \n",
    "else:\n",
    "    print(\"âŒ FTIR data not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d592392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ğŸ”— TIME-MATCHED MERGING\n",
      "============================================================\n",
      "\n",
      "ğŸ“Š Merging pkl_data...\n",
      "----------------------------------------\n",
      "ğŸ”— Merging Red wavelength data for pkl_data...\n",
      "âš ï¸ Removed 6 filter samples with invalid dates\n",
      "ğŸ“Š Processing 162 valid filter samples\n",
      "ğŸ“Š Found 148 overlapping excellent periods with filter samples\n",
      "âœ… Successfully merged 148 periods\n",
      "âœ… pkl_data: 148 merged periods\n",
      "\n",
      "ğŸ“Š Merging csv_data...\n",
      "----------------------------------------\n",
      "ğŸ”— Merging Red wavelength data for csv_data...\n",
      "âš ï¸ Removed 6 filter samples with invalid dates\n",
      "ğŸ“Š Processing 162 valid filter samples\n",
      "ğŸ“Š Found 0 overlapping excellent periods with filter samples\n",
      "âš ï¸ No overlapping periods found\n",
      "âš ï¸ csv_data: No merged periods found\n",
      "\n",
      "ğŸ¯ Merging completed: 1 datasets merged\n"
     ]
    }
   ],
   "source": [
    "def extract_aethalometer_stats(aethalometer_df: pd.DataFrame, \n",
    "                             period_start: pd.Timestamp, \n",
    "                             period_end: pd.Timestamp, \n",
    "                             bc_column: str) -> Optional[Dict]:\n",
    "    \"\"\"Extract statistics for aethalometer data within a specific period\"\"\"\n",
    "    try:\n",
    "        # Handle timezone compatibility\n",
    "        if period_start.tz is not None and aethalometer_df.index.tz is None:\n",
    "            period_start_naive = period_start.tz_localize(None)\n",
    "            period_end_naive = period_end.tz_localize(None)\n",
    "        elif period_start.tz is None and aethalometer_df.index.tz is not None:\n",
    "            period_start = period_start.tz_localize(aethalometer_df.index.tz)\n",
    "            period_end = period_end.tz_localize(aethalometer_df.index.tz)\n",
    "            period_start_naive = period_start\n",
    "            period_end_naive = period_end\n",
    "        else:\n",
    "            period_start_naive = period_start\n",
    "            period_end_naive = period_end\n",
    "        \n",
    "        # Extract data for the period\n",
    "        period_data = aethalometer_df.loc[period_start_naive:period_end_naive, bc_column].dropna()\n",
    "        \n",
    "        if len(period_data) == 0:\n",
    "            return None\n",
    "        \n",
    "        # Calculate statistics\n",
    "        stats = {\n",
    "            'count': len(period_data),\n",
    "            'mean': period_data.mean(),\n",
    "            'median': period_data.median(),\n",
    "            'std': period_data.std(),\n",
    "            'min': period_data.min(),\n",
    "            'max': period_data.max(),\n",
    "            'q25': period_data.quantile(0.25),\n",
    "            'q75': period_data.quantile(0.75),\n",
    "            'negative_count': (period_data < 0).sum(),\n",
    "            'negative_pct': (period_data < 0).mean() * 100,\n",
    "            'data_coverage_pct': (len(period_data) / 1440) * 100  # 1440 minutes in 24h\n",
    "        }\n",
    "        \n",
    "        return stats\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Error extracting stats for period {period_start}: {e}\")\n",
    "        return None\n",
    "\n",
    "def map_ethiopian_seasons(month: int) -> str:\n",
    "    \"\"\"Map month number to Ethiopian season name\"\"\"\n",
    "    if month in [10, 11, 12, 1, 2]:\n",
    "        return 'Dry Season'\n",
    "    elif month in [3, 4, 5]:\n",
    "        return 'Belg Rainy Season'\n",
    "    else:  # months 6-9\n",
    "        return 'Kiremt Rainy Season'\n",
    "\n",
    "def merge_aethalometer_filter_data(aethalometer_df: pd.DataFrame,\n",
    "                                 filter_df: pd.DataFrame,\n",
    "                                 excellent_periods: pd.DataFrame,\n",
    "                                 wavelength: str = \"Red\",\n",
    "                                 dataset_name: str = \"aethalometer\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Merge aethalometer and filter sample data using 9am-9am period alignment.\n",
    "    \"\"\"\n",
    "    print(f\"ğŸ”— Merging {wavelength} wavelength data for {dataset_name}...\")\n",
    "    \n",
    "    # Find the BC column\n",
    "    bc_column = f\"{wavelength}.BCc\"\n",
    "    if bc_column not in aethalometer_df.columns:\n",
    "        # Try alternative naming\n",
    "        alt_columns = [col for col in aethalometer_df.columns \n",
    "                      if wavelength.lower() in col.lower() and 'bc' in col.lower()]\n",
    "        if alt_columns:\n",
    "            bc_column = alt_columns[0]\n",
    "            print(f\"ğŸ“ Using alternative BC column: {bc_column}\")\n",
    "        else:\n",
    "            print(f\"âŒ No BC column found for wavelength '{wavelength}'\")\n",
    "            return pd.DataFrame()\n",
    "    \n",
    "    # Filter out rows with NaT sample_date before processing\n",
    "    valid_filter_df = filter_df.dropna(subset=['sample_date']).copy()\n",
    "    \n",
    "    if len(valid_filter_df) == 0:\n",
    "        print(\"âŒ No valid sample dates found in filter data\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    removed_count = len(filter_df) - len(valid_filter_df)\n",
    "    if removed_count > 0:\n",
    "        print(f\"âš ï¸ Removed {removed_count} filter samples with invalid dates\")\n",
    "    \n",
    "    print(f\"ğŸ“Š Processing {len(valid_filter_df)} valid filter samples\")\n",
    "    \n",
    "    # Convert filter sample dates to corresponding 9am-to-9am measurement periods\n",
    "    filter_measurement_periods = pd.DatetimeIndex([\n",
    "        d.normalize() + pd.Timedelta(hours=9) - pd.Timedelta(days=1)\n",
    "        for d in valid_filter_df['sample_date']\n",
    "    ])\n",
    "    \n",
    "    # Handle timezone compatibility - IMPROVED VERSION\n",
    "    excellent_starts = excellent_periods['start_time']\n",
    "    \n",
    "    # Normalize timezones for comparison\n",
    "    if hasattr(excellent_starts, 'dt') and excellent_starts.dt.tz is not None:\n",
    "        # Excellent periods have timezone\n",
    "        if filter_measurement_periods.tz is None:\n",
    "            # Filter periods are naive - localize to same timezone as excellent periods\n",
    "            tz = excellent_starts.dt.tz\n",
    "            filter_measurement_periods = filter_measurement_periods.tz_localize(tz)\n",
    "        else:\n",
    "            # Both have timezones - convert filter periods to excellent periods timezone\n",
    "            tz = excellent_starts.dt.tz\n",
    "            filter_measurement_periods = filter_measurement_periods.tz_convert(tz)\n",
    "        \n",
    "        # Convert excellent_starts to same timezone if needed\n",
    "        excellent_starts_normalized = excellent_starts\n",
    "        \n",
    "    else:\n",
    "        # Excellent periods are naive\n",
    "        if filter_measurement_periods.tz is not None:\n",
    "            # Filter periods have timezone - convert to naive (remove timezone)\n",
    "            filter_measurement_periods = filter_measurement_periods.tz_localize(None)\n",
    "        \n",
    "        # Both are now naive\n",
    "        excellent_starts_normalized = excellent_starts\n",
    "    \n",
    "    print(f\"ğŸ• Filter periods timezone: {filter_measurement_periods.tz}\")\n",
    "    print(f\"ğŸ• Excellent periods timezone: {getattr(excellent_starts_normalized.dtype, 'tz', 'naive')}\")\n",
    "    \n",
    "    # Find overlap between filter measurement periods and excellent periods\n",
    "    overlap_periods = pd.DatetimeIndex(filter_measurement_periods).intersection(excellent_starts_normalized)\n",
    "    \n",
    "    print(f\"ğŸ“Š Found {len(overlap_periods)} overlapping excellent periods with filter samples\")\n",
    "    \n",
    "    if len(overlap_periods) == 0:\n",
    "        print(\"âš ï¸ No overlapping periods found\")\n",
    "        # Additional debugging\n",
    "        print(f\"ğŸ” Debug info:\")\n",
    "        print(f\"   Filter periods range: {filter_measurement_periods.min()} to {filter_measurement_periods.max()}\")\n",
    "        print(f\"   Excellent periods range: {excellent_starts_normalized.min()} to {excellent_starts_normalized.max()}\")\n",
    "        print(f\"   Sample filter periods: {list(filter_measurement_periods[:3])}\")\n",
    "        print(f\"   Sample excellent periods: {list(excellent_starts_normalized[:3])}\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Create merged dataset\n",
    "    merged_data = []\n",
    "    \n",
    "    for period_start in overlap_periods:\n",
    "        period_end = period_start + pd.Timedelta(days=1)\n",
    "        \n",
    "        # Find the corresponding filter sample\n",
    "        collection_date = period_start + pd.Timedelta(days=1)\n",
    "        \n",
    "        # Find matching filter sample\n",
    "        filter_matches = valid_filter_df[\n",
    "            valid_filter_df['sample_date'].dt.date == collection_date.date()\n",
    "        ]\n",
    "        \n",
    "        if len(filter_matches) == 0:\n",
    "            continue\n",
    "        \n",
    "        filter_data = filter_matches.iloc[0]  # Take first match if multiple\n",
    "        \n",
    "        # Extract aethalometer data for this period\n",
    "        aeth_stats = extract_aethalometer_stats(aethalometer_df, period_start, period_end, bc_column)\n",
    "        \n",
    "        if aeth_stats is None:\n",
    "            continue\n",
    "        \n",
    "        # Combine filter and aethalometer data\n",
    "        row_data = {\n",
    "            'dataset_source': dataset_name,\n",
    "            'period_start': period_start,\n",
    "            'period_end': period_end,\n",
    "            'collection_date': collection_date,\n",
    "            'filter_id': filter_data['filter_id'],\n",
    "            'EC_FTIR': filter_data.get('ec_ftir', np.nan),\n",
    "            'OC_FTIR': filter_data.get('oc_ftir', np.nan),\n",
    "            'Fabs': filter_data.get('fabs', np.nan),\n",
    "            'site': filter_data.get('site_code', SITE_CODE),\n",
    "            'wavelength': wavelength\n",
    "        }\n",
    "        \n",
    "        # Add aethalometer statistics with 'aeth_' prefix\n",
    "        for key, value in aeth_stats.items():\n",
    "            row_data[f'aeth_{key}'] = value\n",
    "        \n",
    "        merged_data.append(row_data)\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    merged_df = pd.DataFrame(merged_data)\n",
    "    \n",
    "    # Add derived variables if we have data\n",
    "    if len(merged_df) > 0:\n",
    "        # Mass Absorption Cross-section (MAC)\n",
    "        if 'EC_FTIR' in merged_df.columns and 'Fabs' in merged_df.columns:\n",
    "            merged_df['MAC'] = merged_df['Fabs'] / merged_df['EC_FTIR']\n",
    "        \n",
    "        # Add season information\n",
    "        merged_df['month'] = merged_df['collection_date'].dt.month\n",
    "        merged_df['season'] = merged_df['month'].apply(map_ethiopian_seasons)\n",
    "        \n",
    "        # Add date information\n",
    "        merged_df['date'] = merged_df['collection_date'].dt.date\n",
    "    \n",
    "    print(f\"âœ… Successfully merged {len(merged_df)} periods\")\n",
    "    \n",
    "    return merged_df\n",
    "\n",
    "# Perform merging for all datasets\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ”— TIME-MATCHED MERGING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "merged_datasets = {}\n",
    "\n",
    "if ftir_data is not None:\n",
    "    for dataset_name, aeth_df in aethalometer_datasets.items():\n",
    "        print(f\"\\nğŸ“Š Merging {dataset_name}...\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        excellent_periods = excellent_periods_dict[dataset_name]\n",
    "        \n",
    "        merged_df = merge_aethalometer_filter_data(\n",
    "            aethalometer_df=aeth_df,\n",
    "            filter_df=ftir_data,\n",
    "            excellent_periods=excellent_periods,\n",
    "            wavelength=WAVELENGTH,\n",
    "            dataset_name=dataset_name\n",
    "        )\n",
    "        \n",
    "        if len(merged_df) > 0:\n",
    "            merged_datasets[dataset_name] = merged_df\n",
    "            print(f\"âœ… {dataset_name}: {len(merged_df)} merged periods\")\n",
    "        else:\n",
    "            print(f\"âš ï¸ {dataset_name}: No merged periods found\")\n",
    "else:\n",
    "    print(\"âŒ Cannot perform merging - FTIR data not available\")\n",
    "\n",
    "print(f\"\\nğŸ¯ Merging completed: {len(merged_datasets)} datasets merged\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c943a820",
   "metadata": {},
   "source": [
    "## 7. Statistical Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5eca4210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” DIAGNOSTIC ANALYSIS: CSV vs PKL Data Overlap\n",
      "============================================================\n",
      "ğŸ“… FTIR date range: 2022-12-07 00:00:00 to 2024-05-12 00:00:00\n",
      "ğŸ“… FTIR 9am-to-9am periods: 2022-12-06 09:00:00 to 2024-05-11 09:00:00\n",
      "ğŸ“Š Total FTIR periods: 162\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "ğŸ“Š PKL_DATA ANALYSIS:\n",
      "   ğŸ“… Aethalometer date range: 2021-01-09 16:38:00 to 2025-06-26 23:18:00\n",
      "   ğŸ“… Excellent periods range: 2021-02-18 09:00:00 to 2025-06-25 09:00:00\n",
      "   ğŸ“Š Total excellent periods: 1036\n",
      "   ğŸ”— Overlapping periods: 148\n",
      "   ğŸ“… First overlap: 2022-12-06 09:00:00\n",
      "   ğŸ“… Last overlap: 2024-05-11 09:00:00\n",
      "   ------------------------------\n",
      "\n",
      "ğŸ“Š CSV_DATA ANALYSIS:\n",
      "   ğŸ“… Aethalometer date range: 2022-04-12 09:46:01+00:00 to 2024-08-20 09:01:00+00:00\n",
      "   ğŸ“… Excellent periods range: 2022-04-14 09:00:00+00:00 to 2024-08-19 09:00:00+00:00\n",
      "   ğŸ“Š Total excellent periods: 718\n",
      "   ğŸ”— Overlapping periods: 0\n",
      "   âš ï¸ NO OVERLAP FOUND\n",
      "   ğŸ“Š Time gap analysis:\n",
      "      FTIR periods: 2022-12-06 09:00:00+03:00 to 2024-05-11 09:00:00+03:00\n",
      "      Excellent periods: 2022-04-14 09:00:00+00:00 to 2024-08-19 09:00:00+00:00\n",
      "      ğŸ¤” Time ranges overlap, but no exact matches found\n",
      "   ------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Diagnostic analysis: Why does CSV data have 0 overlapping periods?\n",
    "print(\"ğŸ” DIAGNOSTIC ANALYSIS: CSV vs PKL Data Overlap\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if ftir_data is not None:\n",
    "    # Check FTIR data date range\n",
    "    valid_ftir_dates = ftir_data['sample_date'].dropna()\n",
    "    ftir_start, ftir_end = valid_ftir_dates.min(), valid_ftir_dates.max()\n",
    "    print(f\"ğŸ“… FTIR date range: {ftir_start} to {ftir_end}\")\n",
    "    \n",
    "    # Convert to 9am-to-9am periods\n",
    "    ftir_periods = pd.DatetimeIndex([\n",
    "        d.normalize() + pd.Timedelta(hours=9) - pd.Timedelta(days=1)\n",
    "        for d in valid_ftir_dates\n",
    "    ])\n",
    "    \n",
    "    print(f\"ğŸ“… FTIR 9am-to-9am periods: {ftir_periods.min()} to {ftir_periods.max()}\")\n",
    "    print(f\"ğŸ“Š Total FTIR periods: {len(ftir_periods)}\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\" * 40)\n",
    "    \n",
    "    for dataset_name, aeth_df in aethalometer_datasets.items():\n",
    "        print(f\"\\nğŸ“Š {dataset_name.upper()} ANALYSIS:\")\n",
    "        \n",
    "        # Check aethalometer data time range\n",
    "        aeth_start, aeth_end = aeth_df.index.min(), aeth_df.index.max()\n",
    "        print(f\"   ğŸ“… Aethalometer date range: {aeth_start} to {aeth_end}\")\n",
    "        \n",
    "        # Check excellent periods\n",
    "        excellent_periods = excellent_periods_dict[dataset_name]\n",
    "        excellent_start = excellent_periods['start_time'].min()\n",
    "        excellent_end = excellent_periods['start_time'].max()\n",
    "        print(f\"   ğŸ“… Excellent periods range: {excellent_start} to {excellent_end}\")\n",
    "        print(f\"   ğŸ“Š Total excellent periods: {len(excellent_periods)}\")\n",
    "        \n",
    "        # Check overlap with FTIR periods\n",
    "        if hasattr(excellent_periods['start_time'], 'dt') and excellent_periods['start_time'].dt.tz is not None:\n",
    "            if ftir_periods.tz is None:\n",
    "                ftir_periods_tz = ftir_periods.tz_localize('Africa/Addis_Ababa')\n",
    "            else:\n",
    "                ftir_periods_tz = ftir_periods\n",
    "        else:\n",
    "            ftir_periods_tz = ftir_periods\n",
    "        \n",
    "        overlap = pd.DatetimeIndex(ftir_periods_tz).intersection(excellent_periods['start_time'])\n",
    "        print(f\"   ğŸ”— Overlapping periods: {len(overlap)}\")\n",
    "        \n",
    "        if len(overlap) > 0:\n",
    "            print(f\"   ğŸ“… First overlap: {overlap.min()}\")\n",
    "            print(f\"   ğŸ“… Last overlap: {overlap.max()}\")\n",
    "        else:\n",
    "            print(\"   âš ï¸ NO OVERLAP FOUND\")\n",
    "            \n",
    "            # Check if there's any time overlap at all\n",
    "            ftir_min_tz = ftir_periods_tz.min()\n",
    "            ftir_max_tz = ftir_periods_tz.max()\n",
    "            excellent_min = excellent_periods['start_time'].min()\n",
    "            excellent_max = excellent_periods['start_time'].max()\n",
    "            \n",
    "            print(f\"   ğŸ“Š Time gap analysis:\")\n",
    "            print(f\"      FTIR periods: {ftir_min_tz} to {ftir_max_tz}\")\n",
    "            print(f\"      Excellent periods: {excellent_min} to {excellent_max}\")\n",
    "            \n",
    "            if ftir_max_tz < excellent_min:\n",
    "                gap = excellent_min - ftir_max_tz\n",
    "                print(f\"      âŒ FTIR ends {gap} before excellent periods start\")\n",
    "            elif ftir_min_tz > excellent_max:\n",
    "                gap = ftir_min_tz - excellent_max\n",
    "                print(f\"      âŒ FTIR starts {gap} after excellent periods end\")\n",
    "            else:\n",
    "                print(f\"      ğŸ¤” Time ranges overlap, but no exact matches found\")\n",
    "        \n",
    "        print(\"   \" + \"-\" * 30)\n",
    "\n",
    "else:\n",
    "    print(\"âŒ FTIR data not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db4163b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_correlation_statistics(merged_df: pd.DataFrame) -> Dict:\n",
    "    \"\"\"Calculate comprehensive correlation statistics\"\"\"\n",
    "    \n",
    "    if len(merged_df) == 0:\n",
    "        return {'error': 'No data available'}\n",
    "    \n",
    "    # Convert aethalometer BC to Âµg/mÂ³ (assuming it's in ng/mÂ³)\n",
    "    merged_df['aeth_bc_ug'] = merged_df['aeth_mean'] / 1000\n",
    "    \n",
    "    stats = {}\n",
    "    \n",
    "    # Basic sample info\n",
    "    stats['sample_info'] = {\n",
    "        'total_samples': len(merged_df),\n",
    "        'date_range': (merged_df['collection_date'].min(), merged_df['collection_date'].max())\n",
    "    }\n",
    "    \n",
    "    # Correlations\n",
    "    correlations = {}\n",
    "    if all(col in merged_df.columns for col in ['aeth_bc_ug', 'EC_FTIR']):\n",
    "        valid_data = merged_df[['aeth_bc_ug', 'EC_FTIR']].dropna()\n",
    "        if len(valid_data) > 1:\n",
    "            correlations['BC_EC'] = valid_data['aeth_bc_ug'].corr(valid_data['EC_FTIR'])\n",
    "    \n",
    "    if all(col in merged_df.columns for col in ['aeth_bc_ug', 'Fabs']):\n",
    "        valid_data = merged_df[['aeth_bc_ug', 'Fabs']].dropna()\n",
    "        if len(valid_data) > 1:\n",
    "            correlations['BC_Fabs'] = valid_data['aeth_bc_ug'].corr(valid_data['Fabs'])\n",
    "    \n",
    "    if all(col in merged_df.columns for col in ['EC_FTIR', 'Fabs']):\n",
    "        valid_data = merged_df[['EC_FTIR', 'Fabs']].dropna()\n",
    "        if len(valid_data) > 1:\n",
    "            correlations['EC_Fabs'] = valid_data['EC_FTIR'].corr(valid_data['Fabs'])\n",
    "    \n",
    "    stats['correlations'] = correlations\n",
    "    \n",
    "    # MAC statistics\n",
    "    if 'MAC' in merged_df.columns:\n",
    "        mac_data = merged_df['MAC'].dropna()\n",
    "        if len(mac_data) > 0:\n",
    "            # Filter out extreme MAC values (typical range: 5-15 mÂ²/g)\n",
    "            mac_filtered = mac_data[(mac_data > 0) & (mac_data < 30)]\n",
    "            \n",
    "            stats['mac_statistics'] = {\n",
    "                'count': len(mac_filtered),\n",
    "                'mean': mac_filtered.mean(),\n",
    "                'median': mac_filtered.median(),\n",
    "                'std': mac_filtered.std(),\n",
    "                'min': mac_filtered.min(),\n",
    "                'max': mac_filtered.max()\n",
    "            }\n",
    "    \n",
    "    # Seasonal statistics\n",
    "    if 'season' in merged_df.columns:\n",
    "        seasonal_stats = {}\n",
    "        for season in merged_df['season'].unique():\n",
    "            season_data = merged_df[merged_df['season'] == season]\n",
    "            if len(season_data) > 0:\n",
    "                seasonal_stats[season] = {\n",
    "                    'count': len(season_data),\n",
    "                    'mean_BC': season_data['aeth_bc_ug'].mean() if 'aeth_bc_ug' in season_data.columns else np.nan,\n",
    "                    'mean_EC': season_data['EC_FTIR'].mean() if 'EC_FTIR' in season_data.columns else np.nan,\n",
    "                    'mean_MAC': season_data['MAC'].mean() if 'MAC' in season_data.columns else np.nan\n",
    "                }\n",
    "        stats['seasonal_statistics'] = seasonal_stats\n",
    "    \n",
    "    return stats\n",
    "\n",
    "# Calculate statistics for all merged datasets\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ“Š STATISTICAL ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "analysis_results = {}\n",
    "\n",
    "for dataset_name, merged_df in merged_datasets.items():\n",
    "    print(f\"\\nğŸ“Š Analyzing {dataset_name}...\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    stats = calculate_correlation_statistics(merged_df)\n",
    "    analysis_results[dataset_name] = stats\n",
    "    \n",
    "    if 'error' not in stats:\n",
    "        # Display results\n",
    "        print(f\"ğŸ“ˆ Sample info:\")\n",
    "        print(f\"   Total samples: {stats['sample_info']['total_samples']}\")\n",
    "        print(f\"   Date range: {stats['sample_info']['date_range'][0]} to {stats['sample_info']['date_range'][1]}\")\n",
    "        \n",
    "        print(f\"\\nğŸ”— Correlations:\")\n",
    "        for corr_name, corr_value in stats['correlations'].items():\n",
    "            print(f\"   {corr_name}: {corr_value:.3f}\")\n",
    "        \n",
    "        if 'mac_statistics' in stats:\n",
    "            mac_stats = stats['mac_statistics']\n",
    "            print(f\"\\nğŸ“Š MAC Statistics:\")\n",
    "            print(f\"   Count: {mac_stats['count']}\")\n",
    "            print(f\"   Mean: {mac_stats['mean']:.2f} mÂ²/g\")\n",
    "            print(f\"   Median: {mac_stats['median']:.2f} mÂ²/g\")\n",
    "            print(f\"   Std: {mac_stats['std']:.2f} mÂ²/g\")\n",
    "        \n",
    "        if 'seasonal_statistics' in stats:\n",
    "            print(f\"\\nğŸŒ Seasonal Statistics:\")\n",
    "            for season, season_stats in stats['seasonal_statistics'].items():\n",
    "                print(f\"   {season}: n={season_stats['count']}, MAC={season_stats['mean_MAC']:.2f} mÂ²/g\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b56a24a",
   "metadata": {},
   "source": [
    "## 8. Visualization Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e614e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_correlation_analysis(merged_datasets: Dict[str, pd.DataFrame]):\n",
    "    \"\"\"Create correlation plots for all datasets\"\"\"\n",
    "    \n",
    "    if not merged_datasets:\n",
    "        print(\"No data available for plotting\")\n",
    "        return\n",
    "    \n",
    "    n_datasets = len(merged_datasets)\n",
    "    fig, axes = plt.subplots(2, n_datasets, figsize=(6*n_datasets, 12))\n",
    "    \n",
    "    if n_datasets == 1:\n",
    "        axes = axes.reshape(-1, 1)\n",
    "    \n",
    "    dataset_names = list(merged_datasets.keys())\n",
    "    \n",
    "    for idx, (dataset_name, merged_df) in enumerate(merged_datasets.items()):\n",
    "        if len(merged_df) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Convert BC to Âµg/mÂ³\n",
    "        merged_df['aeth_bc_ug'] = merged_df['aeth_mean'] / 1000\n",
    "        \n",
    "        # Plot 1: BC vs EC\n",
    "        ax1 = axes[0, idx]\n",
    "        valid_data = merged_df[['aeth_bc_ug', 'EC_FTIR']].dropna()\n",
    "        \n",
    "        if len(valid_data) > 1:\n",
    "            ax1.scatter(valid_data['EC_FTIR'], valid_data['aeth_bc_ug'], alpha=0.7, s=60)\n",
    "            \n",
    "            # Add trend line\n",
    "            z = np.polyfit(valid_data['EC_FTIR'], valid_data['aeth_bc_ug'], 1)\n",
    "            p = np.poly1d(z)\n",
    "            ax1.plot(valid_data['EC_FTIR'], p(valid_data['EC_FTIR']), \"r--\", alpha=0.8)\n",
    "            \n",
    "            # Add correlation\n",
    "            correlation = valid_data['aeth_bc_ug'].corr(valid_data['EC_FTIR'])\n",
    "            ax1.text(0.05, 0.95, f'r = {correlation:.3f}', transform=ax1.transAxes, \n",
    "                    bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"white\", alpha=0.8))\n",
    "        \n",
    "        ax1.set_xlabel('EC FTIR (Âµg/mÂ³)')\n",
    "        ax1.set_ylabel('Aethalometer BC (Âµg/mÂ³)')\n",
    "        ax1.set_title(f'{dataset_name}\\nBC vs EC')\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot 2: BC vs Fabs\n",
    "        ax2 = axes[1, idx]\n",
    "        valid_data_fabs = merged_df[['aeth_bc_ug', 'Fabs']].dropna()\n",
    "        \n",
    "        if len(valid_data_fabs) > 1:\n",
    "            ax2.scatter(valid_data_fabs['Fabs'], valid_data_fabs['aeth_bc_ug'], \n",
    "                       alpha=0.7, s=60, color='orange')\n",
    "            \n",
    "            # Add trend line\n",
    "            z = np.polyfit(valid_data_fabs['Fabs'], valid_data_fabs['aeth_bc_ug'], 1)\n",
    "            p = np.poly1d(z)\n",
    "            ax2.plot(valid_data_fabs['Fabs'], p(valid_data_fabs['Fabs']), \"r--\", alpha=0.8)\n",
    "            \n",
    "            # Add correlation\n",
    "            correlation = valid_data_fabs['aeth_bc_ug'].corr(valid_data_fabs['Fabs'])\n",
    "            ax2.text(0.05, 0.95, f'r = {correlation:.3f}', transform=ax2.transAxes,\n",
    "                    bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"white\", alpha=0.8))\n",
    "        \n",
    "        ax2.set_xlabel('Fabs (Mmâ»Â¹)')\n",
    "        ax2.set_ylabel('Aethalometer BC (Âµg/mÂ³)')\n",
    "        ax2.set_title(f'{dataset_name}\\nBC vs Fabs')\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return fig\n",
    "\n",
    "def plot_time_series_comparison(merged_datasets: Dict[str, pd.DataFrame]):\n",
    "    \"\"\"Create time series comparison plots\"\"\"\n",
    "    \n",
    "    if not merged_datasets:\n",
    "        print(\"No data available for time series plotting\")\n",
    "        return\n",
    "    \n",
    "    fig, axes = plt.subplots(3, 1, figsize=(16, 12), sharex=True)\n",
    "    \n",
    "    colors = ['blue', 'red', 'green', 'orange', 'purple']\n",
    "    \n",
    "    for idx, (dataset_name, merged_df) in enumerate(merged_datasets.items()):\n",
    "        if len(merged_df) == 0:\n",
    "            continue\n",
    "        \n",
    "        color = colors[idx % len(colors)]\n",
    "        merged_df_sorted = merged_df.sort_values('collection_date')\n",
    "        merged_df_sorted['aeth_bc_ug'] = merged_df_sorted['aeth_mean'] / 1000\n",
    "        \n",
    "        # EC FTIR\n",
    "        axes[0].plot(merged_df_sorted['collection_date'], merged_df_sorted['EC_FTIR'], \n",
    "                    'o-', label=f'{dataset_name}', color=color, alpha=0.7, markersize=4)\n",
    "        \n",
    "        # Aethalometer BC\n",
    "        axes[1].plot(merged_df_sorted['collection_date'], merged_df_sorted['aeth_bc_ug'], \n",
    "                    'o-', label=f'{dataset_name}', color=color, alpha=0.7, markersize=4)\n",
    "        \n",
    "        # MAC\n",
    "        if 'MAC' in merged_df_sorted.columns:\n",
    "            # Filter extreme MAC values for better visualization\n",
    "            mac_filtered = merged_df_sorted['MAC'][(merged_df_sorted['MAC'] > 0) & (merged_df_sorted['MAC'] < 30)]\n",
    "            dates_filtered = merged_df_sorted['collection_date'][(merged_df_sorted['MAC'] > 0) & (merged_df_sorted['MAC'] < 30)]\n",
    "            \n",
    "            axes[2].plot(dates_filtered, mac_filtered, \n",
    "                        'o-', label=f'{dataset_name}', color=color, alpha=0.7, markersize=4)\n",
    "    \n",
    "    axes[0].set_ylabel('EC FTIR (Âµg/mÂ³)')\n",
    "    axes[0].set_title('EC FTIR Time Series')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    axes[1].set_ylabel('Aethalometer BC (Âµg/mÂ³)')\n",
    "    axes[1].set_title('Aethalometer BC Time Series')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    axes[2].set_ylabel('MAC (mÂ²/g)')\n",
    "    axes[2].set_title('Mass Absorption Cross-section (MAC) Time Series')\n",
    "    axes[2].set_xlabel('Date')\n",
    "    axes[2].legend()\n",
    "    axes[2].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return fig\n",
    "\n",
    "def plot_seasonal_analysis(merged_datasets: Dict[str, pd.DataFrame]):\n",
    "    \"\"\"Create seasonal analysis plots\"\"\"\n",
    "    \n",
    "    # Combine all datasets for seasonal analysis\n",
    "    all_data = []\n",
    "    for dataset_name, merged_df in merged_datasets.items():\n",
    "        if len(merged_df) > 0:\n",
    "            df_copy = merged_df.copy()\n",
    "            df_copy['dataset'] = dataset_name\n",
    "            df_copy['aeth_bc_ug'] = df_copy['aeth_mean'] / 1000\n",
    "            all_data.append(df_copy)\n",
    "    \n",
    "    if not all_data:\n",
    "        print(\"No data available for seasonal analysis\")\n",
    "        return\n",
    "    \n",
    "    combined_df = pd.concat(all_data, ignore_index=True)\n",
    "    \n",
    "    # Create seasonal plots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # Season colors\n",
    "    season_colors = {\n",
    "        'Dry Season': 'gold',\n",
    "        'Belg Rainy Season': 'limegreen', \n",
    "        'Kiremt Rainy Season': 'royalblue'\n",
    "    }\n",
    "    \n",
    "    # Plot 1: MAC by season\n",
    "    if 'MAC' in combined_df.columns and 'season' in combined_df.columns:\n",
    "        mac_filtered = combined_df[(combined_df['MAC'] > 0) & (combined_df['MAC'] < 30)]\n",
    "        \n",
    "        for season in mac_filtered['season'].unique():\n",
    "            season_data = mac_filtered[mac_filtered['season'] == season]\n",
    "            axes[0, 0].scatter(season_data['season'], season_data['MAC'], \n",
    "                             color=season_colors.get(season, 'gray'), \n",
    "                             alpha=0.6, s=40, label=f'{season} (n={len(season_data)})')\n",
    "        \n",
    "        # Add box plot overlay\n",
    "        seasons = mac_filtered['season'].unique()\n",
    "        mac_by_season = [mac_filtered[mac_filtered['season'] == season]['MAC'] for season in seasons]\n",
    "        axes[0, 0].boxplot(mac_by_season, positions=range(len(seasons)), \n",
    "                          labels=seasons, alpha=0.3)\n",
    "        \n",
    "        axes[0, 0].set_ylabel('MAC (mÂ²/g)')\n",
    "        axes[0, 0].set_title('MAC by Ethiopian Season')\n",
    "        axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "        axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: BC vs EC by season\n",
    "    valid_data = combined_df[['aeth_bc_ug', 'EC_FTIR', 'season']].dropna()\n",
    "    \n",
    "    for season in valid_data['season'].unique():\n",
    "        season_data = valid_data[valid_data['season'] == season]\n",
    "        axes[0, 1].scatter(season_data['EC_FTIR'], season_data['aeth_bc_ug'],\n",
    "                          color=season_colors.get(season, 'gray'),\n",
    "                          alpha=0.6, s=40, label=f'{season} (n={len(season_data)})')\n",
    "    \n",
    "    axes[0, 1].set_xlabel('EC FTIR (Âµg/mÂ³)')\n",
    "    axes[0, 1].set_ylabel('Aethalometer BC (Âµg/mÂ³)')\n",
    "    axes[0, 1].set_title('BC vs EC by Season')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 3: Monthly trends\n",
    "    if 'month' in combined_df.columns:\n",
    "        monthly_stats = combined_df.groupby('month').agg({\n",
    "            'aeth_bc_ug': ['mean', 'count'],\n",
    "            'EC_FTIR': 'mean',\n",
    "            'MAC': 'mean'\n",
    "        }).round(2)\n",
    "        \n",
    "        months = monthly_stats.index\n",
    "        axes[1, 0].plot(months, monthly_stats['aeth_bc_ug']['mean'], 'bo-', label='BC')\n",
    "        axes[1, 0].plot(months, monthly_stats['EC_FTIR']['mean'], 'ro-', label='EC')\n",
    "        \n",
    "        axes[1, 0].set_xlabel('Month')\n",
    "        axes[1, 0].set_ylabel('Concentration (Âµg/mÂ³)')\n",
    "        axes[1, 0].set_title('Monthly BC and EC Trends')\n",
    "        axes[1, 0].legend()\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "        axes[1, 0].set_xticks(range(1, 13))\n",
    "    \n",
    "    # Plot 4: Data availability by season\n",
    "    if 'season' in combined_df.columns:\n",
    "        season_counts = combined_df['season'].value_counts()\n",
    "        colors_list = [season_colors.get(season, 'gray') for season in season_counts.index]\n",
    "        \n",
    "        axes[1, 1].bar(season_counts.index, season_counts.values, color=colors_list, alpha=0.7)\n",
    "        axes[1, 1].set_ylabel('Number of Samples')\n",
    "        axes[1, 1].set_title('Data Availability by Season')\n",
    "        axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Create all visualizations\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ“Š CREATING VISUALIZATIONS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if merged_datasets:\n",
    "    print(\"\\nğŸ“ˆ Creating correlation analysis plots...\")\n",
    "    fig_corr = plot_correlation_analysis(merged_datasets)\n",
    "    \n",
    "    print(\"\\nğŸ“ˆ Creating time series comparison...\")\n",
    "    fig_ts = plot_time_series_comparison(merged_datasets)\n",
    "    \n",
    "    print(\"\\nğŸ“ˆ Creating seasonal analysis...\")\n",
    "    fig_seasonal = plot_seasonal_analysis(merged_datasets)\n",
    "else:\n",
    "    print(\"âš ï¸ No merged datasets available for visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8582eda5",
   "metadata": {},
   "source": [
    "## 9. Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd3a3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_results(merged_datasets: Dict[str, pd.DataFrame], \n",
    "                  analysis_results: Dict,\n",
    "                  output_dir: str):\n",
    "    \"\"\"Export all results to files\"\"\"\n",
    "    \n",
    "    print(f\"\\nğŸ“¤ Exporting results to: {output_dir}\")\n",
    "    \n",
    "    # Create output directory\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Export individual merged datasets\n",
    "    for dataset_name, merged_df in merged_datasets.items():\n",
    "        csv_path = os.path.join(output_dir, f'merged_{dataset_name}.csv')\n",
    "        merged_df.to_csv(csv_path, index=False)\n",
    "        print(f\"âœ… Exported {dataset_name}: {csv_path}\")\n",
    "    \n",
    "    # Export combined dataset if multiple\n",
    "    if len(merged_datasets) > 1:\n",
    "        combined_df = pd.concat(merged_datasets.values(), ignore_index=True)\n",
    "        combined_path = os.path.join(output_dir, 'merged_combined_all_datasets.csv')\n",
    "        combined_df.to_csv(combined_path, index=False)\n",
    "        print(f\"âœ… Exported combined dataset: {combined_path}\")\n",
    "    \n",
    "    # Export analysis results as JSON\n",
    "    import json\n",
    "    \n",
    "    # Convert numpy types to Python types for JSON serialization\n",
    "    def convert_for_json(obj):\n",
    "        if isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        elif isinstance(obj, pd.Timestamp):\n",
    "            return obj.isoformat()\n",
    "        elif hasattr(obj, 'date'):\n",
    "            return obj.isoformat()\n",
    "        return obj\n",
    "    \n",
    "    json_results = {}\n",
    "    for dataset_name, stats in analysis_results.items():\n",
    "        json_results[dataset_name] = {}\n",
    "        for key, value in stats.items():\n",
    "            if isinstance(value, dict):\n",
    "                json_results[dataset_name][key] = {k: convert_for_json(v) for k, v in value.items()}\n",
    "            else:\n",
    "                json_results[dataset_name][key] = convert_for_json(value)\n",
    "    \n",
    "    json_path = os.path.join(output_dir, 'analysis_results.json')\n",
    "    with open(json_path, 'w') as f:\n",
    "        json.dump(json_results, f, indent=2)\n",
    "    print(f\"âœ… Exported analysis results: {json_path}\")\n",
    "    \n",
    "    # Create summary report\n",
    "    summary_path = os.path.join(output_dir, 'pipeline_summary.txt')\n",
    "    with open(summary_path, 'w') as f:\n",
    "        f.write(\"AETHALOMETER-FTIR/HIPS PIPELINE SUMMARY\\n\")\n",
    "        f.write(\"=\" * 50 + \"\\n\\n\")\n",
    "        \n",
    "        f.write(f\"Analysis Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "        f.write(f\"Site: {SITE_CODE}\\n\")\n",
    "        f.write(f\"Wavelength: {WAVELENGTH}\\n\")\n",
    "        f.write(f\"Quality Threshold: {QUALITY_THRESHOLD} minutes\\n\\n\")\n",
    "        \n",
    "        f.write(\"DATASETS PROCESSED:\\n\")\n",
    "        f.write(\"-\" * 20 + \"\\n\")\n",
    "        for dataset_name, merged_df in merged_datasets.items():\n",
    "            f.write(f\"{dataset_name}: {len(merged_df)} merged periods\\n\")\n",
    "        \n",
    "        f.write(f\"\\nFTIR/HIPS DATA:\\n\")\n",
    "        f.write(\"-\" * 20 + \"\\n\")\n",
    "        if ftir_data is not None:\n",
    "            f.write(f\"Total filter samples: {len(ftir_data)}\\n\")\n",
    "            f.write(f\"Date range: {ftir_data['sample_date'].min()} to {ftir_data['sample_date'].max()}\\n\")\n",
    "        \n",
    "        f.write(f\"\\nKEY CORRELATIONS:\\n\")\n",
    "        f.write(\"-\" * 20 + \"\\n\")\n",
    "        for dataset_name, stats in analysis_results.items():\n",
    "            if 'correlations' in stats:\n",
    "                f.write(f\"{dataset_name}:\\n\")\n",
    "                for corr_name, corr_value in stats['correlations'].items():\n",
    "                    f.write(f\"  {corr_name}: {corr_value:.3f}\\n\")\n",
    "                f.write(\"\\n\")\n",
    "    \n",
    "    print(f\"âœ… Exported summary report: {summary_path}\")\n",
    "    print(f\"\\nğŸ¯ Export completed! All results saved to: {output_dir}\")\n",
    "\n",
    "# Export all results\n",
    "export_results(merged_datasets, analysis_results, OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0e17be",
   "metadata": {},
   "source": [
    "## 10. Pipeline Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96e0b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pipeline_summary():\n",
    "    \"\"\"Create a comprehensive summary of the pipeline results\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ğŸ¯ PIPELINE SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(f\"ğŸ“Š Configuration:\")\n",
    "    print(f\"   Site: {SITE_CODE}\")\n",
    "    print(f\"   Wavelength: {WAVELENGTH}\")\n",
    "    print(f\"   Quality threshold: {QUALITY_THRESHOLD} minutes\")\n",
    "    print(f\"   Output directory: {OUTPUT_DIR}\")\n",
    "    \n",
    "    print(f\"\\nğŸ“ Data Loading:\")\n",
    "    print(f\"   Aethalometer files processed: {len(aethalometer_datasets)}\")\n",
    "    for name, summary in aethalometer_summaries.items():\n",
    "        print(f\"     {name}: {summary['shape'][0]:,} rows ({summary['file_type']})\")\n",
    "    \n",
    "    if ftir_data is not None:\n",
    "        print(f\"   FTIR/HIPS samples: {len(ftir_data)}\")\n",
    "        print(f\"   FTIR date range: {ftir_data['sample_date'].min()} to {ftir_data['sample_date'].max()}\")\n",
    "    \n",
    "    print(f\"\\nğŸ” Quality Assessment:\")\n",
    "    for dataset_name, excellent_periods in excellent_periods_dict.items():\n",
    "        print(f\"   {dataset_name}: {len(excellent_periods)} excellent periods\")\n",
    "    \n",
    "    print(f\"\\nğŸ”— Merging Results:\")\n",
    "    total_merged = sum(len(df) for df in merged_datasets.values())\n",
    "    print(f\"   Total merged periods: {total_merged}\")\n",
    "    for name, merged_df in merged_datasets.items():\n",
    "        print(f\"     {name}: {len(merged_df)} periods\")\n",
    "    \n",
    "    print(f\"\\nğŸ“ˆ Key Findings:\")\n",
    "    for dataset_name, stats in analysis_results.items():\n",
    "        if 'correlations' in stats and stats['correlations']:\n",
    "            print(f\"   {dataset_name}:\")\n",
    "            for corr_name, corr_value in stats['correlations'].items():\n",
    "                print(f\"     {corr_name}: {corr_value:.3f}\")\n",
    "            \n",
    "            if 'mac_statistics' in stats:\n",
    "                mac_mean = stats['mac_statistics']['mean']\n",
    "                print(f\"     Mean MAC: {mac_mean:.2f} mÂ²/g\")\n",
    "    \n",
    "    print(f\"\\nâœ… Pipeline Status: COMPLETED SUCCESSFULLY\")\n",
    "    print(f\"ğŸ“¤ Results exported to: {OUTPUT_DIR}\")\n",
    "    \n",
    "    if merged_datasets:\n",
    "        print(f\"\\nğŸ’¡ Next Steps:\")\n",
    "        print(f\"   â€¢ Review correlation plots for method comparison\")\n",
    "        print(f\"   â€¢ Examine seasonal patterns in the data\")\n",
    "        print(f\"   â€¢ Consider extending analysis to other wavelengths\")\n",
    "        print(f\"   â€¢ Investigate periods with high/low correlations\")\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "\n",
    "# Create final summary\n",
    "create_pipeline_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f4d93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final cell - display key results for quick reference\n",
    "if merged_datasets:\n",
    "    print(\"ğŸ‰ PIPELINE COMPLETED SUCCESSFULLY!\")\n",
    "    print(\"\\nğŸ“Š Quick Results Summary:\")\n",
    "    \n",
    "    for dataset_name, merged_df in merged_datasets.items():\n",
    "        print(f\"\\n{dataset_name}:\")\n",
    "        print(f\"  â€¢ Merged periods: {len(merged_df)}\")\n",
    "        \n",
    "        if len(merged_df) > 0:\n",
    "            # Quick correlation\n",
    "            merged_df['aeth_bc_ug'] = merged_df['aeth_mean'] / 1000\n",
    "            \n",
    "            if 'EC_FTIR' in merged_df.columns:\n",
    "                bc_ec_corr = merged_df['aeth_bc_ug'].corr(merged_df['EC_FTIR'])\n",
    "                print(f\"  â€¢ BC-EC correlation: {bc_ec_corr:.3f}\")\n",
    "            \n",
    "            if 'MAC' in merged_df.columns:\n",
    "                mac_mean = merged_df['MAC'][(merged_df['MAC'] > 0) & (merged_df['MAC'] < 30)].mean()\n",
    "                print(f\"  â€¢ Mean MAC: {mac_mean:.2f} mÂ²/g\")\n",
    "    \n",
    "    print(f\"\\nğŸ“ All results saved to: {OUTPUT_DIR}\")\n",
    "else:\n",
    "    print(\"âš ï¸ No successful merging occurred. Check configuration and data availability.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
