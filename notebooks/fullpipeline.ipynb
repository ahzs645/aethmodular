{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17ca5e71",
   "metadata": {},
   "source": [
    "# Complete Aethalometer-FTIR/HIPS Data Pipeline\n",
    "\n",
    "This notebook provides a complete pipeline for loading, processing, and merging:\n",
    "- **Aethalometer data** (PKL and CSV formats)\n",
    "- **FTIR/HIPS filter data** (SQLite database)\n",
    "- **Time-matched merging** with quality assessment\n",
    "- **Statistical analysis** and visualization\n",
    "\n",
    "## Pipeline Overview:\n",
    "1. **Setup and Configuration**\n",
    "2. **Database Loading** (FTIR/HIPS data)\n",
    "3. **Aethalometer Loading** (PKL and CSV files)\n",
    "4. **Quality Assessment** (9am-9am period analysis)\n",
    "5. **Time-Matched Merging**\n",
    "6. **Statistical Analysis**\n",
    "7. **Visualization and Export**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1dde30",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "863c1d21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Aethalometer-FTIR/HIPS Pipeline with Modular System\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Core imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sqlite3\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Dict, List, Union, Optional, Any, Tuple\n",
    "import pickle\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add src directory to path for modular system access\n",
    "src_path = str(Path('../src').resolve())\n",
    "if src_path not in sys.path:\n",
    "    sys.path.insert(0, src_path)\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "print(\"üöÄ Aethalometer-FTIR/HIPS Pipeline with Modular System\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "580f07db",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2. Configuration Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fbbafc37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Configuration:\n",
      "   Site: ETAD\n",
      "   Wavelength: Red\n",
      "   Output format: jpl\n",
      "   Quality threshold: 10 minutes\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CONFIGURATION SECTION - UPDATE THESE PATHS FOR YOUR DATA\n",
    "# =============================================================================\n",
    "\n",
    "# File paths - UPDATE THESE TO YOUR ACTUAL FILE PATHS\n",
    "AETHALOMETER_FILES = {\n",
    "    'pkl_data': \"/Users/ahzs645/Library/CloudStorage/GoogleDrive-ahzs645@gmail.com/My Drive/University/Research/Grad/UC Davis Ann/NASA MAIA/Data/Aethelometry Data/Kyan Data/Mergedcleaned and uncleaned MA350 data20250707030704/df_uncleaned_Jacros_API_and_OG.pkl\",\n",
    "    'csv_data': \"/Users/ahzs645/Library/CloudStorage/GoogleDrive-ahzs645@gmail.com/My Drive/University/Research/Grad/UC Davis Ann/NASA MAIA/Data/Aethelometry Data/Raw/Jacros_MA350_1-min_2022-2024_Cleaned.csv\"\n",
    "}\n",
    "\n",
    "FTIR_DB_PATH = \"/Users/ahzs645/Library/CloudStorage/GoogleDrive-ahzs645@gmail.com/My Drive/University/Research/Grad/UC Davis Ann/NASA MAIA/Data/EC-HIPS-Aeth Comparison/Data/Original Data/Combined Database/spartan_ftir_hips.db\"\n",
    "\n",
    "# Analysis parameters\n",
    "SITE_CODE = 'ETAD'\n",
    "WAVELENGTH = 'Red'  # Options: 'Red', 'Blue', 'Green', 'UV', 'IR'\n",
    "QUALITY_THRESHOLD = 10  # Maximum missing minutes for \"excellent\" quality\n",
    "OUTPUT_DIR = \"outputs\"\n",
    "OUTPUT_FORMAT = \"jpl\"  # 'jpl' or 'standard' format\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"üìä Configuration:\")\n",
    "print(f\"   Site: {SITE_CODE}\")\n",
    "print(f\"   Wavelength: {WAVELENGTH}\")\n",
    "print(f\"   Output format: {OUTPUT_FORMAT}\")\n",
    "print(f\"   Quality threshold: {QUALITY_THRESHOLD} minutes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "034f7692",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Setting up modular system...\n",
      "‚úÖ Added /Users/ahzs645/Github/aethmodular/src to Python path\n",
      "‚úÖ Aethalometer loaders imported\n",
      "‚úÖ Database loader imported\n",
      "‚úÖ Black carbon analyzer imported\n",
      "‚úÖ Source apportionment analyzer imported\n",
      "‚úÖ Aethalometer plotter imported\n",
      "‚úÖ Plotting style configured\n",
      "‚úÖ File I/O utilities imported\n",
      "‚úÖ Data validation functions imported\n",
      "‚úÖ Successfully imported 12 components\n",
      "\n",
      "üéâ Modular system available!\n",
      "üìä Available components:\n",
      "   loaders: ['AethalometerPKLLoader', 'AethalometerCSVLoader', 'load_aethalometer_data', 'FTIRHIPSLoader']\n",
      "   analysis: ['BlackCarbonAnalyzer', 'SourceApportionmentAnalyzer']\n",
      "   utils: ['AethalometerPlotter', 'ensure_output_directory', 'validate_columns_exist', 'get_valid_data_mask', 'validate_sample_size', 'check_data_range']\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# MODULAR SYSTEM IMPORTS AND SETUP\n",
    "# =============================================================================\n",
    "\n",
    "def setup_modular_system():\n",
    "    \"\"\"Import and setup the modular aethalometer system\"\"\"\n",
    "    \n",
    "    print(\"üì¶ Setting up modular system...\")\n",
    "    \n",
    "    # Add the src directory to Python path\n",
    "    import sys\n",
    "    import os\n",
    "    \n",
    "    # Get the parent directory (aethmodular) and add src to path\n",
    "    notebook_dir = os.path.dirname(os.path.abspath(''))\n",
    "    if notebook_dir.endswith('notebooks'):\n",
    "        project_root = os.path.dirname(notebook_dir)\n",
    "    else:\n",
    "        project_root = notebook_dir\n",
    "    \n",
    "    src_path = os.path.join(project_root, 'src')\n",
    "    if src_path not in sys.path:\n",
    "        sys.path.insert(0, src_path)\n",
    "    print(f\"‚úÖ Added {src_path} to Python path\")\n",
    "    \n",
    "    # Dictionary to store successfully imported components\n",
    "    imported_components = {\n",
    "        'loaders': {},\n",
    "        'analysis': {},\n",
    "        'utils': {}\n",
    "    }\n",
    "    \n",
    "    # Try importing aethalometer loaders\n",
    "    try:\n",
    "        from data.loaders.aethalometer import (\n",
    "            AethalometerPKLLoader, \n",
    "            AethalometerCSVLoader,\n",
    "            load_aethalometer_data\n",
    "        )\n",
    "        imported_components['loaders'].update({\n",
    "            'AethalometerPKLLoader': AethalometerPKLLoader,\n",
    "            'AethalometerCSVLoader': AethalometerCSVLoader,\n",
    "            'load_aethalometer_data': load_aethalometer_data\n",
    "        })\n",
    "        print(\"‚úÖ Aethalometer loaders imported\")\n",
    "    except ImportError as e:\n",
    "        print(f\"‚ö†Ô∏è Aethalometer loaders failed: {e}\")\n",
    "    \n",
    "    # Try importing database loader\n",
    "    try:\n",
    "        from data.loaders.database import FTIRHIPSLoader\n",
    "        imported_components['loaders']['FTIRHIPSLoader'] = FTIRHIPSLoader\n",
    "        print(\"‚úÖ Database loader imported\")\n",
    "    except ImportError as e:\n",
    "        print(f\"‚ö†Ô∏è Database loader failed: {e}\")\n",
    "    \n",
    "    # Try importing analysis modules\n",
    "    try:\n",
    "        from analysis.bc.black_carbon_analyzer import BlackCarbonAnalyzer\n",
    "        imported_components['analysis']['BlackCarbonAnalyzer'] = BlackCarbonAnalyzer\n",
    "        print(\"‚úÖ Black carbon analyzer imported\")\n",
    "    except ImportError as e:\n",
    "        print(f\"‚ö†Ô∏è Black carbon analyzer failed: {e}\")\n",
    "    \n",
    "    try:\n",
    "        from analysis.bc.source_apportionment import SourceApportionmentAnalyzer\n",
    "        imported_components['analysis']['SourceApportionmentAnalyzer'] = SourceApportionmentAnalyzer\n",
    "        print(\"‚úÖ Source apportionment analyzer imported\")\n",
    "    except ImportError as e:\n",
    "        print(f\"‚ö†Ô∏è Source apportionment analyzer failed: {e}\")\n",
    "    \n",
    "    # Try importing visualization utilities\n",
    "    try:\n",
    "        from utils.plotting import AethalometerPlotter\n",
    "        imported_components['utils']['AethalometerPlotter'] = AethalometerPlotter\n",
    "        print(\"‚úÖ Aethalometer plotter imported\")\n",
    "    except ImportError as e:\n",
    "        print(f\"‚ö†Ô∏è Aethalometer plotter failed: {e}\")\n",
    "    \n",
    "    try:\n",
    "        from config.plotting import setup_plotting_style\n",
    "        setup_plotting_style()\n",
    "        print(\"‚úÖ Plotting style configured\")\n",
    "    except ImportError as e:\n",
    "        print(f\"‚ö†Ô∏è Plotting style config failed: {e}\")\n",
    "    \n",
    "    # Try importing utility functions\n",
    "    try:\n",
    "        from utils.file_io import ensure_output_directory\n",
    "        imported_components['utils']['ensure_output_directory'] = ensure_output_directory\n",
    "        print(\"‚úÖ File I/O utilities imported\")\n",
    "    except ImportError as e:\n",
    "        print(f\"‚ö†Ô∏è File I/O utilities failed: {e}\")\n",
    "    \n",
    "    try:\n",
    "        # Import validation functions from data processors (only existing functions)\n",
    "        from data.processors.validation import (\n",
    "            validate_columns_exist, \n",
    "            get_valid_data_mask,\n",
    "            validate_sample_size,\n",
    "            check_data_range\n",
    "        )\n",
    "        imported_components['utils'].update({\n",
    "            'validate_columns_exist': validate_columns_exist,\n",
    "            'get_valid_data_mask': get_valid_data_mask,\n",
    "            'validate_sample_size': validate_sample_size,\n",
    "            'check_data_range': check_data_range\n",
    "        })\n",
    "        print(\"‚úÖ Data validation functions imported\")\n",
    "    except ImportError as e:\n",
    "        print(f\"‚ö†Ô∏è Data validation failed: {e}\")\n",
    "    \n",
    "    # Return what we successfully imported\n",
    "    if any(imported_components.values()):\n",
    "        success_count = sum(len(v) for v in imported_components.values())\n",
    "        print(f\"‚úÖ Successfully imported {success_count} components\")\n",
    "        return imported_components\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No modular components could be imported\")\n",
    "        print(f\"üìç Current working directory: {os.getcwd()}\")\n",
    "        print(f\"üìç Python path includes: {sys.path[:3]}...\")\n",
    "        return None\n",
    "\n",
    "# Setup the modular system\n",
    "modular_components = setup_modular_system()\n",
    "MODULAR_AVAILABLE = modular_components is not None\n",
    "\n",
    "if MODULAR_AVAILABLE:\n",
    "    print(f\"\\nüéâ Modular system available!\")\n",
    "    print(f\"üìä Available components:\")\n",
    "    for category, components in modular_components.items():\n",
    "        if components:\n",
    "            print(f\"   {category}: {list(components.keys())}\")\n",
    "else:\n",
    "    print(\"\\nüìù Falling back to basic functionality...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3d5e8be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ENHANCED AETHALOMETER LOADING WITH MODULAR SYSTEM\n",
    "# =============================================================================\n",
    "\n",
    "def load_aethalometer_with_modular_system(file_path: str, \n",
    "                                        output_format: str = \"jpl\",\n",
    "                                        site_filter: Optional[str] = None) -> Tuple[Optional[pd.DataFrame], Dict]:\n",
    "    \"\"\"\n",
    "    Load aethalometer data using the modular system\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    file_path : str\n",
    "        Path to aethalometer data file (.pkl or .csv)\n",
    "    output_format : str\n",
    "        Output format ('jpl' or 'standard')\n",
    "    site_filter : str, optional\n",
    "        Filter data by site\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple\n",
    "        (DataFrame, summary_dict)\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"üìÅ Loading with modular system: {Path(file_path).name}\")\n",
    "    print(f\"üîß Output format: {output_format}\")\n",
    "    \n",
    "    if not MODULAR_AVAILABLE:\n",
    "        raise ImportError(\"Modular system not available\")\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"‚ùå File not found: {file_path}\")\n",
    "        return None, {}\n",
    "    \n",
    "    try:\n",
    "        # Use the unified loading function from modular system\n",
    "        load_function = modular_components['loaders']['load_aethalometer_data']\n",
    "        \n",
    "        df = load_function(\n",
    "            file_path,\n",
    "            output_format=output_format,\n",
    "            site_filter=site_filter,\n",
    "            set_datetime_index=True\n",
    "        )\n",
    "        \n",
    "        if df is None or len(df) == 0:\n",
    "            print(f\"‚ùå No data loaded from {file_path}\")\n",
    "            return None, {}\n",
    "        \n",
    "        # Generate comprehensive summary\n",
    "        summary = {\n",
    "            'file_name': Path(file_path).name,\n",
    "            'file_type': Path(file_path).suffix,\n",
    "            'format': output_format,\n",
    "            'shape': df.shape,\n",
    "            'columns': len(df.columns),\n",
    "            'memory_mb': df.memory_usage(deep=True).sum() / 1024**2,\n",
    "            'bc_columns': [col for col in df.columns if '.BCc' in col or 'BC' in col.upper()],\n",
    "            'atn_columns': [col for col in df.columns if '.ATN' in col or 'ATN' in col.upper()],\n",
    "            'time_range': (df.index.min(), df.index.max()) if hasattr(df.index, 'min') else None,\n",
    "            'missing_data_pct': (df.isnull().sum().sum() / df.size) * 100,\n",
    "            'has_datetime_index': isinstance(df.index, pd.DatetimeIndex)\n",
    "        }\n",
    "        \n",
    "        print(f\"‚úÖ Successfully loaded: {df.shape[0]:,} rows √ó {df.shape[1]} columns\")\n",
    "        print(f\"üìä Format: {output_format}\")\n",
    "        print(f\"üìä Memory usage: {summary['memory_mb']:.2f} MB\")\n",
    "        print(f\"üßÆ BC columns found: {len(summary['bc_columns'])}\")\n",
    "        print(f\"üìà ATN columns found: {len(summary['atn_columns'])}\")\n",
    "        \n",
    "        if summary['time_range']:\n",
    "            print(f\"üìÖ Time range: {summary['time_range'][0]} to {summary['time_range'][1]}\")\n",
    "        \n",
    "        return df, summary\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading with modular system: {e}\")\n",
    "        return None, {}\n",
    "\n",
    "def load_aethalometer_fallback(file_path: str) -> Tuple[Optional[pd.DataFrame], Dict]:\n",
    "    \"\"\"\n",
    "    Fallback direct loading method when modular system fails\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"üîÑ Using fallback loading for: {Path(file_path).name}\")\n",
    "    \n",
    "    try:\n",
    "        file_ext = Path(file_path).suffix.lower()\n",
    "        \n",
    "        if file_ext == '.pkl':\n",
    "            with open(file_path, 'rb') as f:\n",
    "                df = pickle.load(f)\n",
    "            print(f\"‚úÖ Direct PKL load: {df.shape[0]:,} rows √ó {df.shape[1]} columns\")\n",
    "            \n",
    "        elif file_ext == '.csv':\n",
    "            df = pd.read_csv(file_path)\n",
    "            print(f\"‚úÖ Direct CSV load: {df.shape[0]:,} rows √ó {df.shape[1]} columns\")\n",
    "            \n",
    "            # Handle datetime conversion for CSV\n",
    "            if 'Time (UTC)' in df.columns:\n",
    "                df['Time (UTC)'] = pd.to_datetime(df['Time (UTC)'], utc=True)\n",
    "                df['Time (Local)'] = df['Time (UTC)'].dt.tz_convert('Africa/Addis_Ababa')\n",
    "                df.set_index('Time (Local)', inplace=True)\n",
    "            elif 'datetime' in df.columns:\n",
    "                df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "                df.set_index('datetime', inplace=True)\n",
    "        \n",
    "        else:\n",
    "            print(f\"‚ùå Unsupported file format: {file_ext}\")\n",
    "            return None, {}\n",
    "        \n",
    "        # Ensure datetime index\n",
    "        if not isinstance(df.index, pd.DatetimeIndex):\n",
    "            # Try to convert index or find datetime column\n",
    "            datetime_cols = ['datetime', 'timestamp', 'Time', 'Date']\n",
    "            for col in datetime_cols:\n",
    "                if col in df.columns:\n",
    "                    df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "                    df = df[df[col].notna()]\n",
    "                    df.set_index(col, inplace=True)\n",
    "                    break\n",
    "        \n",
    "        # Generate basic summary\n",
    "        summary = {\n",
    "            'file_name': Path(file_path).name,\n",
    "            'file_type': Path(file_path).suffix,\n",
    "            'format': 'fallback',\n",
    "            'shape': df.shape,\n",
    "            'bc_columns': [col for col in df.columns if 'BC' in col.upper()],\n",
    "            'atn_columns': [col for col in df.columns if 'ATN' in col.upper()],\n",
    "            'time_range': (df.index.min(), df.index.max()) if isinstance(df.index, pd.DatetimeIndex) else None,\n",
    "            'has_datetime_index': isinstance(df.index, pd.DatetimeIndex)\n",
    "        }\n",
    "        \n",
    "        return df, summary\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Fallback loading failed: {e}\")\n",
    "        return None, {}\n",
    "\n",
    "def load_aethalometer_robust(file_path: str, \n",
    "                           output_format: str = \"jpl\",\n",
    "                           site_filter: Optional[str] = None) -> Tuple[Optional[pd.DataFrame], Dict]:\n",
    "    \"\"\"\n",
    "    Robust aethalometer loading with modular system + fallback\n",
    "    \"\"\"\n",
    "    \n",
    "    # Try modular system first\n",
    "    if MODULAR_AVAILABLE:\n",
    "        df, summary = load_aethalometer_with_modular_system(file_path, output_format, site_filter)\n",
    "        if df is not None:\n",
    "            return df, summary\n",
    "        print(\"‚ö†Ô∏è Modular system failed, trying fallback...\")\n",
    "    \n",
    "    # Fallback to direct loading\n",
    "    df, summary = load_aethalometer_fallback(file_path)\n",
    "    \n",
    "    if df is not None:\n",
    "        print(\"‚úÖ Fallback loading successful\")\n",
    "    else:\n",
    "        print(\"‚ùå All loading methods failed\")\n",
    "    \n",
    "    return df, summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "89fd0e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ENHANCED FTIR/HIPS LOADING WITH MODULAR SYSTEM\n",
    "# =============================================================================\n",
    "\n",
    "def load_ftir_hips_with_modular_system(db_path: str, site_code: str) -> Optional[pd.DataFrame]:\n",
    "    \"\"\"Load FTIR/HIPS data using modular system\"\"\"\n",
    "    \n",
    "    print(f\"üóÉÔ∏è Loading FTIR/HIPS data with modular system...\")\n",
    "    \n",
    "    if not MODULAR_AVAILABLE:\n",
    "        print(\"‚ö†Ô∏è Modular system not available for FTIR loading\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        FTIRHIPSLoader = modular_components['loaders']['FTIRHIPSLoader']\n",
    "        \n",
    "        loader = FTIRHIPSLoader(db_path)\n",
    "        \n",
    "        # Get available sites\n",
    "        available_sites = loader.get_available_sites()\n",
    "        print(f\"üìä Available sites: {available_sites}\")\n",
    "        \n",
    "        if site_code not in available_sites:\n",
    "            print(f\"‚ö†Ô∏è Site '{site_code}' not found in database\")\n",
    "            return None\n",
    "        \n",
    "        # Load data\n",
    "        df = loader.load(site_code)\n",
    "        \n",
    "        if len(df) > 0:\n",
    "            print(f\"‚úÖ Loaded {len(df)} FTIR/HIPS samples\")\n",
    "            print(f\"üìÖ Date range: {df['sample_date'].min()} to {df['sample_date'].max()}\")\n",
    "            return df\n",
    "        else:\n",
    "            print(\"‚ùå No FTIR/HIPS data found\")\n",
    "            return None\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Modular FTIR loading failed: {e}\")\n",
    "        return None\n",
    "\n",
    "def load_ftir_hips_fallback(db_path: str, site_code: str) -> Optional[pd.DataFrame]:\n",
    "    \"\"\"Fallback FTIR/HIPS loading\"\"\"\n",
    "    \n",
    "    print(f\"üîÑ Using fallback FTIR loading...\")\n",
    "    \n",
    "    try:\n",
    "        conn = sqlite3.connect(db_path)\n",
    "        \n",
    "        # Try standard query\n",
    "        query = \"\"\"\n",
    "        SELECT \n",
    "            f.filter_id, f.sample_date, f.site_code, f.filter_type,\n",
    "            m.volume_m3, m.ec_ftir, m.ec_ftir_mdl, m.oc_ftir, m.oc_ftir_mdl,\n",
    "            m.fabs, m.fabs_mdl, m.fabs_uncertainty, m.ftir_batch_id\n",
    "        FROM filters f\n",
    "        JOIN ftir_sample_measurements m ON f.filter_id = m.filter_id\n",
    "        WHERE f.site_code = ?\n",
    "        ORDER BY f.sample_date\n",
    "        \"\"\"\n",
    "        \n",
    "        df = pd.read_sql_query(query, conn, params=(site_code,))\n",
    "        conn.close()\n",
    "        \n",
    "        if len(df) > 0:\n",
    "            df['sample_date'] = pd.to_datetime(df['sample_date'])\n",
    "            print(f\"‚úÖ Fallback FTIR load: {len(df)} samples\")\n",
    "            return df\n",
    "        else:\n",
    "            print(\"‚ùå No FTIR data found with fallback method\")\n",
    "            return None\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Fallback FTIR loading failed: {e}\")\n",
    "        return None\n",
    "\n",
    "def load_ftir_hips_robust(db_path: str, site_code: str) -> Optional[pd.DataFrame]:\n",
    "    \"\"\"Robust FTIR/HIPS loading with modular + fallback\"\"\"\n",
    "    \n",
    "    # Try modular system first\n",
    "    if MODULAR_AVAILABLE:\n",
    "        df = load_ftir_hips_with_modular_system(db_path, site_code)\n",
    "        if df is not None:\n",
    "            return df\n",
    "        print(\"‚ö†Ô∏è Modular FTIR loading failed, trying fallback...\")\n",
    "    \n",
    "    # Fallback method\n",
    "    return load_ftir_hips_fallback(db_path, site_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "400d18c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_aethalometer_robust_patched(file_path: str, \n",
    "                                   output_format: str = \"jpl\",\n",
    "                                   site_filter: Optional[str] = None) -> Tuple[Optional[pd.DataFrame], Dict]:\n",
    "    \"\"\"\n",
    "    Patched robust aethalometer loading with modular system + improved fallback\n",
    "    \"\"\"\n",
    "    print(f\"üìÅ Loading with patched system: {Path(file_path).name}\")\n",
    "    print(f\"üîß Output format: {output_format}\")\n",
    "    \n",
    "    file_ext = Path(file_path).suffix.lower()\n",
    "    \n",
    "    # For CSV files, use the modular CSV loader if available\n",
    "    if file_ext == '.csv':\n",
    "        print(f\"üéØ Using improved CSV loading...\")\n",
    "        \n",
    "        if MODULAR_AVAILABLE and 'AethalometerCSVLoader' in modular_components.get('loaders', {}):\n",
    "            try:\n",
    "                AethalometerCSVLoader = modular_components['loaders']['AethalometerCSVLoader']\n",
    "                csv_loader = AethalometerCSVLoader(file_path)\n",
    "                df = csv_loader.load(set_datetime_index=True)\n",
    "                \n",
    "                # Check if datetime index was properly set\n",
    "                if df is not None and not isinstance(df.index, pd.DatetimeIndex):\n",
    "                    print(\"üîß Fixing datetime index for CSV data...\")\n",
    "                    \n",
    "                    # Try to find and parse datetime columns\n",
    "                    datetime_candidates = []\n",
    "                    for col in df.columns:\n",
    "                        if any(keyword in col.lower() for keyword in ['date', 'time', 'datetime']):\n",
    "                            datetime_candidates.append(col)\n",
    "                    \n",
    "                    print(f\"üîç Found datetime candidates: {datetime_candidates}\")\n",
    "                    \n",
    "                    # Try to create datetime index from candidates\n",
    "                    datetime_col = None\n",
    "                    for col in datetime_candidates:\n",
    "                        try:\n",
    "                            # Try parsing the column as datetime\n",
    "                            df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "                            if df[col].notna().sum() > len(df) * 0.8:  # At least 80% valid dates\n",
    "                                datetime_col = col\n",
    "                                break\n",
    "                        except Exception as e:\n",
    "                            print(f\"   Failed to parse {col}: {e}\")\n",
    "                    \n",
    "                    # If we found a valid datetime column, set it as index\n",
    "                    if datetime_col:\n",
    "                        print(f\"‚úÖ Using {datetime_col} as datetime index\")\n",
    "                        df = df.set_index(datetime_col)\n",
    "                        df = df.sort_index()\n",
    "                    else:\n",
    "                        print(\"‚ö†Ô∏è Could not find valid datetime column, checking for combined date/time columns...\")\n",
    "                        \n",
    "                        # Look for separate date and time columns\n",
    "                        date_cols = [col for col in df.columns if 'date' in col.lower()]\n",
    "                        time_cols = [col for col in df.columns if 'time' in col.lower() and 'date' not in col.lower()]\n",
    "                        \n",
    "                        if date_cols and time_cols:\n",
    "                            try:\n",
    "                                # Combine first date and time columns\n",
    "                                date_col = date_cols[0]\n",
    "                                time_col = time_cols[0]\n",
    "                                print(f\"üîß Combining {date_col} + {time_col}\")\n",
    "                                \n",
    "                                datetime_combined = pd.to_datetime(df[date_col].astype(str) + ' ' + df[time_col].astype(str), errors='coerce')\n",
    "                                \n",
    "                                if datetime_combined.notna().sum() > len(df) * 0.8:\n",
    "                                    df['datetime_combined'] = datetime_combined\n",
    "                                    df = df.set_index('datetime_combined')\n",
    "                                    df = df.sort_index()\n",
    "                                    print(\"‚úÖ Successfully created combined datetime index\")\n",
    "                                else:\n",
    "                                    print(\"‚ùå Combined datetime parsing failed\")\n",
    "                            except Exception as e:\n",
    "                                print(f\"‚ùå Error combining date/time columns: {e}\")\n",
    "                \n",
    "                if df is not None and len(df) > 0:\n",
    "                    # Apply format conversion if needed\n",
    "                    if output_format == 'jpl':\n",
    "                        # Convert standard column names to JPL format if needed\n",
    "                        column_mapping = {\n",
    "                            'IR BCc': 'IR.BCc',\n",
    "                            'Blue BCc': 'Blue.BCc', \n",
    "                            'Green BCc': 'Green.BCc',\n",
    "                            'Red BCc': 'Red.BCc',\n",
    "                            'UV BCc': 'UV.BCc',\n",
    "                            'Biomass BCc': 'Biomass.BCc',\n",
    "                            'Fossil fuel BCc': 'Fossil.fuel.BCc',\n",
    "                        }\n",
    "                        \n",
    "                        rename_dict = {}\n",
    "                        for std_col, jpl_col in column_mapping.items():\n",
    "                            if std_col in df.columns:\n",
    "                                rename_dict[std_col] = jpl_col\n",
    "                        \n",
    "                        if rename_dict:\n",
    "                            df = df.rename(columns=rename_dict)\n",
    "                            print(f\"Converted {len(rename_dict)} columns to JPL format\")\n",
    "                    \n",
    "                    # Generate comprehensive summary\n",
    "                    summary = {\n",
    "                        'file_name': Path(file_path).name,\n",
    "                        'file_type': Path(file_path).suffix,\n",
    "                        'format': output_format,\n",
    "                        'shape': df.shape,\n",
    "                        'columns': len(df.columns),\n",
    "                        'memory_mb': df.memory_usage(deep=True).sum() / 1024**2,\n",
    "                        'bc_columns': [col for col in df.columns if '.BCc' in col or 'BC' in col.upper()],\n",
    "                        'atn_columns': [col for col in df.columns if '.ATN' in col or 'ATN' in col.upper()],\n",
    "                        'time_range': (df.index.min(), df.index.max()) if hasattr(df.index, 'min') else None,\n",
    "                        'missing_data_pct': (df.isnull().sum().sum() / df.size) * 100,\n",
    "                        'has_datetime_index': isinstance(df.index, pd.DatetimeIndex)\n",
    "                    }\n",
    "                    \n",
    "                    print(f\"‚úÖ Successfully loaded: {df.shape[0]:,} rows √ó {df.shape[1]} columns\")\n",
    "                    print(f\"üìä Format: {output_format}\")\n",
    "                    print(f\"üìä Memory usage: {summary['memory_mb']:.2f} MB\")\n",
    "                    print(f\"üßÆ BC columns found: {len(summary['bc_columns'])}\")\n",
    "                    print(f\"üìà ATN columns found: {len(summary['atn_columns'])}\")\n",
    "                    \n",
    "                    if summary['time_range']:\n",
    "                        print(f\"üìÖ Time range: {summary['time_range'][0]} to {summary['time_range'][1]}\")\n",
    "                    \n",
    "                    return df, summary\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Modular CSV loader failed: {e}\")\n",
    "        \n",
    "        # Fallback for CSV files\n",
    "        print(f\"üîÑ Using CSV fallback...\")\n",
    "        return load_aethalometer_fallback(file_path)\n",
    "    \n",
    "    # For PKL files, try modular system first\n",
    "    elif file_ext == '.pkl':\n",
    "        if MODULAR_AVAILABLE:\n",
    "            try:\n",
    "                load_function = modular_components['loaders']['load_aethalometer_data']\n",
    "                \n",
    "                df = load_function(\n",
    "                    file_path,\n",
    "                    output_format=output_format,\n",
    "                    site_filter=site_filter,\n",
    "                    set_datetime_index=True\n",
    "                )\n",
    "                \n",
    "                if df is not None and len(df) > 0:\n",
    "                    # Generate comprehensive summary\n",
    "                    summary = {\n",
    "                        'file_name': Path(file_path).name,\n",
    "                        'file_type': Path(file_path).suffix,\n",
    "                        'format': output_format,\n",
    "                        'shape': df.shape,\n",
    "                        'columns': len(df.columns),\n",
    "                        'memory_mb': df.memory_usage(deep=True).sum() / 1024**2,\n",
    "                        'bc_columns': [col for col in df.columns if '.BCc' in col or 'BC' in col.upper()],\n",
    "                        'atn_columns': [col for col in df.columns if '.ATN' in col or 'ATN' in col.upper()],\n",
    "                        'time_range': (df.index.min(), df.index.max()) if hasattr(df.index, 'min') else None,\n",
    "                        'missing_data_pct': (df.isnull().sum().sum() / df.size) * 100,\n",
    "                        'has_datetime_index': isinstance(df.index, pd.DatetimeIndex)\n",
    "                    }\n",
    "                    \n",
    "                    print(f\"‚úÖ Successfully loaded: {df.shape[0]:,} rows √ó {df.shape[1]} columns\")\n",
    "                    print(f\"üìä Format: {output_format}\")\n",
    "                    print(f\"üìä Memory usage: {summary['memory_mb']:.2f} MB\")\n",
    "                    print(f\"üßÆ BC columns found: {len(summary['bc_columns'])}\")\n",
    "                    print(f\"üìà ATN columns found: {len(summary['atn_columns'])}\")\n",
    "                    \n",
    "                    if summary['time_range']:\n",
    "                        print(f\"üìÖ Time range: {summary['time_range'][0]} to {summary['time_range'][1]}\")\n",
    "                    \n",
    "                    return df, summary\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Modular system failed: {e}\")\n",
    "        \n",
    "        # Fallback for PKL files\n",
    "        print(f\"üîÑ Using fallback for PKL...\")\n",
    "        return load_aethalometer_fallback(file_path)\n",
    "    \n",
    "    else:\n",
    "        print(f\"‚ùå Unsupported file format: {file_ext}\")\n",
    "        return None, {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59d9771",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bc65b43f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üìÅ LOADING DATASETS WITH MODULAR SYSTEM\n",
      "============================================================\n",
      "üßπ Clearing previous dataset cache...\n",
      "\n",
      "üóÉÔ∏è Loading FTIR/HIPS database...\n",
      "üóÉÔ∏è Loading FTIR/HIPS data with modular system...\n",
      "üìä Available sites: ['ILNZ', 'ILHA', 'ZAJB', 'CAHA', 'CASH', 'AEAZ', 'AUMN', 'KRUL', 'MXMC', 'ZAPR', 'CHTS', 'ETAD', 'INDH', 'TWTA', 'USPA', 'TWKA', 'KRSE', 'PRFJ', 'BDDU', 'BIBU', 'USNO', 'IDBD', None]\n",
      "‚úÖ Loaded 168 FTIR/HIPS samples\n",
      "üìÖ Date range: 2022-12-07 00:00:00 to 2024-05-12 00:00:00\n",
      "‚úÖ FTIR/HIPS data loaded successfully\n",
      "\n",
      "üìã Sample FTIR/HIPS data:\n",
      "     filter_id sample_date  ec_ftir  oc_ftir  fabs\n",
      "0  ETAD-0122-2         NaT      NaN      NaN   NaN\n",
      "1  ETAD-0123-3         NaT      NaN      NaN   NaN\n",
      "2  ETAD-0124-4         NaT      NaN      NaN   NaN\n",
      "3  ETAD-0125-5         NaT      NaN      NaN   NaN\n",
      "4  ETAD-0126-6         NaT      NaN      NaN   NaN\n",
      "\n",
      "üìä Loading aethalometer datasets...\n",
      "üìã Files to process: ['pkl_data', 'csv_data']\n",
      "\n",
      "============================================================\n",
      "üìÅ Processing pkl_data\n",
      "üìÇ File: df_uncleaned_Jacros_API_and_OG.pkl\n",
      "üìç Path exists: True\n",
      "üìä File extension: .pkl\n",
      "============================================================\n",
      "üìÅ Loading with patched system: df_uncleaned_Jacros_API_and_OG.pkl\n",
      "üîß Output format: jpl\n",
      "Detected format: standard\n",
      "Detected format: standard\n",
      "Set 'datetime_local' as DatetimeIndex for time series operations\n",
      "Set 'datetime_local' as DatetimeIndex for time series operations\n",
      "Converted 17 columns to JPL format\n",
      "Warning: Missing recommended columns: ['datetime_local', 'Biomass.BCc', 'Fossil.fuel.BCc']\n",
      "Converted 17 columns to JPL format\n",
      "Warning: Missing recommended columns: ['datetime_local', 'Biomass.BCc', 'Fossil.fuel.BCc']\n",
      "‚úÖ Successfully loaded: 1,665,156 rows √ó 238 columns\n",
      "üìä Format: jpl\n",
      "üìä Memory usage: 7443.05 MB\n",
      "üßÆ BC columns found: 30\n",
      "üìà ATN columns found: 25\n",
      "üìÖ Time range: 2021-01-09 16:38:00 to 2025-06-26 23:18:00\n",
      "\n",
      "‚úÖ Raw data loaded: 1,665,156 rows √ó 238 columns\n",
      "‚úÖ pkl_data successfully stored in datasets\n",
      "üéØ Red BC columns found: ['Red BC1', 'Red BC2', 'Red.BCc']\n",
      "üìä Dataset info:\n",
      "   - Shape: (1665156, 238)\n",
      "   - Index type: DatetimeIndex\n",
      "   - Date range: 2021-01-09 16:38:00 to 2025-06-26 23:18:00\n",
      "\n",
      "============================================================\n",
      "üìÅ Processing csv_data\n",
      "üìÇ File: Jacros_MA350_1-min_2022-2024_Cleaned.csv\n",
      "üìç Path exists: True\n",
      "üìä File extension: .csv\n",
      "============================================================\n",
      "üìÅ Loading with patched system: Jacros_MA350_1-min_2022-2024_Cleaned.csv\n",
      "üîß Output format: jpl\n",
      "üéØ Using improved CSV loading...\n",
      "‚úÖ Successfully loaded: 1,665,156 rows √ó 238 columns\n",
      "üìä Format: jpl\n",
      "üìä Memory usage: 7443.05 MB\n",
      "üßÆ BC columns found: 30\n",
      "üìà ATN columns found: 25\n",
      "üìÖ Time range: 2021-01-09 16:38:00 to 2025-06-26 23:18:00\n",
      "\n",
      "‚úÖ Raw data loaded: 1,665,156 rows √ó 238 columns\n",
      "‚úÖ pkl_data successfully stored in datasets\n",
      "üéØ Red BC columns found: ['Red BC1', 'Red BC2', 'Red.BCc']\n",
      "üìä Dataset info:\n",
      "   - Shape: (1665156, 238)\n",
      "   - Index type: DatetimeIndex\n",
      "   - Date range: 2021-01-09 16:38:00 to 2025-06-26 23:18:00\n",
      "\n",
      "============================================================\n",
      "üìÅ Processing csv_data\n",
      "üìÇ File: Jacros_MA350_1-min_2022-2024_Cleaned.csv\n",
      "üìç Path exists: True\n",
      "üìä File extension: .csv\n",
      "============================================================\n",
      "üìÅ Loading with patched system: Jacros_MA350_1-min_2022-2024_Cleaned.csv\n",
      "üîß Output format: jpl\n",
      "üéØ Using improved CSV loading...\n",
      "üîß Fixing datetime index for CSV data...\n",
      "üîç Found datetime candidates: ['Time (UTC)', 'Timezone offset (mins)', 'Date local (yyyy/MM/dd)', 'Time local (hh:mm:ss)', 'Timebase (s)']\n",
      "‚úÖ Using Time (UTC) as datetime index\n",
      "üîß Fixing datetime index for CSV data...\n",
      "üîç Found datetime candidates: ['Time (UTC)', 'Timezone offset (mins)', 'Date local (yyyy/MM/dd)', 'Time local (hh:mm:ss)', 'Timebase (s)']\n",
      "‚úÖ Using Time (UTC) as datetime index\n",
      "Converted 5 columns to JPL format\n",
      "Converted 5 columns to JPL format\n",
      "‚úÖ Successfully loaded: 1,095,086 rows √ó 76 columns\n",
      "üìä Format: jpl\n",
      "üìä Memory usage: 876.48 MB\n",
      "üßÆ BC columns found: 15\n",
      "üìà ATN columns found: 10\n",
      "üìÖ Time range: 2022-04-12 09:46:01+00:00 to 2024-08-20 09:01:00+00:00\n",
      "\n",
      "‚úÖ Raw data loaded: 1,095,086 rows √ó 76 columns\n",
      "‚úÖ csv_data successfully stored in datasets\n",
      "üéØ Red BC columns found: ['Red BC1', 'Red BC2', 'Red.BCc']\n",
      "üìä Dataset info:\n",
      "   - Shape: (1095086, 76)\n",
      "   - Index type: DatetimeIndex\n",
      "   - Date range: 2022-04-12 09:46:01+00:00 to 2024-08-20 09:01:00+00:00\n",
      "\n",
      "============================================================\n",
      "üìä LOADING SUMMARY\n",
      "============================================================\n",
      "‚úÖ Successfully loaded 2 aethalometer datasets\n",
      "üìã Dataset names: ['pkl_data', 'csv_data']\n",
      "   - pkl_data: 1,665,156 rows √ó 238 columns\n",
      "   - csv_data: 1,095,086 rows √ó 76 columns\n",
      "============================================================\n",
      "‚úÖ Successfully loaded: 1,095,086 rows √ó 76 columns\n",
      "üìä Format: jpl\n",
      "üìä Memory usage: 876.48 MB\n",
      "üßÆ BC columns found: 15\n",
      "üìà ATN columns found: 10\n",
      "üìÖ Time range: 2022-04-12 09:46:01+00:00 to 2024-08-20 09:01:00+00:00\n",
      "\n",
      "‚úÖ Raw data loaded: 1,095,086 rows √ó 76 columns\n",
      "‚úÖ csv_data successfully stored in datasets\n",
      "üéØ Red BC columns found: ['Red BC1', 'Red BC2', 'Red.BCc']\n",
      "üìä Dataset info:\n",
      "   - Shape: (1095086, 76)\n",
      "   - Index type: DatetimeIndex\n",
      "   - Date range: 2022-04-12 09:46:01+00:00 to 2024-08-20 09:01:00+00:00\n",
      "\n",
      "============================================================\n",
      "üìä LOADING SUMMARY\n",
      "============================================================\n",
      "‚úÖ Successfully loaded 2 aethalometer datasets\n",
      "üìã Dataset names: ['pkl_data', 'csv_data']\n",
      "   - pkl_data: 1,665,156 rows √ó 238 columns\n",
      "   - csv_data: 1,095,086 rows √ó 76 columns\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# LOAD ALL DATASETS WITH ENHANCED SYSTEM (CLEANED VERSION)\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìÅ LOADING DATASETS WITH MODULAR SYSTEM\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Clear any previous data to prevent confusion\n",
    "if 'aethalometer_datasets' in globals():\n",
    "    print(\"üßπ Clearing previous dataset cache...\")\n",
    "    del aethalometer_datasets\n",
    "if 'aethalometer_summaries' in globals():\n",
    "    del aethalometer_summaries\n",
    "\n",
    "# Load FTIR/HIPS data\n",
    "print(f\"\\nüóÉÔ∏è Loading FTIR/HIPS database...\")\n",
    "ftir_data = load_ftir_hips_robust(FTIR_DB_PATH, SITE_CODE)\n",
    "\n",
    "if ftir_data is not None:\n",
    "    print(f\"‚úÖ FTIR/HIPS data loaded successfully\")\n",
    "    \n",
    "    # Display sample\n",
    "    print(f\"\\nüìã Sample FTIR/HIPS data:\")\n",
    "    display_cols = ['filter_id', 'sample_date', 'ec_ftir', 'oc_ftir', 'fabs']\n",
    "    available_cols = [col for col in display_cols if col in ftir_data.columns]\n",
    "    print(ftir_data[available_cols].head())\n",
    "else:\n",
    "    print(\"‚ùå Failed to load FTIR/HIPS data\")\n",
    "\n",
    "# Initialize fresh datasets dictionary\n",
    "aethalometer_datasets = {}\n",
    "aethalometer_summaries = {}\n",
    "\n",
    "# Load aethalometer datasets with explicit tracking\n",
    "print(f\"\\nüìä Loading aethalometer datasets...\")\n",
    "print(f\"üìã Files to process: {list(AETHALOMETER_FILES.keys())}\")\n",
    "\n",
    "for dataset_name, file_path in AETHALOMETER_FILES.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"üìÅ Processing {dataset_name}\")\n",
    "    print(f\"üìÇ File: {Path(file_path).name}\")\n",
    "    print(f\"üìç Path exists: {os.path.exists(file_path)}\")\n",
    "    print(f\"üìä File extension: {Path(file_path).suffix}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    if os.path.exists(file_path):\n",
    "        # Call the loading function\n",
    "        df, summary = load_aethalometer_robust_patched(\n",
    "            file_path, \n",
    "            output_format=OUTPUT_FORMAT,\n",
    "            site_filter=None\n",
    "        )\n",
    "        \n",
    "        # Validate the results\n",
    "        if df is not None and len(df) > 0:\n",
    "            print(f\"\\n‚úÖ Raw data loaded: {df.shape[0]:,} rows √ó {df.shape[1]} columns\")\n",
    "            \n",
    "            # Check if it has a proper datetime index\n",
    "            if isinstance(df.index, pd.DatetimeIndex):\n",
    "                aethalometer_datasets[dataset_name] = df\n",
    "                aethalometer_summaries[dataset_name] = summary\n",
    "                print(f\"‚úÖ {dataset_name} successfully stored in datasets\")\n",
    "                \n",
    "                # Show available BC columns for selected wavelength\n",
    "                bc_cols = [col for col in df.columns if WAVELENGTH in col and 'BC' in col]\n",
    "                print(f\"üéØ {WAVELENGTH} BC columns found: {bc_cols}\")\n",
    "                \n",
    "                # Show basic dataset info\n",
    "                print(f\"üìä Dataset info:\")\n",
    "                print(f\"   - Shape: {df.shape}\")\n",
    "                print(f\"   - Index type: {type(df.index).__name__}\")\n",
    "                print(f\"   - Date range: {df.index.min()} to {df.index.max()}\")\n",
    "                \n",
    "            else:\n",
    "                print(f\"‚ö†Ô∏è {dataset_name} has invalid datetime index - type: {type(df.index).__name__}\")\n",
    "                print(f\"   Index sample: {df.index[:5].tolist()}\")\n",
    "        else:\n",
    "            print(f\"‚ùå Failed to load {dataset_name} properly - df is None or empty\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è File not found: {file_path}\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(f\"üìä LOADING SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"‚úÖ Successfully loaded {len(aethalometer_datasets)} aethalometer datasets\")\n",
    "print(f\"üìã Dataset names: {list(aethalometer_datasets.keys())}\")\n",
    "\n",
    "for name, df in aethalometer_datasets.items():\n",
    "    print(f\"   - {name}: {df.shape[0]:,} rows √ó {df.shape[1]} columns\")\n",
    "    \n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9fbe348",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1960b2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 4. Aethalometer Data Loading Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4a1e14",
   "metadata": {},
   "source": [
    "## 5. Quality Assessment Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1853bc4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üîç CLEAN QUALITY ASSESSMENT\n",
      "================================================================================\n",
      "\n",
      "============================================================\n",
      "üîç Analyzing pkl_data data quality...\n",
      "üìä Quality threshold: ‚â§10 missing minutes per 24h period\n",
      "üìÖ Time range: 2021-01-09 16:38:00 to 2025-06-26 23:18:00\n",
      "üìä Actual data points: 1,665,156\n",
      "üìä Expected data points (1-min resolution): 2,346,161\n",
      "‚ö†Ô∏è Missing data points: 681,005\n",
      "üìä Data completeness: 71.0%\n",
      "üìÖ Analyzing 1630 24-hour periods...\n",
      "‚úÖ Found 1036 excellent quality periods\n",
      "üìÖ Excellent periods range: 2021-02-18 09:00:00 to 2025-06-25 09:00:00\n",
      "üìä Missing minutes distribution:\n",
      "   0 minutes missing: 778 periods\n",
      "   1-5 minutes missing: 239 periods\n",
      "   6-10 minutes missing: 19 periods\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "üîç Analyzing csv_data data quality...\n",
      "üìä Quality threshold: ‚â§10 missing minutes per 24h period\n",
      "üìÖ Time range: 2022-04-12 09:46:01+00:00 to 2024-08-20 09:01:00+00:00\n",
      "üìä Actual data points: 1,095,086\n",
      "üìä Expected data points (1-min resolution): 1,239,795\n",
      "‚ö†Ô∏è Missing data points: 144,709\n",
      "üìä Data completeness: 88.3%\n",
      "üìÖ Analyzing 862 24-hour periods...\n",
      "‚úÖ Found 718 excellent quality periods\n",
      "üìÖ Excellent periods range: 2022-04-14 09:00:00+00:00 to 2024-08-19 09:00:00+00:00\n",
      "üìä Missing minutes distribution:\n",
      "   0 minutes missing: 112 periods\n",
      "   1-5 minutes missing: 594 periods\n",
      "   6-10 minutes missing: 12 periods\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Clear any cached variables to ensure clean calculation\n",
    "if 'excellent_periods_dict' in globals():\n",
    "    del excellent_periods_dict\n",
    "\n",
    "def assess_data_quality_clean(aethalometer_df: pd.DataFrame, \n",
    "                             dataset_name: str,\n",
    "                             quality_threshold: int = 10) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Clean implementation of quality assessment for 24-hour periods (9am-to-9am).\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    aethalometer_df : pd.DataFrame\n",
    "        Aethalometer data with datetime index\n",
    "    dataset_name : str\n",
    "        Name of the dataset for logging\n",
    "    quality_threshold : int\n",
    "        Maximum missing minutes per 24h period for \"excellent\" quality\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        DataFrame with excellent periods (start_time, end_time, missing_minutes)\n",
    "    \"\"\"\n",
    "    print(f\"üîç Analyzing {dataset_name} data quality...\")\n",
    "    print(f\"üìä Quality threshold: ‚â§{quality_threshold} missing minutes per 24h period\")\n",
    "    \n",
    "    # Ensure datetime index\n",
    "    if not isinstance(aethalometer_df.index, pd.DatetimeIndex):\n",
    "        print(f\"‚ùå Invalid index type: {type(aethalometer_df.index)}\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Get basic info\n",
    "    df_start = aethalometer_df.index.min()\n",
    "    df_end = aethalometer_df.index.max()\n",
    "    actual_points = len(aethalometer_df.index.unique())\n",
    "    \n",
    "    print(f\"üìÖ Time range: {df_start} to {df_end}\")\n",
    "    print(f\"üìä Actual data points: {actual_points:,}\")\n",
    "    \n",
    "    # Calculate expected points based on 1-minute resolution\n",
    "    total_minutes = int((df_end - df_start).total_seconds() / 60) + 1\n",
    "    print(f\"üìä Expected data points (1-min resolution): {total_minutes:,}\")\n",
    "    \n",
    "    # Calculate missing points\n",
    "    missing_points = total_minutes - actual_points\n",
    "    completeness = (actual_points / total_minutes) * 100\n",
    "    \n",
    "    print(f\"‚ö†Ô∏è Missing data points: {missing_points:,}\")\n",
    "    print(f\"üìä Data completeness: {completeness:.1f}%\")\n",
    "    \n",
    "    # Quick sanity check\n",
    "    if missing_points < 0:\n",
    "        print(\"‚ö†Ô∏è Warning: More data points than expected - possible duplicates or sub-minute data\")\n",
    "        missing_points = 0\n",
    "    \n",
    "    # Create all possible 9am-to-9am periods\n",
    "    first_9am = df_start.normalize() + pd.Timedelta(hours=9)\n",
    "    if df_start.hour < 9:\n",
    "        first_9am -= pd.Timedelta(days=1)\n",
    "    \n",
    "    last_9am = df_end.normalize() + pd.Timedelta(hours=9)\n",
    "    if df_end.hour < 9:\n",
    "        last_9am -= pd.Timedelta(days=1)\n",
    "    \n",
    "    all_period_starts = pd.date_range(first_9am, last_9am, freq='D')\n",
    "    \n",
    "    print(f\"üìÖ Analyzing {len(all_period_starts)} 24-hour periods...\")\n",
    "    \n",
    "    # For each period, count missing minutes\n",
    "    excellent_periods_list = []\n",
    "    \n",
    "    for period_start in all_period_starts:\n",
    "        period_end = period_start + pd.Timedelta(days=1)\n",
    "        \n",
    "        # Get data for this period\n",
    "        period_data = aethalometer_df.loc[period_start:period_end]\n",
    "        actual_minutes = len(period_data)\n",
    "        expected_minutes = 1440  # 24 hours * 60 minutes\n",
    "        missing_minutes = max(0, expected_minutes - actual_minutes)\n",
    "        \n",
    "        # Check if this period qualifies as excellent\n",
    "        if missing_minutes <= quality_threshold:\n",
    "            excellent_periods_list.append({\n",
    "                'start_time': period_start,\n",
    "                'end_time': period_end,\n",
    "                'missing_minutes': missing_minutes\n",
    "            })\n",
    "    \n",
    "    # Create DataFrame\n",
    "    excellent_periods = pd.DataFrame(excellent_periods_list)\n",
    "    \n",
    "    print(f\"‚úÖ Found {len(excellent_periods)} excellent quality periods\")\n",
    "    \n",
    "    if len(excellent_periods) > 0:\n",
    "        print(f\"üìÖ Excellent periods range: {excellent_periods['start_time'].min()} to {excellent_periods['start_time'].max()}\")\n",
    "        print(f\"üìä Missing minutes distribution:\")\n",
    "        print(f\"   0 minutes missing: {(excellent_periods['missing_minutes'] == 0).sum()} periods\")\n",
    "        print(f\"   1-5 minutes missing: {((excellent_periods['missing_minutes'] >= 1) & (excellent_periods['missing_minutes'] <= 5)).sum()} periods\")\n",
    "        print(f\"   6-10 minutes missing: {((excellent_periods['missing_minutes'] >= 6) & (excellent_periods['missing_minutes'] <= 10)).sum()} periods\")\n",
    "    else:\n",
    "        print(\"‚ùå No excellent quality periods found\")\n",
    "    \n",
    "    return excellent_periods\n",
    "\n",
    "# Run clean quality assessment\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üîç CLEAN QUALITY ASSESSMENT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "excellent_periods_dict = {}\n",
    "\n",
    "for dataset_name, df in aethalometer_datasets.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    excellent_periods = assess_data_quality_clean(df, dataset_name, QUALITY_THRESHOLD)\n",
    "    excellent_periods_dict[dataset_name] = excellent_periods\n",
    "    print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf66502a",
   "metadata": {},
   "source": [
    "## 6. Time-Matched Merging Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "15f8db43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç FTIR Data Inspection:\n",
      "==================================================\n",
      "üìä FTIR data shape: (168, 12)\n",
      "üìä Columns: ['filter_id', 'sample_date', 'site_code', 'volume_m3', 'ec_ftir', 'ec_ftir_mdl', 'oc_ftir', 'oc_ftir_mdl', 'fabs', 'fabs_mdl', 'fabs_uncertainty', 'ftir_batch_id']\n",
      "\n",
      "üìÖ Sample date info:\n",
      "   - Total samples: 168\n",
      "   - Non-null sample_date: 162\n",
      "   - NaT/null sample_date: 6\n",
      "   - Data type: datetime64[ns]\n",
      "   - First 5 values:\n",
      "     [0]: NaT (type: <class 'pandas._libs.tslibs.nattype.NaTType'>)\n",
      "     [1]: NaT (type: <class 'pandas._libs.tslibs.nattype.NaTType'>)\n",
      "     [2]: NaT (type: <class 'pandas._libs.tslibs.nattype.NaTType'>)\n",
      "     [3]: NaT (type: <class 'pandas._libs.tslibs.nattype.NaTType'>)\n",
      "     [4]: NaT (type: <class 'pandas._libs.tslibs.nattype.NaTType'>)\n",
      "\n",
      "‚ö†Ô∏è Found 6 rows with NaT sample_date:\n",
      "     filter_id sample_date\n",
      "0  ETAD-0122-2         NaT\n",
      "1  ETAD-0123-3         NaT\n",
      "2  ETAD-0124-4         NaT\n",
      "3  ETAD-0125-5         NaT\n",
      "4  ETAD-0126-6         NaT\n",
      "\n",
      "üìÖ Valid date range: 2022-12-07 00:00:00 to 2024-05-12 00:00:00\n"
     ]
    }
   ],
   "source": [
    "# Inspect FTIR data for NaT values and data quality\n",
    "print(\"üîç FTIR Data Inspection:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if ftir_data is not None:\n",
    "    print(f\"üìä FTIR data shape: {ftir_data.shape}\")\n",
    "    print(f\"üìä Columns: {list(ftir_data.columns)}\")\n",
    "    \n",
    "    # Check sample_date column\n",
    "    print(f\"\\nüìÖ Sample date info:\")\n",
    "    print(f\"   - Total samples: {len(ftir_data)}\")\n",
    "    print(f\"   - Non-null sample_date: {ftir_data['sample_date'].notna().sum()}\")\n",
    "    print(f\"   - NaT/null sample_date: {ftir_data['sample_date'].isna().sum()}\")\n",
    "    \n",
    "    # Show sample_date data type and first few values\n",
    "    print(f\"   - Data type: {ftir_data['sample_date'].dtype}\")\n",
    "    print(f\"   - First 5 values:\")\n",
    "    for i, date in enumerate(ftir_data['sample_date'].head()):\n",
    "        print(f\"     [{i}]: {date} (type: {type(date)})\")\n",
    "    \n",
    "    # Show rows with NaT values if any\n",
    "    nat_rows = ftir_data[ftir_data['sample_date'].isna()]\n",
    "    if len(nat_rows) > 0:\n",
    "        print(f\"\\n‚ö†Ô∏è Found {len(nat_rows)} rows with NaT sample_date:\")\n",
    "        print(nat_rows[['filter_id', 'sample_date']].head())\n",
    "    \n",
    "    # Date range for valid dates\n",
    "    valid_dates = ftir_data['sample_date'].dropna()\n",
    "    if len(valid_dates) > 0:\n",
    "        print(f\"\\nüìÖ Valid date range: {valid_dates.min()} to {valid_dates.max()}\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå FTIR data not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d592392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üîó TIME-MATCHED MERGING\n",
      "============================================================\n",
      "\n",
      "üìä Merging pkl_data...\n",
      "----------------------------------------\n",
      "üîó Merging Red wavelength data for pkl_data...\n",
      "‚ö†Ô∏è Removed 6 filter samples with invalid dates\n",
      "üìä Processing 162 valid filter samples\n",
      "üìä Found 148 overlapping excellent periods with filter samples\n",
      "‚úÖ Successfully merged 148 periods\n",
      "‚úÖ pkl_data: 148 merged periods\n",
      "\n",
      "üìä Merging csv_data...\n",
      "----------------------------------------\n",
      "üîó Merging Red wavelength data for csv_data...\n",
      "‚ö†Ô∏è Removed 6 filter samples with invalid dates\n",
      "üìä Processing 162 valid filter samples\n",
      "üìä Found 0 overlapping excellent periods with filter samples\n",
      "‚ö†Ô∏è No overlapping periods found\n",
      "‚ö†Ô∏è csv_data: No merged periods found\n",
      "\n",
      "üéØ Merging completed: 1 datasets merged\n"
     ]
    }
   ],
   "source": [
    "def extract_aethalometer_stats(aethalometer_df: pd.DataFrame, \n",
    "                             period_start: pd.Timestamp, \n",
    "                             period_end: pd.Timestamp, \n",
    "                             bc_column: str) -> Optional[Dict]:\n",
    "    \"\"\"Extract statistics for aethalometer data within a specific period\"\"\"\n",
    "    try:\n",
    "        # Handle timezone compatibility\n",
    "        if period_start.tz is not None and aethalometer_df.index.tz is None:\n",
    "            period_start_naive = period_start.tz_localize(None)\n",
    "            period_end_naive = period_end.tz_localize(None)\n",
    "        elif period_start.tz is None and aethalometer_df.index.tz is not None:\n",
    "            period_start = period_start.tz_localize(aethalometer_df.index.tz)\n",
    "            period_end = period_end.tz_localize(aethalometer_df.index.tz)\n",
    "            period_start_naive = period_start\n",
    "            period_end_naive = period_end\n",
    "        else:\n",
    "            period_start_naive = period_start\n",
    "            period_end_naive = period_end\n",
    "        \n",
    "        # Extract data for the period\n",
    "        period_data = aethalometer_df.loc[period_start_naive:period_end_naive, bc_column].dropna()\n",
    "        \n",
    "        if len(period_data) == 0:\n",
    "            return None\n",
    "        \n",
    "        # Calculate statistics\n",
    "        stats = {\n",
    "            'count': len(period_data),\n",
    "            'mean': period_data.mean(),\n",
    "            'median': period_data.median(),\n",
    "            'std': period_data.std(),\n",
    "            'min': period_data.min(),\n",
    "            'max': period_data.max(),\n",
    "            'q25': period_data.quantile(0.25),\n",
    "            'q75': period_data.quantile(0.75),\n",
    "            'negative_count': (period_data < 0).sum(),\n",
    "            'negative_pct': (period_data < 0).mean() * 100,\n",
    "            'data_coverage_pct': (len(period_data) / 1440) * 100  # 1440 minutes in 24h\n",
    "        }\n",
    "        \n",
    "        return stats\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error extracting stats for period {period_start}: {e}\")\n",
    "        return None\n",
    "\n",
    "def map_ethiopian_seasons(month: int) -> str:\n",
    "    \"\"\"Map month number to Ethiopian season name\"\"\"\n",
    "    if month in [10, 11, 12, 1, 2]:\n",
    "        return 'Dry Season'\n",
    "    elif month in [3, 4, 5]:\n",
    "        return 'Belg Rainy Season'\n",
    "    else:  # months 6-9\n",
    "        return 'Kiremt Rainy Season'\n",
    "\n",
    "def merge_aethalometer_filter_data(aethalometer_df: pd.DataFrame,\n",
    "                                 filter_df: pd.DataFrame,\n",
    "                                 excellent_periods: pd.DataFrame,\n",
    "                                 wavelength: str = \"Red\",\n",
    "                                 dataset_name: str = \"aethalometer\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Merge aethalometer and filter sample data using 9am-9am period alignment.\n",
    "    \"\"\"\n",
    "    print(f\"üîó Merging {wavelength} wavelength data for {dataset_name}...\")\n",
    "    \n",
    "    # Find the BC column\n",
    "    bc_column = f\"{wavelength}.BCc\"\n",
    "    if bc_column not in aethalometer_df.columns:\n",
    "        # Try alternative naming\n",
    "        alt_columns = [col for col in aethalometer_df.columns \n",
    "                      if wavelength.lower() in col.lower() and 'bc' in col.lower()]\n",
    "        if alt_columns:\n",
    "            bc_column = alt_columns[0]\n",
    "            print(f\"üìù Using alternative BC column: {bc_column}\")\n",
    "        else:\n",
    "            print(f\"‚ùå No BC column found for wavelength '{wavelength}'\")\n",
    "            return pd.DataFrame()\n",
    "    \n",
    "    # Filter out rows with NaT sample_date before processing\n",
    "    valid_filter_df = filter_df.dropna(subset=['sample_date']).copy()\n",
    "    \n",
    "    if len(valid_filter_df) == 0:\n",
    "        print(\"‚ùå No valid sample dates found in filter data\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    removed_count = len(filter_df) - len(valid_filter_df)\n",
    "    if removed_count > 0:\n",
    "        print(f\"‚ö†Ô∏è Removed {removed_count} filter samples with invalid dates\")\n",
    "    \n",
    "    print(f\"üìä Processing {len(valid_filter_df)} valid filter samples\")\n",
    "    \n",
    "    # Convert filter sample dates to corresponding 9am-to-9am measurement periods\n",
    "    filter_measurement_periods = pd.DatetimeIndex([\n",
    "        d.normalize() + pd.Timedelta(hours=9) - pd.Timedelta(days=1)\n",
    "        for d in valid_filter_df['sample_date']\n",
    "    ])\n",
    "    \n",
    "    # Handle timezone compatibility - IMPROVED VERSION\n",
    "    excellent_starts = excellent_periods['start_time']\n",
    "    \n",
    "    # Normalize timezones for comparison\n",
    "    if hasattr(excellent_starts, 'dt') and excellent_starts.dt.tz is not None:\n",
    "        # Excellent periods have timezone\n",
    "        if filter_measurement_periods.tz is None:\n",
    "            # Filter periods are naive - localize to same timezone as excellent periods\n",
    "            tz = excellent_starts.dt.tz\n",
    "            filter_measurement_periods = filter_measurement_periods.tz_localize(tz)\n",
    "        else:\n",
    "            # Both have timezones - convert filter periods to excellent periods timezone\n",
    "            tz = excellent_starts.dt.tz\n",
    "            filter_measurement_periods = filter_measurement_periods.tz_convert(tz)\n",
    "        \n",
    "        # Convert excellent_starts to same timezone if needed\n",
    "        excellent_starts_normalized = excellent_starts\n",
    "        \n",
    "    else:\n",
    "        # Excellent periods are naive\n",
    "        if filter_measurement_periods.tz is not None:\n",
    "            # Filter periods have timezone - convert to naive (remove timezone)\n",
    "            filter_measurement_periods = filter_measurement_periods.tz_localize(None)\n",
    "        \n",
    "        # Both are now naive\n",
    "        excellent_starts_normalized = excellent_starts\n",
    "    \n",
    "    print(f\"üïê Filter periods timezone: {filter_measurement_periods.tz}\")\n",
    "    print(f\"üïê Excellent periods timezone: {getattr(excellent_starts_normalized.dtype, 'tz', 'naive')}\")\n",
    "    \n",
    "    # Find overlap between filter measurement periods and excellent periods\n",
    "    overlap_periods = pd.DatetimeIndex(filter_measurement_periods).intersection(excellent_starts_normalized)\n",
    "    \n",
    "    print(f\"üìä Found {len(overlap_periods)} overlapping excellent periods with filter samples\")\n",
    "    \n",
    "    if len(overlap_periods) == 0:\n",
    "        print(\"‚ö†Ô∏è No overlapping periods found\")\n",
    "        # Additional debugging\n",
    "        print(f\"üîç Debug info:\")\n",
    "        print(f\"   Filter periods range: {filter_measurement_periods.min()} to {filter_measurement_periods.max()}\")\n",
    "        print(f\"   Excellent periods range: {excellent_starts_normalized.min()} to {excellent_starts_normalized.max()}\")\n",
    "        print(f\"   Sample filter periods: {list(filter_measurement_periods[:3])}\")\n",
    "        print(f\"   Sample excellent periods: {list(excellent_starts_normalized[:3])}\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Create merged dataset\n",
    "    merged_data = []\n",
    "    \n",
    "    for period_start in overlap_periods:\n",
    "        period_end = period_start + pd.Timedelta(days=1)\n",
    "        \n",
    "        # Find the corresponding filter sample\n",
    "        collection_date = period_start + pd.Timedelta(days=1)\n",
    "        \n",
    "        # Find matching filter sample\n",
    "        filter_matches = valid_filter_df[\n",
    "            valid_filter_df['sample_date'].dt.date == collection_date.date()\n",
    "        ]\n",
    "        \n",
    "        if len(filter_matches) == 0:\n",
    "            continue\n",
    "        \n",
    "        filter_data = filter_matches.iloc[0]  # Take first match if multiple\n",
    "        \n",
    "        # Extract aethalometer data for this period\n",
    "        aeth_stats = extract_aethalometer_stats(aethalometer_df, period_start, period_end, bc_column)\n",
    "        \n",
    "        if aeth_stats is None:\n",
    "            continue\n",
    "        \n",
    "        # Combine filter and aethalometer data\n",
    "        row_data = {\n",
    "            'dataset_source': dataset_name,\n",
    "            'period_start': period_start,\n",
    "            'period_end': period_end,\n",
    "            'collection_date': collection_date,\n",
    "            'filter_id': filter_data['filter_id'],\n",
    "            'EC_FTIR': filter_data.get('ec_ftir', np.nan),\n",
    "            'OC_FTIR': filter_data.get('oc_ftir', np.nan),\n",
    "            'Fabs': filter_data.get('fabs', np.nan),\n",
    "            'site': filter_data.get('site_code', SITE_CODE),\n",
    "            'wavelength': wavelength\n",
    "        }\n",
    "        \n",
    "        # Add aethalometer statistics with 'aeth_' prefix\n",
    "        for key, value in aeth_stats.items():\n",
    "            row_data[f'aeth_{key}'] = value\n",
    "        \n",
    "        merged_data.append(row_data)\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    merged_df = pd.DataFrame(merged_data)\n",
    "    \n",
    "    # Add derived variables if we have data\n",
    "    if len(merged_df) > 0:\n",
    "        # Mass Absorption Cross-section (MAC)\n",
    "        if 'EC_FTIR' in merged_df.columns and 'Fabs' in merged_df.columns:\n",
    "            merged_df['MAC'] = merged_df['Fabs'] / merged_df['EC_FTIR']\n",
    "        \n",
    "        # Add season information\n",
    "        merged_df['month'] = merged_df['collection_date'].dt.month\n",
    "        merged_df['season'] = merged_df['month'].apply(map_ethiopian_seasons)\n",
    "        \n",
    "        # Add date information\n",
    "        merged_df['date'] = merged_df['collection_date'].dt.date\n",
    "    \n",
    "    print(f\"‚úÖ Successfully merged {len(merged_df)} periods\")\n",
    "    \n",
    "    return merged_df\n",
    "\n",
    "# Perform merging for all datasets\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üîó TIME-MATCHED MERGING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "merged_datasets = {}\n",
    "\n",
    "if ftir_data is not None:\n",
    "    for dataset_name, aeth_df in aethalometer_datasets.items():\n",
    "        print(f\"\\nüìä Merging {dataset_name}...\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        excellent_periods = excellent_periods_dict[dataset_name]\n",
    "        \n",
    "        merged_df = merge_aethalometer_filter_data(\n",
    "            aethalometer_df=aeth_df,\n",
    "            filter_df=ftir_data,\n",
    "            excellent_periods=excellent_periods,\n",
    "            wavelength=WAVELENGTH,\n",
    "            dataset_name=dataset_name\n",
    "        )\n",
    "        \n",
    "        if len(merged_df) > 0:\n",
    "            merged_datasets[dataset_name] = merged_df\n",
    "            print(f\"‚úÖ {dataset_name}: {len(merged_df)} merged periods\")\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è {dataset_name}: No merged periods found\")\n",
    "else:\n",
    "    print(\"‚ùå Cannot perform merging - FTIR data not available\")\n",
    "\n",
    "print(f\"\\nüéØ Merging completed: {len(merged_datasets)} datasets merged\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c943a820",
   "metadata": {},
   "source": [
    "## 7. Statistical Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5eca4210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç DIAGNOSTIC ANALYSIS: CSV vs PKL Data Overlap\n",
      "============================================================\n",
      "üìÖ FTIR date range: 2022-12-07 00:00:00 to 2024-05-12 00:00:00\n",
      "üìÖ FTIR 9am-to-9am periods: 2022-12-06 09:00:00 to 2024-05-11 09:00:00\n",
      "üìä Total FTIR periods: 162\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "üìä PKL_DATA ANALYSIS:\n",
      "   üìÖ Aethalometer date range: 2021-01-09 16:38:00 to 2025-06-26 23:18:00\n",
      "   üìÖ Excellent periods range: 2021-02-18 09:00:00 to 2025-06-25 09:00:00\n",
      "   üìä Total excellent periods: 1036\n",
      "   üîó Overlapping periods: 148\n",
      "   üìÖ First overlap: 2022-12-06 09:00:00\n",
      "   üìÖ Last overlap: 2024-05-11 09:00:00\n",
      "   ------------------------------\n",
      "\n",
      "üìä CSV_DATA ANALYSIS:\n",
      "   üìÖ Aethalometer date range: 2022-04-12 09:46:01+00:00 to 2024-08-20 09:01:00+00:00\n",
      "   üìÖ Excellent periods range: 2022-04-14 09:00:00+00:00 to 2024-08-19 09:00:00+00:00\n",
      "   üìä Total excellent periods: 718\n",
      "   üîó Overlapping periods: 0\n",
      "   ‚ö†Ô∏è NO OVERLAP FOUND\n",
      "   üìä Time gap analysis:\n",
      "      FTIR periods: 2022-12-06 09:00:00+03:00 to 2024-05-11 09:00:00+03:00\n",
      "      Excellent periods: 2022-04-14 09:00:00+00:00 to 2024-08-19 09:00:00+00:00\n",
      "      ü§î Time ranges overlap, but no exact matches found\n",
      "   ------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Diagnostic analysis: Why does CSV data have 0 overlapping periods?\n",
    "print(\"üîç DIAGNOSTIC ANALYSIS: CSV vs PKL Data Overlap\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if ftir_data is not None:\n",
    "    # Check FTIR data date range\n",
    "    valid_ftir_dates = ftir_data['sample_date'].dropna()\n",
    "    ftir_start, ftir_end = valid_ftir_dates.min(), valid_ftir_dates.max()\n",
    "    print(f\"üìÖ FTIR date range: {ftir_start} to {ftir_end}\")\n",
    "    \n",
    "    # Convert to 9am-to-9am periods\n",
    "    ftir_periods = pd.DatetimeIndex([\n",
    "        d.normalize() + pd.Timedelta(hours=9) - pd.Timedelta(days=1)\n",
    "        for d in valid_ftir_dates\n",
    "    ])\n",
    "    \n",
    "    print(f\"üìÖ FTIR 9am-to-9am periods: {ftir_periods.min()} to {ftir_periods.max()}\")\n",
    "    print(f\"üìä Total FTIR periods: {len(ftir_periods)}\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\" * 40)\n",
    "    \n",
    "    for dataset_name, aeth_df in aethalometer_datasets.items():\n",
    "        print(f\"\\nüìä {dataset_name.upper()} ANALYSIS:\")\n",
    "        \n",
    "        # Check aethalometer data time range\n",
    "        aeth_start, aeth_end = aeth_df.index.min(), aeth_df.index.max()\n",
    "        print(f\"   üìÖ Aethalometer date range: {aeth_start} to {aeth_end}\")\n",
    "        \n",
    "        # Check excellent periods\n",
    "        excellent_periods = excellent_periods_dict[dataset_name]\n",
    "        excellent_start = excellent_periods['start_time'].min()\n",
    "        excellent_end = excellent_periods['start_time'].max()\n",
    "        print(f\"   üìÖ Excellent periods range: {excellent_start} to {excellent_end}\")\n",
    "        print(f\"   üìä Total excellent periods: {len(excellent_periods)}\")\n",
    "        \n",
    "        # Check overlap with FTIR periods\n",
    "        if hasattr(excellent_periods['start_time'], 'dt') and excellent_periods['start_time'].dt.tz is not None:\n",
    "            if ftir_periods.tz is None:\n",
    "                ftir_periods_tz = ftir_periods.tz_localize('Africa/Addis_Ababa')\n",
    "            else:\n",
    "                ftir_periods_tz = ftir_periods\n",
    "        else:\n",
    "            ftir_periods_tz = ftir_periods\n",
    "        \n",
    "        overlap = pd.DatetimeIndex(ftir_periods_tz).intersection(excellent_periods['start_time'])\n",
    "        print(f\"   üîó Overlapping periods: {len(overlap)}\")\n",
    "        \n",
    "        if len(overlap) > 0:\n",
    "            print(f\"   üìÖ First overlap: {overlap.min()}\")\n",
    "            print(f\"   üìÖ Last overlap: {overlap.max()}\")\n",
    "        else:\n",
    "            print(\"   ‚ö†Ô∏è NO OVERLAP FOUND\")\n",
    "            \n",
    "            # Check if there's any time overlap at all\n",
    "            ftir_min_tz = ftir_periods_tz.min()\n",
    "            ftir_max_tz = ftir_periods_tz.max()\n",
    "            excellent_min = excellent_periods['start_time'].min()\n",
    "            excellent_max = excellent_periods['start_time'].max()\n",
    "            \n",
    "            print(f\"   üìä Time gap analysis:\")\n",
    "            print(f\"      FTIR periods: {ftir_min_tz} to {ftir_max_tz}\")\n",
    "            print(f\"      Excellent periods: {excellent_min} to {excellent_max}\")\n",
    "            \n",
    "            if ftir_max_tz < excellent_min:\n",
    "                gap = excellent_min - ftir_max_tz\n",
    "                print(f\"      ‚ùå FTIR ends {gap} before excellent periods start\")\n",
    "            elif ftir_min_tz > excellent_max:\n",
    "                gap = ftir_min_tz - excellent_max\n",
    "                print(f\"      ‚ùå FTIR starts {gap} after excellent periods end\")\n",
    "            else:\n",
    "                print(f\"      ü§î Time ranges overlap, but no exact matches found\")\n",
    "        \n",
    "        print(\"   \" + \"-\" * 30)\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå FTIR data not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db4163b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_correlation_statistics(merged_df: pd.DataFrame) -> Dict:\n",
    "    \"\"\"Calculate comprehensive correlation statistics\"\"\"\n",
    "    \n",
    "    if len(merged_df) == 0:\n",
    "        return {'error': 'No data available'}\n",
    "    \n",
    "    # Convert aethalometer BC to ¬µg/m¬≥ (assuming it's in ng/m¬≥)\n",
    "    merged_df['aeth_bc_ug'] = merged_df['aeth_mean'] / 1000\n",
    "    \n",
    "    stats = {}\n",
    "    \n",
    "    # Basic sample info\n",
    "    stats['sample_info'] = {\n",
    "        'total_samples': len(merged_df),\n",
    "        'date_range': (merged_df['collection_date'].min(), merged_df['collection_date'].max())\n",
    "    }\n",
    "    \n",
    "    # Correlations\n",
    "    correlations = {}\n",
    "    if all(col in merged_df.columns for col in ['aeth_bc_ug', 'EC_FTIR']):\n",
    "        valid_data = merged_df[['aeth_bc_ug', 'EC_FTIR']].dropna()\n",
    "        if len(valid_data) > 1:\n",
    "            correlations['BC_EC'] = valid_data['aeth_bc_ug'].corr(valid_data['EC_FTIR'])\n",
    "    \n",
    "    if all(col in merged_df.columns for col in ['aeth_bc_ug', 'Fabs']):\n",
    "        valid_data = merged_df[['aeth_bc_ug', 'Fabs']].dropna()\n",
    "        if len(valid_data) > 1:\n",
    "            correlations['BC_Fabs'] = valid_data['aeth_bc_ug'].corr(valid_data['Fabs'])\n",
    "    \n",
    "    if all(col in merged_df.columns for col in ['EC_FTIR', 'Fabs']):\n",
    "        valid_data = merged_df[['EC_FTIR', 'Fabs']].dropna()\n",
    "        if len(valid_data) > 1:\n",
    "            correlations['EC_Fabs'] = valid_data['EC_FTIR'].corr(valid_data['Fabs'])\n",
    "    \n",
    "    stats['correlations'] = correlations\n",
    "    \n",
    "    # MAC statistics\n",
    "    if 'MAC' in merged_df.columns:\n",
    "        mac_data = merged_df['MAC'].dropna()\n",
    "        if len(mac_data) > 0:\n",
    "            # Filter out extreme MAC values (typical range: 5-15 m¬≤/g)\n",
    "            mac_filtered = mac_data[(mac_data > 0) & (mac_data < 30)]\n",
    "            \n",
    "            stats['mac_statistics'] = {\n",
    "                'count': len(mac_filtered),\n",
    "                'mean': mac_filtered.mean(),\n",
    "                'median': mac_filtered.median(),\n",
    "                'std': mac_filtered.std(),\n",
    "                'min': mac_filtered.min(),\n",
    "                'max': mac_filtered.max()\n",
    "            }\n",
    "    \n",
    "    # Seasonal statistics\n",
    "    if 'season' in merged_df.columns:\n",
    "        seasonal_stats = {}\n",
    "        for season in merged_df['season'].unique():\n",
    "            season_data = merged_df[merged_df['season'] == season]\n",
    "            if len(season_data) > 0:\n",
    "                seasonal_stats[season] = {\n",
    "                    'count': len(season_data),\n",
    "                    'mean_BC': season_data['aeth_bc_ug'].mean() if 'aeth_bc_ug' in season_data.columns else np.nan,\n",
    "                    'mean_EC': season_data['EC_FTIR'].mean() if 'EC_FTIR' in season_data.columns else np.nan,\n",
    "                    'mean_MAC': season_data['MAC'].mean() if 'MAC' in season_data.columns else np.nan\n",
    "                }\n",
    "        stats['seasonal_statistics'] = seasonal_stats\n",
    "    \n",
    "    return stats\n",
    "\n",
    "# Calculate statistics for all merged datasets\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìä STATISTICAL ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "analysis_results = {}\n",
    "\n",
    "for dataset_name, merged_df in merged_datasets.items():\n",
    "    print(f\"\\nüìä Analyzing {dataset_name}...\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    stats = calculate_correlation_statistics(merged_df)\n",
    "    analysis_results[dataset_name] = stats\n",
    "    \n",
    "    if 'error' not in stats:\n",
    "        # Display results\n",
    "        print(f\"üìà Sample info:\")\n",
    "        print(f\"   Total samples: {stats['sample_info']['total_samples']}\")\n",
    "        print(f\"   Date range: {stats['sample_info']['date_range'][0]} to {stats['sample_info']['date_range'][1]}\")\n",
    "        \n",
    "        print(f\"\\nüîó Correlations:\")\n",
    "        for corr_name, corr_value in stats['correlations'].items():\n",
    "            print(f\"   {corr_name}: {corr_value:.3f}\")\n",
    "        \n",
    "        if 'mac_statistics' in stats:\n",
    "            mac_stats = stats['mac_statistics']\n",
    "            print(f\"\\nüìä MAC Statistics:\")\n",
    "            print(f\"   Count: {mac_stats['count']}\")\n",
    "            print(f\"   Mean: {mac_stats['mean']:.2f} m¬≤/g\")\n",
    "            print(f\"   Median: {mac_stats['median']:.2f} m¬≤/g\")\n",
    "            print(f\"   Std: {mac_stats['std']:.2f} m¬≤/g\")\n",
    "        \n",
    "        if 'seasonal_statistics' in stats:\n",
    "            print(f\"\\nüåç Seasonal Statistics:\")\n",
    "            for season, season_stats in stats['seasonal_statistics'].items():\n",
    "                print(f\"   {season}: n={season_stats['count']}, MAC={season_stats['mean_MAC']:.2f} m¬≤/g\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b56a24a",
   "metadata": {},
   "source": [
    "## 8. Visualization Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e614e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_correlation_analysis(merged_datasets: Dict[str, pd.DataFrame]):\n",
    "    \"\"\"Create correlation plots for all datasets\"\"\"\n",
    "    \n",
    "    if not merged_datasets:\n",
    "        print(\"No data available for plotting\")\n",
    "        return\n",
    "    \n",
    "    n_datasets = len(merged_datasets)\n",
    "    fig, axes = plt.subplots(2, n_datasets, figsize=(6*n_datasets, 12))\n",
    "    \n",
    "    if n_datasets == 1:\n",
    "        axes = axes.reshape(-1, 1)\n",
    "    \n",
    "    dataset_names = list(merged_datasets.keys())\n",
    "    \n",
    "    for idx, (dataset_name, merged_df) in enumerate(merged_datasets.items()):\n",
    "        if len(merged_df) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Convert BC to ¬µg/m¬≥\n",
    "        merged_df['aeth_bc_ug'] = merged_df['aeth_mean'] / 1000\n",
    "        \n",
    "        # Plot 1: BC vs EC\n",
    "        ax1 = axes[0, idx]\n",
    "        valid_data = merged_df[['aeth_bc_ug', 'EC_FTIR']].dropna()\n",
    "        \n",
    "        if len(valid_data) > 1:\n",
    "            ax1.scatter(valid_data['EC_FTIR'], valid_data['aeth_bc_ug'], alpha=0.7, s=60)\n",
    "            \n",
    "            # Add trend line\n",
    "            z = np.polyfit(valid_data['EC_FTIR'], valid_data['aeth_bc_ug'], 1)\n",
    "            p = np.poly1d(z)\n",
    "            ax1.plot(valid_data['EC_FTIR'], p(valid_data['EC_FTIR']), \"r--\", alpha=0.8)\n",
    "            \n",
    "            # Add correlation\n",
    "            correlation = valid_data['aeth_bc_ug'].corr(valid_data['EC_FTIR'])\n",
    "            ax1.text(0.05, 0.95, f'r = {correlation:.3f}', transform=ax1.transAxes, \n",
    "                    bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"white\", alpha=0.8))\n",
    "        \n",
    "        ax1.set_xlabel('EC FTIR (¬µg/m¬≥)')\n",
    "        ax1.set_ylabel('Aethalometer BC (¬µg/m¬≥)')\n",
    "        ax1.set_title(f'{dataset_name}\\nBC vs EC')\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot 2: BC vs Fabs\n",
    "        ax2 = axes[1, idx]\n",
    "        valid_data_fabs = merged_df[['aeth_bc_ug', 'Fabs']].dropna()\n",
    "        \n",
    "        if len(valid_data_fabs) > 1:\n",
    "            ax2.scatter(valid_data_fabs['Fabs'], valid_data_fabs['aeth_bc_ug'], \n",
    "                       alpha=0.7, s=60, color='orange')\n",
    "            \n",
    "            # Add trend line\n",
    "            z = np.polyfit(valid_data_fabs['Fabs'], valid_data_fabs['aeth_bc_ug'], 1)\n",
    "            p = np.poly1d(z)\n",
    "            ax2.plot(valid_data_fabs['Fabs'], p(valid_data_fabs['Fabs']), \"r--\", alpha=0.8)\n",
    "            \n",
    "            # Add correlation\n",
    "            correlation = valid_data_fabs['aeth_bc_ug'].corr(valid_data_fabs['Fabs'])\n",
    "            ax2.text(0.05, 0.95, f'r = {correlation:.3f}', transform=ax2.transAxes,\n",
    "                    bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"white\", alpha=0.8))\n",
    "        \n",
    "        ax2.set_xlabel('Fabs (Mm‚Åª¬π)')\n",
    "        ax2.set_ylabel('Aethalometer BC (¬µg/m¬≥)')\n",
    "        ax2.set_title(f'{dataset_name}\\nBC vs Fabs')\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return fig\n",
    "\n",
    "def plot_time_series_comparison(merged_datasets: Dict[str, pd.DataFrame]):\n",
    "    \"\"\"Create time series comparison plots\"\"\"\n",
    "    \n",
    "    if not merged_datasets:\n",
    "        print(\"No data available for time series plotting\")\n",
    "        return\n",
    "    \n",
    "    fig, axes = plt.subplots(3, 1, figsize=(16, 12), sharex=True)\n",
    "    \n",
    "    colors = ['blue', 'red', 'green', 'orange', 'purple']\n",
    "    \n",
    "    for idx, (dataset_name, merged_df) in enumerate(merged_datasets.items()):\n",
    "        if len(merged_df) == 0:\n",
    "            continue\n",
    "        \n",
    "        color = colors[idx % len(colors)]\n",
    "        merged_df_sorted = merged_df.sort_values('collection_date')\n",
    "        merged_df_sorted['aeth_bc_ug'] = merged_df_sorted['aeth_mean'] / 1000\n",
    "        \n",
    "        # EC FTIR\n",
    "        axes[0].plot(merged_df_sorted['collection_date'], merged_df_sorted['EC_FTIR'], \n",
    "                    'o-', label=f'{dataset_name}', color=color, alpha=0.7, markersize=4)\n",
    "        \n",
    "        # Aethalometer BC\n",
    "        axes[1].plot(merged_df_sorted['collection_date'], merged_df_sorted['aeth_bc_ug'], \n",
    "                    'o-', label=f'{dataset_name}', color=color, alpha=0.7, markersize=4)\n",
    "        \n",
    "        # MAC\n",
    "        if 'MAC' in merged_df_sorted.columns:\n",
    "            # Filter extreme MAC values for better visualization\n",
    "            mac_filtered = merged_df_sorted['MAC'][(merged_df_sorted['MAC'] > 0) & (merged_df_sorted['MAC'] < 30)]\n",
    "            dates_filtered = merged_df_sorted['collection_date'][(merged_df_sorted['MAC'] > 0) & (merged_df_sorted['MAC'] < 30)]\n",
    "            \n",
    "            axes[2].plot(dates_filtered, mac_filtered, \n",
    "                        'o-', label=f'{dataset_name}', color=color, alpha=0.7, markersize=4)\n",
    "    \n",
    "    axes[0].set_ylabel('EC FTIR (¬µg/m¬≥)')\n",
    "    axes[0].set_title('EC FTIR Time Series')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    axes[1].set_ylabel('Aethalometer BC (¬µg/m¬≥)')\n",
    "    axes[1].set_title('Aethalometer BC Time Series')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    axes[2].set_ylabel('MAC (m¬≤/g)')\n",
    "    axes[2].set_title('Mass Absorption Cross-section (MAC) Time Series')\n",
    "    axes[2].set_xlabel('Date')\n",
    "    axes[2].legend()\n",
    "    axes[2].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return fig\n",
    "\n",
    "def plot_seasonal_analysis(merged_datasets: Dict[str, pd.DataFrame]):\n",
    "    \"\"\"Create seasonal analysis plots\"\"\"\n",
    "    \n",
    "    # Combine all datasets for seasonal analysis\n",
    "    all_data = []\n",
    "    for dataset_name, merged_df in merged_datasets.items():\n",
    "        if len(merged_df) > 0:\n",
    "            df_copy = merged_df.copy()\n",
    "            df_copy['dataset'] = dataset_name\n",
    "            df_copy['aeth_bc_ug'] = df_copy['aeth_mean'] / 1000\n",
    "            all_data.append(df_copy)\n",
    "    \n",
    "    if not all_data:\n",
    "        print(\"No data available for seasonal analysis\")\n",
    "        return\n",
    "    \n",
    "    combined_df = pd.concat(all_data, ignore_index=True)\n",
    "    \n",
    "    # Create seasonal plots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # Season colors\n",
    "    season_colors = {\n",
    "        'Dry Season': 'gold',\n",
    "        'Belg Rainy Season': 'limegreen', \n",
    "        'Kiremt Rainy Season': 'royalblue'\n",
    "    }\n",
    "    \n",
    "    # Plot 1: MAC by season\n",
    "    if 'MAC' in combined_df.columns and 'season' in combined_df.columns:\n",
    "        mac_filtered = combined_df[(combined_df['MAC'] > 0) & (combined_df['MAC'] < 30)]\n",
    "        \n",
    "        for season in mac_filtered['season'].unique():\n",
    "            season_data = mac_filtered[mac_filtered['season'] == season]\n",
    "            axes[0, 0].scatter(season_data['season'], season_data['MAC'], \n",
    "                             color=season_colors.get(season, 'gray'), \n",
    "                             alpha=0.6, s=40, label=f'{season} (n={len(season_data)})')\n",
    "        \n",
    "        # Add box plot overlay\n",
    "        seasons = mac_filtered['season'].unique()\n",
    "        mac_by_season = [mac_filtered[mac_filtered['season'] == season]['MAC'] for season in seasons]\n",
    "        axes[0, 0].boxplot(mac_by_season, positions=range(len(seasons)), \n",
    "                          labels=seasons, alpha=0.3)\n",
    "        \n",
    "        axes[0, 0].set_ylabel('MAC (m¬≤/g)')\n",
    "        axes[0, 0].set_title('MAC by Ethiopian Season')\n",
    "        axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "        axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: BC vs EC by season\n",
    "    valid_data = combined_df[['aeth_bc_ug', 'EC_FTIR', 'season']].dropna()\n",
    "    \n",
    "    for season in valid_data['season'].unique():\n",
    "        season_data = valid_data[valid_data['season'] == season]\n",
    "        axes[0, 1].scatter(season_data['EC_FTIR'], season_data['aeth_bc_ug'],\n",
    "                          color=season_colors.get(season, 'gray'),\n",
    "                          alpha=0.6, s=40, label=f'{season} (n={len(season_data)})')\n",
    "    \n",
    "    axes[0, 1].set_xlabel('EC FTIR (¬µg/m¬≥)')\n",
    "    axes[0, 1].set_ylabel('Aethalometer BC (¬µg/m¬≥)')\n",
    "    axes[0, 1].set_title('BC vs EC by Season')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 3: Monthly trends\n",
    "    if 'month' in combined_df.columns:\n",
    "        monthly_stats = combined_df.groupby('month').agg({\n",
    "            'aeth_bc_ug': ['mean', 'count'],\n",
    "            'EC_FTIR': 'mean',\n",
    "            'MAC': 'mean'\n",
    "        }).round(2)\n",
    "        \n",
    "        months = monthly_stats.index\n",
    "        axes[1, 0].plot(months, monthly_stats['aeth_bc_ug']['mean'], 'bo-', label='BC')\n",
    "        axes[1, 0].plot(months, monthly_stats['EC_FTIR']['mean'], 'ro-', label='EC')\n",
    "        \n",
    "        axes[1, 0].set_xlabel('Month')\n",
    "        axes[1, 0].set_ylabel('Concentration (¬µg/m¬≥)')\n",
    "        axes[1, 0].set_title('Monthly BC and EC Trends')\n",
    "        axes[1, 0].legend()\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "        axes[1, 0].set_xticks(range(1, 13))\n",
    "    \n",
    "    # Plot 4: Data availability by season\n",
    "    if 'season' in combined_df.columns:\n",
    "        season_counts = combined_df['season'].value_counts()\n",
    "        colors_list = [season_colors.get(season, 'gray') for season in season_counts.index]\n",
    "        \n",
    "        axes[1, 1].bar(season_counts.index, season_counts.values, color=colors_list, alpha=0.7)\n",
    "        axes[1, 1].set_ylabel('Number of Samples')\n",
    "        axes[1, 1].set_title('Data Availability by Season')\n",
    "        axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Create all visualizations\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìä CREATING VISUALIZATIONS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if merged_datasets:\n",
    "    print(\"\\nüìà Creating correlation analysis plots...\")\n",
    "    fig_corr = plot_correlation_analysis(merged_datasets)\n",
    "    \n",
    "    print(\"\\nüìà Creating time series comparison...\")\n",
    "    fig_ts = plot_time_series_comparison(merged_datasets)\n",
    "    \n",
    "    print(\"\\nüìà Creating seasonal analysis...\")\n",
    "    fig_seasonal = plot_seasonal_analysis(merged_datasets)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No merged datasets available for visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8582eda5",
   "metadata": {},
   "source": [
    "## 9. Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd3a3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_results(merged_datasets: Dict[str, pd.DataFrame], \n",
    "                  analysis_results: Dict,\n",
    "                  output_dir: str):\n",
    "    \"\"\"Export all results to files\"\"\"\n",
    "    \n",
    "    print(f\"\\nüì§ Exporting results to: {output_dir}\")\n",
    "    \n",
    "    # Create output directory\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Export individual merged datasets\n",
    "    for dataset_name, merged_df in merged_datasets.items():\n",
    "        csv_path = os.path.join(output_dir, f'merged_{dataset_name}.csv')\n",
    "        merged_df.to_csv(csv_path, index=False)\n",
    "        print(f\"‚úÖ Exported {dataset_name}: {csv_path}\")\n",
    "    \n",
    "    # Export combined dataset if multiple\n",
    "    if len(merged_datasets) > 1:\n",
    "        combined_df = pd.concat(merged_datasets.values(), ignore_index=True)\n",
    "        combined_path = os.path.join(output_dir, 'merged_combined_all_datasets.csv')\n",
    "        combined_df.to_csv(combined_path, index=False)\n",
    "        print(f\"‚úÖ Exported combined dataset: {combined_path}\")\n",
    "    \n",
    "    # Export analysis results as JSON\n",
    "    import json\n",
    "    \n",
    "    # Convert numpy types to Python types for JSON serialization\n",
    "    def convert_for_json(obj):\n",
    "        if isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        elif isinstance(obj, pd.Timestamp):\n",
    "            return obj.isoformat()\n",
    "        elif hasattr(obj, 'date'):\n",
    "            return obj.isoformat()\n",
    "        return obj\n",
    "    \n",
    "    json_results = {}\n",
    "    for dataset_name, stats in analysis_results.items():\n",
    "        json_results[dataset_name] = {}\n",
    "        for key, value in stats.items():\n",
    "            if isinstance(value, dict):\n",
    "                json_results[dataset_name][key] = {k: convert_for_json(v) for k, v in value.items()}\n",
    "            else:\n",
    "                json_results[dataset_name][key] = convert_for_json(value)\n",
    "    \n",
    "    json_path = os.path.join(output_dir, 'analysis_results.json')\n",
    "    with open(json_path, 'w') as f:\n",
    "        json.dump(json_results, f, indent=2)\n",
    "    print(f\"‚úÖ Exported analysis results: {json_path}\")\n",
    "    \n",
    "    # Create summary report\n",
    "    summary_path = os.path.join(output_dir, 'pipeline_summary.txt')\n",
    "    with open(summary_path, 'w') as f:\n",
    "        f.write(\"AETHALOMETER-FTIR/HIPS PIPELINE SUMMARY\\n\")\n",
    "        f.write(\"=\" * 50 + \"\\n\\n\")\n",
    "        \n",
    "        f.write(f\"Analysis Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "        f.write(f\"Site: {SITE_CODE}\\n\")\n",
    "        f.write(f\"Wavelength: {WAVELENGTH}\\n\")\n",
    "        f.write(f\"Quality Threshold: {QUALITY_THRESHOLD} minutes\\n\\n\")\n",
    "        \n",
    "        f.write(\"DATASETS PROCESSED:\\n\")\n",
    "        f.write(\"-\" * 20 + \"\\n\")\n",
    "        for dataset_name, merged_df in merged_datasets.items():\n",
    "            f.write(f\"{dataset_name}: {len(merged_df)} merged periods\\n\")\n",
    "        \n",
    "        f.write(f\"\\nFTIR/HIPS DATA:\\n\")\n",
    "        f.write(\"-\" * 20 + \"\\n\")\n",
    "        if ftir_data is not None:\n",
    "            f.write(f\"Total filter samples: {len(ftir_data)}\\n\")\n",
    "            f.write(f\"Date range: {ftir_data['sample_date'].min()} to {ftir_data['sample_date'].max()}\\n\")\n",
    "        \n",
    "        f.write(f\"\\nKEY CORRELATIONS:\\n\")\n",
    "        f.write(\"-\" * 20 + \"\\n\")\n",
    "        for dataset_name, stats in analysis_results.items():\n",
    "            if 'correlations' in stats:\n",
    "                f.write(f\"{dataset_name}:\\n\")\n",
    "                for corr_name, corr_value in stats['correlations'].items():\n",
    "                    f.write(f\"  {corr_name}: {corr_value:.3f}\\n\")\n",
    "                f.write(\"\\n\")\n",
    "    \n",
    "    print(f\"‚úÖ Exported summary report: {summary_path}\")\n",
    "    print(f\"\\nüéØ Export completed! All results saved to: {output_dir}\")\n",
    "\n",
    "# Export all results\n",
    "export_results(merged_datasets, analysis_results, OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0e17be",
   "metadata": {},
   "source": [
    "## 10. Pipeline Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96e0b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pipeline_summary():\n",
    "    \"\"\"Create a comprehensive summary of the pipeline results\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üéØ PIPELINE SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(f\"üìä Configuration:\")\n",
    "    print(f\"   Site: {SITE_CODE}\")\n",
    "    print(f\"   Wavelength: {WAVELENGTH}\")\n",
    "    print(f\"   Quality threshold: {QUALITY_THRESHOLD} minutes\")\n",
    "    print(f\"   Output directory: {OUTPUT_DIR}\")\n",
    "    \n",
    "    print(f\"\\nüìÅ Data Loading:\")\n",
    "    print(f\"   Aethalometer files processed: {len(aethalometer_datasets)}\")\n",
    "    for name, summary in aethalometer_summaries.items():\n",
    "        print(f\"     {name}: {summary['shape'][0]:,} rows ({summary['file_type']})\")\n",
    "    \n",
    "    if ftir_data is not None:\n",
    "        print(f\"   FTIR/HIPS samples: {len(ftir_data)}\")\n",
    "        print(f\"   FTIR date range: {ftir_data['sample_date'].min()} to {ftir_data['sample_date'].max()}\")\n",
    "    \n",
    "    print(f\"\\nüîç Quality Assessment:\")\n",
    "    for dataset_name, excellent_periods in excellent_periods_dict.items():\n",
    "        print(f\"   {dataset_name}: {len(excellent_periods)} excellent periods\")\n",
    "    \n",
    "    print(f\"\\nüîó Merging Results:\")\n",
    "    total_merged = sum(len(df) for df in merged_datasets.values())\n",
    "    print(f\"   Total merged periods: {total_merged}\")\n",
    "    for name, merged_df in merged_datasets.items():\n",
    "        print(f\"     {name}: {len(merged_df)} periods\")\n",
    "    \n",
    "    print(f\"\\nüìà Key Findings:\")\n",
    "    for dataset_name, stats in analysis_results.items():\n",
    "        if 'correlations' in stats and stats['correlations']:\n",
    "            print(f\"   {dataset_name}:\")\n",
    "            for corr_name, corr_value in stats['correlations'].items():\n",
    "                print(f\"     {corr_name}: {corr_value:.3f}\")\n",
    "            \n",
    "            if 'mac_statistics' in stats:\n",
    "                mac_mean = stats['mac_statistics']['mean']\n",
    "                print(f\"     Mean MAC: {mac_mean:.2f} m¬≤/g\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Pipeline Status: COMPLETED SUCCESSFULLY\")\n",
    "    print(f\"üì§ Results exported to: {OUTPUT_DIR}\")\n",
    "    \n",
    "    if merged_datasets:\n",
    "        print(f\"\\nüí° Next Steps:\")\n",
    "        print(f\"   ‚Ä¢ Review correlation plots for method comparison\")\n",
    "        print(f\"   ‚Ä¢ Examine seasonal patterns in the data\")\n",
    "        print(f\"   ‚Ä¢ Consider extending analysis to other wavelengths\")\n",
    "        print(f\"   ‚Ä¢ Investigate periods with high/low correlations\")\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "\n",
    "# Create final summary\n",
    "create_pipeline_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f4d93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final cell - display key results for quick reference\n",
    "if merged_datasets:\n",
    "    print(\"üéâ PIPELINE COMPLETED SUCCESSFULLY!\")\n",
    "    print(\"\\nüìä Quick Results Summary:\")\n",
    "    \n",
    "    for dataset_name, merged_df in merged_datasets.items():\n",
    "        print(f\"\\n{dataset_name}:\")\n",
    "        print(f\"  ‚Ä¢ Merged periods: {len(merged_df)}\")\n",
    "        \n",
    "        if len(merged_df) > 0:\n",
    "            # Quick correlation\n",
    "            merged_df['aeth_bc_ug'] = merged_df['aeth_mean'] / 1000\n",
    "            \n",
    "            if 'EC_FTIR' in merged_df.columns:\n",
    "                bc_ec_corr = merged_df['aeth_bc_ug'].corr(merged_df['EC_FTIR'])\n",
    "                print(f\"  ‚Ä¢ BC-EC correlation: {bc_ec_corr:.3f}\")\n",
    "            \n",
    "            if 'MAC' in merged_df.columns:\n",
    "                mac_mean = merged_df['MAC'][(merged_df['MAC'] > 0) & (merged_df['MAC'] < 30)].mean()\n",
    "                print(f\"  ‚Ä¢ Mean MAC: {mac_mean:.2f} m¬≤/g\")\n",
    "    \n",
    "    print(f\"\\nüìÅ All results saved to: {OUTPUT_DIR}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No successful merging occurred. Check configuration and data availability.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
